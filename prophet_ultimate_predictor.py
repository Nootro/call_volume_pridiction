#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
==============================================================================
Prophet Ultimate Predictor for Call Center Daily Volume Forecasting v2.1
==============================================================================

è¶…é«˜ç²¾åº¦ã‚³ãƒ¼ãƒ«ã‚»ãƒ³ã‚¿ãƒ¼æ—¥æ¬¡å‘¼é‡äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ  (2ãƒ¶æœˆäºˆæ¸¬ + è©³ç´°æ¤œè¨¼ä»˜ã)

ä¸»è¦æ©Ÿèƒ½:
---------
1. è‡ªå‹•è¨ºæ–­ã‚·ã‚¹ãƒ†ãƒ  (CV, ANOVA, ACF, ADF, ã‚¹ãƒšã‚¯ãƒˆãƒ«è§£æ, STLåˆ†è§£)
2. ãƒ¬ã‚¸ãƒ¼ãƒ è‡ªå‹•æ¤œå‡º (K-means + åˆ†ä½ç‚¹ãƒ™ãƒ¼ã‚¹)
3. ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ– (Grid Search + æ™‚ç³»åˆ—äº¤å·®æ¤œè¨¼)
4. ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’ (è¤‡æ•°Prophetãƒ¢ãƒ‡ãƒ« + é‡ã¿ä»˜ãå¹³å‡)
5. é«˜åº¦ãªç‰¹å¾´é‡ç”Ÿæˆ (ç¥æ—¥, æœˆåˆæœˆæœ«, ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³, å¤–ç”Ÿå¤‰æ•°)
6. 2ãƒ¶æœˆå›ºå®šäºˆæ¸¬ (æœ€å¾Œã®æœˆã‹ã‚‰2ãƒ¶æœˆå…ˆ)
7. è©³ç´°æ¤œè¨¼ (1ãƒ¶æœˆç›®/2ãƒ¶æœˆç›®/2ãƒ¶æœˆé–“ã® RMSE/MAE/MAPE)
8. åŒ…æ‹¬çš„å¯è¦–åŒ– (20+ãƒãƒ£ãƒ¼ãƒˆ)
9. è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆ (JSON, CSV, TXT)
10. ãƒ¢ãƒ‡ãƒ«æ°¸ç¶šåŒ– (ä¿å­˜/ãƒ­ãƒ¼ãƒ‰)

ä½¿ç”¨ä¾‹:
-------
# ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³
python prophet_ultimate_predictor.py data.csv

# Python ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
from prophet_ultimate_predictor import ProphetUltimatePredictor
predictor = ProphetUltimatePredictor()
results = predictor.fit_predict('data.csv')

ä½œæˆè€…: AI Assistant
ãƒãƒ¼ã‚¸ãƒ§ãƒ³: 2.1
æœ€çµ‚æ›´æ–°: 2026-02-16
"""

import warnings
warnings.filterwarnings('ignore')

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta
from dateutil.relativedelta import relativedelta
import calendar
import json
import pickle
import logging
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Union
import sys

# Prophet ã¨è¨ºæ–­ãƒ„ãƒ¼ãƒ«
try:
    from prophet import Prophet
    from prophet.diagnostics import cross_validation, performance_metrics
    from prophet.plot import plot_cross_validation_metric
except ImportError:
    print("âŒ Prophet not installed. Run: pip install prophet")
    sys.exit(1)

# çµ±è¨ˆãƒ»æ™‚ç³»åˆ—åˆ†æ
from scipy import stats, signal
from scipy.stats import normaltest, shapiro, anderson, jarque_bera
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score, mean_absolute_error, mean_squared_error
from statsmodels.tsa.stattools import adfuller, acf, pacf
from statsmodels.tsa.seasonal import STL, seasonal_decompose
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf

# æ—¥æœ¬ã®ç¥æ—¥
try:
    import jpholiday
except ImportError:
    print("âš ï¸  jpholiday not installed. Run: pip install jpholiday")
    jpholiday = None

# ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼
try:
    from tqdm import tqdm
except ImportError:
    tqdm = lambda x, **kwargs: x


# ============================================================================
# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
# ============================================================================
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)s | %(message)s',
    handlers=[
        logging.FileHandler('prophet_ultimate_predictor.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


# ============================================================================
# ProphetUltimatePredictor ãƒ¡ã‚¤ãƒ³ã‚¯ãƒ©ã‚¹ (v2.1)
# ============================================================================
class ProphetUltimatePredictor:
    """
    ã‚³ãƒ¼ãƒ«ã‚»ãƒ³ã‚¿ãƒ¼æ—¥æ¬¡å‘¼é‡äºˆæ¸¬ç”¨ã®è¶…é«˜ç²¾åº¦Prophetã‚·ã‚¹ãƒ†ãƒ 
    2ãƒ¶æœˆå›ºå®šäºˆæ¸¬ + è©³ç´°æ¤œè¨¼æ©Ÿèƒ½ä»˜ã
    
    Parameters
    ----------
    output_dir : str
        å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 'prophet_ultimate_results')
    """
    
    def __init__(self, output_dir: str = 'prophet_ultimate_results'):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True, parents=True)
        
        self.df = None
        self.df_train = None  # å­¦ç¿’ç”¨ (æœ€å¾Œã®2ãƒ¶æœˆã‚’é™¤ã)
        self.df_validation = None  # æ¤œè¨¼ç”¨ (æœ€å¾Œã®2ãƒ¶æœˆ)
        self.diagnostics = {}
        self.regimes = {}
        self.models = {}
        self.forecasts = {}
        self.best_params = {}
        self.cv_results = {}
        self.ensemble_forecast = None
        self.validation_metrics = {}  # æ¤œè¨¼çµæœ
        
        logger.info(f"âœ… ProphetUltimatePredictor v2.1 initialized. Output: {self.output_dir}")
    
    # ========================================================================
    # 1. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ & å‰å‡¦ç† (æ¤œè¨¼åˆ†å‰²ä»˜ã)
    # ========================================================================
    def load_data(self, filepath: Union[str, Path], date_col: str = 'ds', 
                  value_col: str = 'y', validation_months: int = 2) -> pd.DataFrame:
        """
        ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨åŸºæœ¬å‰å‡¦ç† (æœ€å¾Œã®N ãƒ¶æœˆã‚’æ¤œè¨¼ç”¨ã«åˆ†å‰²)
        
        Parameters
        ----------
        filepath : str or Path
            CSVãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        date_col : str
            æ—¥ä»˜ã‚«ãƒ©ãƒ å
        value_col : str
            ç›®çš„å¤‰æ•°ã‚«ãƒ©ãƒ å
        validation_months : int
            æ¤œè¨¼æœŸé–“ (æœˆæ•°ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 2)
        
        Returns
        -------
        pd.DataFrame
            å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿
        """
        logger.info(f"ğŸ“‚ Loading data from {filepath}")
        
        df = pd.read_csv(filepath)
        
        # ã‚«ãƒ©ãƒ åæ¨™æº–åŒ–
        if date_col not in df.columns or value_col not in df.columns:
            logger.warning(f"âš ï¸  Columns not found. Available: {df.columns.tolist()}")
            # è‡ªå‹•æ¤œå‡º
            date_candidates = [c for c in df.columns if 'date' in c.lower() or 'ds' in c.lower()]
            value_candidates = [c for c in df.columns if c.lower() in ['y', 'value', 'volume', 'calls']]
            
            if date_candidates:
                date_col = date_candidates[0]
            if value_candidates:
                value_col = value_candidates[0]
        
        df = df[[date_col, value_col]].copy()
        df.columns = ['ds', 'y']
        
        # æ—¥ä»˜å¤‰æ›
        df['ds'] = pd.to_datetime(df['ds'])
        df = df.sort_values('ds').reset_index(drop=True)
        
        # æ¬ æå€¤å‡¦ç†
        missing_count = df['y'].isna().sum()
        if missing_count > 0:
            logger.warning(f"âš ï¸  {missing_count} missing values detected. Filling with interpolation.")
            df['y'] = df['y'].interpolate(method='time')
            df['y'] = df['y'].fillna(df['y'].median())
        
        # è² å€¤å‡¦ç†
        negative_count = (df['y'] < 0).sum()
        if negative_count > 0:
            logger.warning(f"âš ï¸  {negative_count} negative values detected. Clipping to 0.")
            df['y'] = df['y'].clip(lower=0)
        
        # æ¤œè¨¼æœŸé–“ã®è¨ˆç®— (æœ€å¾Œã®N ãƒ¶æœˆ)
        max_date = df['ds'].max()
        validation_start = max_date - relativedelta(months=validation_months) + timedelta(days=1)
        
        # ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
        self.df = df
        self.df_train = df[df['ds'] < validation_start].copy()
        self.df_validation = df[df['ds'] >= validation_start].copy()
        
        logger.info(f"âœ… Data loaded: {len(df)} rows, {df['ds'].min()} to {df['ds'].max()}")
        logger.info(f"  ğŸ“Š Train: {len(self.df_train)} rows ({self.df_train['ds'].min()} to {self.df_train['ds'].max()})")
        logger.info(f"  ğŸ” Validation: {len(self.df_validation)} rows ({self.df_validation['ds'].min()} to {self.df_validation['ds'].max()})")
        
        # æ¤œè¨¼æœŸé–“ã®æœˆæƒ…å ±
        val_months = self.df_validation.groupby(self.df_validation['ds'].dt.to_period('M')).size()
        logger.info(f"  ğŸ“… Validation months: {list(val_months.index.astype(str))}")
        
        return df
    
    # ========================================================================
    # 2. åŒ…æ‹¬çš„è¨ºæ–­ã‚·ã‚¹ãƒ†ãƒ  (å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®ã¿)
    # ========================================================================
    def run_comprehensive_diagnostics(self) -> Dict:
        """
        åŒ…æ‹¬çš„æ™‚ç³»åˆ—è¨ºæ–­ã‚’å®Ÿè¡Œ (å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®ã¿ä½¿ç”¨)
        
        Returns
        -------
        dict
            è¨ºæ–­çµæœè¾æ›¸
        """
        logger.info("ğŸ” Running comprehensive diagnostics on training data...")
        
        df = self.df_train.copy()
        y = df['y'].values
        
        diagnostics = {}
        
        # ------------------------------------------------------------------
        # 2.1 åŸºæœ¬çµ±è¨ˆé‡
        # ------------------------------------------------------------------
        diagnostics['basic_stats'] = {
            'mean': float(np.mean(y)),
            'std': float(np.std(y)),
            'cv': float(np.std(y) / np.mean(y)),
            'min': float(np.min(y)),
            'max': float(np.max(y)),
            'q25': float(np.percentile(y, 25)),
            'median': float(np.median(y)),
            'q75': float(np.percentile(y, 75)),
            'iqr': float(np.percentile(y, 75) - np.percentile(y, 25)),
            'skewness': float(stats.skew(y)),
            'kurtosis': float(stats.kurtosis(y))
        }
        
        cv = diagnostics['basic_stats']['cv']
        logger.info(f"  ğŸ“Š Mean: {diagnostics['basic_stats']['mean']:.1f}, CV: {cv:.3f}")
        
        # ------------------------------------------------------------------
        # 2.2 æ­£è¦æ€§æ¤œå®š
        # ------------------------------------------------------------------
        try:
            _, p_shapiro = shapiro(y[:5000] if len(y) > 5000 else y)
            _, p_normal = normaltest(y)
            jb_stat, p_jb = jarque_bera(y)
            
            diagnostics['normality'] = {
                'shapiro_p': float(p_shapiro),
                'normaltest_p': float(p_normal),
                'jarque_bera_p': float(p_jb),
                'is_normal': bool(p_normal > 0.05)
            }
            logger.info(f"  ğŸ“ˆ Normality test p-value: {p_normal:.4f}")
        except Exception as e:
            logger.warning(f"  âš ï¸  Normality test failed: {e}")
            diagnostics['normality'] = {'is_normal': False}
        
        # ------------------------------------------------------------------
        # 2.3 å®šå¸¸æ€§æ¤œå®š (ADF)
        # ------------------------------------------------------------------
        try:
            adf_result = adfuller(y, autolag='AIC')
            diagnostics['stationarity'] = {
                'adf_statistic': float(adf_result[0]),
                'adf_p_value': float(adf_result[1]),
                'is_stationary': bool(adf_result[1] < 0.05)
            }
            logger.info(f"  ğŸ“‰ ADF p-value: {adf_result[1]:.4f} ({'å®šå¸¸' if adf_result[1] < 0.05 else 'éå®šå¸¸'})")
        except Exception as e:
            logger.warning(f"  âš ï¸  ADF test failed: {e}")
            diagnostics['stationarity'] = {'is_stationary': False}
        
        # ------------------------------------------------------------------
        # 2.4 è‡ªå·±ç›¸é–¢åˆ†æ
        # ------------------------------------------------------------------
        try:
            acf_values = acf(y, nlags=min(30, len(y)//2 - 1), fft=True)
            pacf_values = pacf(y, nlags=min(30, len(y)//2 - 1))
            
            diagnostics['autocorrelation'] = {
                'acf_lag1': float(acf_values[1]),
                'acf_lag7': float(acf_values[7]) if len(acf_values) > 7 else 0.0,
                'pacf_lag1': float(pacf_values[1]),
                'significant_lags': [int(i) for i, val in enumerate(acf_values[1:15]) if abs(val) > 1.96/np.sqrt(len(y))]
            }
            logger.info(f"  ğŸ”„ ACF(lag=7): {diagnostics['autocorrelation']['acf_lag7']:.3f}")
        except Exception as e:
            logger.warning(f"  âš ï¸  ACF/PACF failed: {e}")
            diagnostics['autocorrelation'] = {}
        
        # ------------------------------------------------------------------
        # 2.5 æ›œæ—¥åŠ¹æœ (ANOVA)
        # ------------------------------------------------------------------
        df['weekday'] = df['ds'].dt.dayofweek
        try:
            weekday_groups = [df[df['weekday'] == i]['y'].values for i in range(7)]
            f_stat, p_value = stats.f_oneway(*weekday_groups)
            
            diagnostics['weekday_effect'] = {
                'f_statistic': float(f_stat),
                'p_value': float(p_value),
                'has_effect': bool(p_value < 0.05)
            }
            logger.info(f"  ğŸ“… Weekday ANOVA p-value: {p_value:.4e}")
        except Exception as e:
            logger.warning(f"  âš ï¸  Weekday ANOVA failed: {e}")
            diagnostics['weekday_effect'] = {'has_effect': False}
        
        # ------------------------------------------------------------------
        # 2.6 æœˆåŠ¹æœ (ANOVA)
        # ------------------------------------------------------------------
        df['month'] = df['ds'].dt.month
        try:
            month_groups = [df[df['month'] == i]['y'].values for i in range(1, 13) if len(df[df['month'] == i]) > 0]
            if len(month_groups) > 1:
                f_stat, p_value = stats.f_oneway(*month_groups)
                
                diagnostics['month_effect'] = {
                    'f_statistic': float(f_stat),
                    'p_value': float(p_value),
                    'has_effect': bool(p_value < 0.05)
                }
                logger.info(f"  ğŸ“† Month ANOVA p-value: {p_value:.4e}")
            else:
                diagnostics['month_effect'] = {'has_effect': False}
        except Exception as e:
            logger.warning(f"  âš ï¸  Month ANOVA failed: {e}")
            diagnostics['month_effect'] = {'has_effect': False}
        
        # ------------------------------------------------------------------
        # 2.7 ã‚¹ãƒšã‚¯ãƒˆãƒ«è§£æ
        # ------------------------------------------------------------------
        try:
            freqs, psd = signal.periodogram(y, fs=1.0)
            top_freq_idx = np.argsort(psd[1:])[-3:] + 1
            top_periods = [1.0 / freqs[i] for i in top_freq_idx if freqs[i] > 0]
            
            diagnostics['spectral'] = {
                'dominant_periods': [float(p) for p in sorted(top_periods, reverse=True)]
            }
            logger.info(f"  ğŸŒŠ Dominant periods: {[f'{p:.1f}' for p in top_periods[:3]]}")
        except Exception as e:
            logger.warning(f"  âš ï¸  Spectral analysis failed: {e}")
            diagnostics['spectral'] = {}
        
        # ------------------------------------------------------------------
        # 2.8 STLåˆ†è§£
        # ------------------------------------------------------------------
        try:
            if len(y) >= 14:
                stl = STL(y, seasonal=7, robust=True)
                result = stl.fit()
                
                trend_strength = 1 - np.var(result.resid) / np.var(result.trend + result.resid)
                seasonal_strength = 1 - np.var(result.resid) / np.var(result.seasonal + result.resid)
                
                diagnostics['stl_decomposition'] = {
                    'trend_strength': float(max(0, trend_strength)),
                    'seasonal_strength': float(max(0, seasonal_strength))
                }
                logger.info(f"  ğŸ”¬ Trend strength: {trend_strength:.3f}, Seasonal: {seasonal_strength:.3f}")
        except Exception as e:
            logger.warning(f"  âš ï¸  STL decomposition failed: {e}")
            diagnostics['stl_decomposition'] = {}
        
        # ------------------------------------------------------------------
        # 2.9 å¤–ã‚Œå€¤æ¤œå‡º
        # ------------------------------------------------------------------
        q1, q3 = np.percentile(y, [25, 75])
        iqr = q3 - q1
        lower_bound = q1 - 1.5 * iqr
        upper_bound = q3 + 1.5 * iqr
        outliers = (y < lower_bound) | (y > upper_bound)
        
        diagnostics['outliers'] = {
            'count': int(np.sum(outliers)),
            'percentage': float(100 * np.sum(outliers) / len(y)),
            'lower_bound': float(lower_bound),
            'upper_bound': float(upper_bound)
        }
        logger.info(f"  ğŸš¨ Outliers: {diagnostics['outliers']['count']} ({diagnostics['outliers']['percentage']:.2f}%)")
        
        # ------------------------------------------------------------------
        # 2.10 äºŒå³°æ€§æ¤œå‡º
        # ------------------------------------------------------------------
        try:
            from scipy.stats import gaussian_kde
            kde = gaussian_kde(y)
            x_grid = np.linspace(y.min(), y.max(), 200)
            density = kde(x_grid)
            
            peaks = signal.find_peaks(density, prominence=0.01)[0]
            diagnostics['bimodality'] = {
                'num_peaks': len(peaks),
                'is_bimodal': bool(len(peaks) >= 2)
            }
            logger.info(f"  ğŸ‘¥ Distribution peaks: {len(peaks)} ({'äºŒå³°æ€§' if len(peaks) >= 2 else 'å˜å³°æ€§'})")
        except Exception as e:
            logger.warning(f"  âš ï¸  Bimodality test failed: {e}")
            diagnostics['bimodality'] = {'is_bimodal': False}
        
        self.diagnostics = diagnostics
        
        # è¨ºæ–­çµæœã‚’ä¿å­˜
        with open(self.output_dir / 'diagnostics.json', 'w', encoding='utf-8') as f:
            json.dump(diagnostics, f, indent=2, ensure_ascii=False)
        
        logger.info(f"âœ… Diagnostics completed. Saved to {self.output_dir / 'diagnostics.json'}")
        
        return diagnostics
    
    # ========================================================================
    # 3. ãƒ¬ã‚¸ãƒ¼ãƒ è‡ªå‹•æ¤œå‡º
    # ========================================================================
    def detect_regimes(self, n_regimes: int = 2, method: str = 'kmeans') -> Dict:
        """
        ãƒ¬ã‚¸ãƒ¼ãƒ è‡ªå‹•æ¤œå‡º (K-means ã¾ãŸã¯åˆ†ä½ç‚¹ãƒ™ãƒ¼ã‚¹)
        
        Parameters
        ----------
        n_regimes : int
            ãƒ¬ã‚¸ãƒ¼ãƒ æ•°
        method : str
            'kmeans' ã¾ãŸã¯ 'quantile'
        
        Returns
        -------
        dict
            ãƒ¬ã‚¸ãƒ¼ãƒ æƒ…å ±
        """
        logger.info(f"ğŸ¯ Detecting {n_regimes} regimes using {method} method...")
        
        df = self.df_train.copy()
        y = df['y'].values.reshape(-1, 1)
        
        if method == 'kmeans':
            scaler = StandardScaler()
            y_scaled = scaler.fit_transform(y)
            
            kmeans = KMeans(n_clusters=n_regimes, random_state=42, n_init=10)
            labels = kmeans.fit_predict(y_scaled)
            
            silhouette = silhouette_score(y_scaled, labels)
            logger.info(f"  ğŸ¯ Silhouette score: {silhouette:.3f}")
            
        elif method == 'quantile':
            quantiles = np.linspace(0, 1, n_regimes + 1)[1:-1]
            thresholds = np.quantile(y, quantiles)
            
            labels = np.zeros(len(y), dtype=int)
            for i, threshold in enumerate(thresholds):
                labels[y.flatten() > threshold] = i + 1
            
            silhouette = None
        
        else:
            raise ValueError(f"Unknown method: {method}")
        
        df['regime'] = labels
        
        # ãƒ¬ã‚¸ãƒ¼ãƒ çµ±è¨ˆ
        regime_stats = {}
        for regime_id in range(n_regimes):
            regime_data = df[df['regime'] == regime_id]['y']
            regime_stats[f'regime_{regime_id}'] = {
                'count': int(len(regime_data)),
                'mean': float(regime_data.mean()),
                'std': float(regime_data.std()),
                'cv': float(regime_data.std() / regime_data.mean()),
                'min': float(regime_data.min()),
                'max': float(regime_data.max())
            }
            logger.info(f"  ğŸ“Š Regime {regime_id}: N={len(regime_data)}, Mean={regime_data.mean():.1f}, CV={regime_stats[f'regime_{regime_id}']['cv']:.3f}")
        
        self.regimes = {
            'method': method,
            'n_regimes': n_regimes,
            'labels': labels.tolist(),
            'stats': regime_stats,
            'silhouette_score': float(silhouette) if silhouette is not None else None
        }
        
        self.df_train = df
        
        # ãƒ¬ã‚¸ãƒ¼ãƒ æƒ…å ±ã‚’ä¿å­˜
        with open(self.output_dir / 'regimes.json', 'w', encoding='utf-8') as f:
            json.dump(self.regimes, f, indent=2, ensure_ascii=False)
        
        logger.info(f"âœ… Regimes detected. Saved to {self.output_dir / 'regimes.json'}")
        
        return self.regimes
    
    # ========================================================================
    # 4. ç¥æ—¥ãƒ»ç‰¹æ®Šæ—¥ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ç”Ÿæˆ
    # ========================================================================
    def create_holiday_dataframe(self, start_year: int = None, 
                                  end_year: int = None) -> pd.DataFrame:
        """
        æ—¥æœ¬ã®ç¥æ—¥ãƒ»ç‰¹æ®Šæ—¥ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ç”Ÿæˆ
        
        Parameters
        ----------
        start_year : int
            é–‹å§‹å¹´ (Noneã®å ´åˆã¯ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰è‡ªå‹•æ¤œå‡º)
        end_year : int
            çµ‚äº†å¹´ (Noneã®å ´åˆã¯ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰è‡ªå‹•æ¤œå‡º)
        
        Returns
        -------
        pd.DataFrame
            ç¥æ—¥ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
        """
        if self.df is None:
            raise ValueError("Data not loaded. Call load_data() first.")
        
        if start_year is None:
            start_year = self.df['ds'].dt.year.min()
        if end_year is None:
            end_year = self.df['ds'].dt.year.max() + 2  # äºˆæ¸¬æœŸé–“ã‚’è€ƒæ…®
        
        logger.info(f"ğŸ“… Creating holiday dataframe ({start_year}-{end_year})")
        
        holidays = []
        
        # ------------------------------------------------------------------
        # 4.1 æ—¥æœ¬ã®ç¥æ—¥
        # ------------------------------------------------------------------
        if jpholiday is not None:
            for year in range(start_year, end_year + 1):
                for month in range(1, 13):
                    for day in range(1, 32):
                        try:
                            date = datetime(year, month, day)
                            if jpholiday.is_holiday(date):
                                name = jpholiday.is_holiday_name(date)
                                holidays.append({
                                    'ds': date,
                                    'holiday': 'jp_holiday',
                                    'lower_window': 0,
                                    'upper_window': 0,
                                    'prior_scale': 20.0
                                })
                        except ValueError:
                            continue
            logger.info(f"  ğŸŒ Added {len(holidays)} Japanese holidays")
        
        # ------------------------------------------------------------------
        # 4.2 æœˆåˆ (1-3æ—¥)
        # ------------------------------------------------------------------
        for year in range(start_year, end_year + 1):
            for month in range(1, 13):
                for day in [1, 2, 3]:
                    try:
                        holidays.append({
                            'ds': datetime(year, month, day),
                            'holiday': 'month_start',
                            'lower_window': 0,
                            'upper_window': 0,
                            'prior_scale': 15.0
                        })
                    except ValueError:
                        continue
        
        # ------------------------------------------------------------------
        # 4.3 æœˆæœ« (æœ€çµ‚3æ—¥)
        # ------------------------------------------------------------------
        for year in range(start_year, end_year + 1):
            for month in range(1, 13):
                last_day = calendar.monthrange(year, month)[1]
                
                for day in [last_day - 2, last_day - 1, last_day]:
                    if day > 0:
                        try:
                            holidays.append({
                                'ds': datetime(year, month, day),
                                'holiday': 'month_end',
                                'lower_window': 0,
                                'upper_window': 0,
                                'prior_scale': 15.0
                            })
                        except ValueError:
                            continue
        
        # ------------------------------------------------------------------
        # 4.4 å¹´æœ«å¹´å§‹
        # ------------------------------------------------------------------
        for year in range(start_year, end_year + 1):
            # å¹´æœ« (12/28-31)
            for day in range(28, 32):
                try:
                    holidays.append({
                        'ds': datetime(year, 12, day),
                        'holiday': 'year_end',
                        'lower_window': 0,
                        'upper_window': 0,
                        'prior_scale': 30.0
                    })
                except ValueError:
                    continue
            
            # å¹´å§‹ (1/1-7)
            for day in range(1, 8):
                try:
                    holidays.append({
                        'ds': datetime(year, 1, day),
                        'holiday': 'new_year',
                        'lower_window': 0,
                        'upper_window': 0,
                        'prior_scale': 30.0
                    })
                except ValueError:
                    continue
        
        holidays_df = pd.DataFrame(holidays)
        
        # é‡è¤‡å‰Šé™¤
        holidays_df = holidays_df.sort_values('prior_scale', ascending=False).drop_duplicates('ds', keep='first')
        holidays_df = holidays_df.sort_values('ds').reset_index(drop=True)
        
        logger.info(f"âœ… Holiday dataframe created: {len(holidays_df)} entries")
        
        return holidays_df
    
    # ========================================================================
    # 5. ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ– (ç°¡ç•¥ç‰ˆ)
    # ========================================================================
    def optimize_hyperparameters(self, df: pd.DataFrame, 
                                  holidays: pd.DataFrame = None,
                                  quick_mode: bool = True) -> Dict:
        """
        è¨ºæ–­ãƒ™ãƒ¼ã‚¹ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿é¸æŠ (é«˜é€Ÿç‰ˆ)
        
        Parameters
        ----------
        df : pd.DataFrame
            å­¦ç¿’ãƒ‡ãƒ¼ã‚¿
        holidays : pd.DataFrame
            ç¥æ—¥ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
        quick_mode : bool
            Trueã®å ´åˆã€äº¤å·®æ¤œè¨¼ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¦è¨ºæ–­ãƒ™ãƒ¼ã‚¹ã§é¸æŠ
        
        Returns
        -------
        dict
            æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        """
        logger.info("ğŸ” Selecting hyperparameters based on diagnostics...")
        
        # è¨ºæ–­çµæœã«åŸºã¥ã„ã¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿é¸æŠ
        cv = self.diagnostics.get('basic_stats', {}).get('cv', 0.3)
        
        if cv < 0.3:
            # ä½å¤‰å‹•
            best_params = {
                'changepoint_prior_scale': 0.05,
                'seasonality_prior_scale': 5.0,
                'holidays_prior_scale': 10.0,
                'seasonality_mode': 'additive'
            }
            logger.info("  ğŸ“Š Low variability (CV<0.3) â†’ Conservative parameters")
        elif cv < 0.5:
            # ä¸­å¤‰å‹•
            best_params = {
                'changepoint_prior_scale': 0.1,
                'seasonality_prior_scale': 10.0,
                'holidays_prior_scale': 20.0,
                'seasonality_mode': 'multiplicative'
            }
            logger.info("  ğŸ“Š Medium variability (0.3â‰¤CV<0.5) â†’ Standard parameters")
        else:
            # é«˜å¤‰å‹•
            best_params = {
                'changepoint_prior_scale': 0.3,
                'seasonality_prior_scale': 15.0,
                'holidays_prior_scale': 30.0,
                'seasonality_mode': 'multiplicative'
            }
            logger.info("  ğŸ“Š High variability (CVâ‰¥0.5) â†’ Aggressive parameters")
        
        self.best_params = best_params
        
        # çµæœä¿å­˜
        with open(self.output_dir / 'best_params.json', 'w', encoding='utf-8') as f:
            json.dump({'params': best_params, 'cv': float(cv)}, f, indent=2, ensure_ascii=False)
        
        logger.info(f"âœ… Parameters selected: {best_params}")
        
        return best_params
    
    # ========================================================================
    # 6. ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«äºˆæ¸¬ (2ãƒ¶æœˆå›ºå®š)
    # ========================================================================
    def fit_ensemble_models(self, df: pd.DataFrame, 
                            holidays: pd.DataFrame = None) -> Dict:
        """
        è¤‡æ•°ã®Prophetãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã—ã¦ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«äºˆæ¸¬ (2ãƒ¶æœˆå›ºå®š)
        
        Parameters
        ----------
        df : pd.DataFrame
            å­¦ç¿’ãƒ‡ãƒ¼ã‚¿
        holidays : pd.DataFrame
            ç¥æ—¥ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
        
        Returns
        -------
        dict
            ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«äºˆæ¸¬çµæœ
        """
        # 2ãƒ¶æœˆå¾Œã®æ—¥æ•°ã‚’è¨ˆç®—
        max_date = df['ds'].max()
        future_end = max_date + relativedelta(months=2)
        horizon_days = (future_end - max_date).days
        
        logger.info(f"ğŸ¯ Training ensemble models (2-month forecast: {horizon_days} days)...")
        logger.info(f"  ğŸ“… Forecast period: {max_date.date()} â†’ {future_end.date()}")
        
        models = {}
        forecasts = {}
        
        # ------------------------------------------------------------------
        # ãƒ¢ãƒ‡ãƒ«1: æœ€é©åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¢ãƒ‡ãƒ«
        # ------------------------------------------------------------------
        logger.info("  ğŸ”§ Model 1: Optimized parameters")
        try:
            best_params = self.best_params if self.best_params else {
                'changepoint_prior_scale': 0.1,
                'seasonality_prior_scale': 10.0,
                'holidays_prior_scale': 20.0,
                'seasonality_mode': 'multiplicative'
            }
            
            model1 = Prophet(
                changepoint_prior_scale=best_params['changepoint_prior_scale'],
                seasonality_prior_scale=best_params['seasonality_prior_scale'],
                holidays_prior_scale=best_params['holidays_prior_scale'],
                seasonality_mode=best_params['seasonality_mode'],
                holidays=holidays,
                daily_seasonality=False,
                weekly_seasonality=False,
                yearly_seasonality=False,
                uncertainty_samples=1000
            )
            
            model1.add_seasonality(name='weekly', period=7, fourier_order=5)
            model1.add_seasonality(name='monthly', period=30.5, fourier_order=10)
            model1.add_seasonality(name='yearly', period=365.25, fourier_order=15)
            
            model1.fit(df[['ds', 'y']])
            
            # äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ä½œæˆ
            future1 = model1.make_future_dataframe(periods=horizon_days)
            forecast1 = model1.predict(future1)
            
            models['optimized'] = model1
            forecasts['optimized'] = forecast1
            
            logger.info("    âœ… Model 1 trained")
        except Exception as e:
            logger.error(f"    âŒ Model 1 failed: {e}")
        
        # ------------------------------------------------------------------
        # ãƒ¢ãƒ‡ãƒ«2: ä¿å®ˆçš„ãƒ¢ãƒ‡ãƒ«
        # ------------------------------------------------------------------
        logger.info("  ğŸ”§ Model 2: Conservative")
        try:
            model2 = Prophet(
                changepoint_prior_scale=0.01,
                seasonality_prior_scale=5.0,
                holidays_prior_scale=15.0,
                seasonality_mode='additive',
                holidays=holidays,
                daily_seasonality=False,
                weekly_seasonality=False,
                yearly_seasonality=False,
                uncertainty_samples=1000
            )
            
            model2.add_seasonality(name='weekly', period=7, fourier_order=3)
            model2.add_seasonality(name='monthly', period=30.5, fourier_order=5)
            model2.add_seasonality(name='yearly', period=365.25, fourier_order=10)
            
            model2.fit(df[['ds', 'y']])
            future2 = model2.make_future_dataframe(periods=horizon_days)
            forecast2 = model2.predict(future2)
            
            models['conservative'] = model2
            forecasts['conservative'] = forecast2
            
            logger.info("    âœ… Model 2 trained")
        except Exception as e:
            logger.error(f"    âŒ Model 2 failed: {e}")
        
        # ------------------------------------------------------------------
        # ãƒ¢ãƒ‡ãƒ«3: ã‚¢ã‚°ãƒ¬ãƒƒã‚·ãƒ–ãƒ¢ãƒ‡ãƒ«
        # ------------------------------------------------------------------
        logger.info("  ğŸ”§ Model 3: Aggressive")
        try:
            model3 = Prophet(
                changepoint_prior_scale=0.5,
                seasonality_prior_scale=20.0,
                holidays_prior_scale=30.0,
                seasonality_mode='multiplicative',
                holidays=holidays,
                daily_seasonality=False,
                weekly_seasonality=False,
                yearly_seasonality=False,
                uncertainty_samples=1000
            )
            
            model3.add_seasonality(name='weekly', period=7, fourier_order=10)
            model3.add_seasonality(name='monthly', period=30.5, fourier_order=15)
            model3.add_seasonality(name='yearly', period=365.25, fourier_order=20)
            
            model3.fit(df[['ds', 'y']])
            future3 = model3.make_future_dataframe(periods=horizon_days)
            forecast3 = model3.predict(future3)
            
            models['aggressive'] = model3
            forecasts['aggressive'] = forecast3
            
            logger.info("    âœ… Model 3 trained")
        except Exception as e:
            logger.error(f"    âŒ Model 3 failed: {e}")
        
        # ------------------------------------------------------------------
        # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ« (é‡ã¿ä»˜ãå¹³å‡)
        # ------------------------------------------------------------------
        logger.info("  ğŸ¯ Creating ensemble forecast...")
        
        # å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã§ã®æ€§èƒ½è©•ä¾¡
        train_maes = {}
        for name, forecast in forecasts.items():
            train_forecast = forecast[forecast['ds'].isin(df['ds'])]
            merged = pd.merge(df[['ds', 'y']], train_forecast[['ds', 'yhat']], on='ds')
            mae = mean_absolute_error(merged['y'], merged['yhat'])
            train_maes[name] = mae
            logger.info(f"    ğŸ“Š {name} MAE: {mae:.2f}")
        
        # é€†MAEã§é‡ã¿è¨ˆç®—
        weights = {name: 1.0 / mae for name, mae in train_maes.items()}
        total_weight = sum(weights.values())
        weights = {name: w / total_weight for name, w in weights.items()}
        
        logger.info(f"    âš–ï¸  Ensemble weights: {weights}")
        
        # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«äºˆæ¸¬
        ensemble_forecast = forecasts[list(forecasts.keys())[0]].copy()
        ensemble_forecast['yhat'] = sum(forecasts[name]['yhat'] * weights[name] 
                                        for name in forecasts.keys())
        ensemble_forecast['yhat_lower'] = sum(forecasts[name]['yhat_lower'] * weights[name] 
                                               for name in forecasts.keys())
        ensemble_forecast['yhat_upper'] = sum(forecasts[name]['yhat_upper'] * weights[name] 
                                               for name in forecasts.keys())
        
        self.models = models
        self.forecasts = forecasts
        self.ensemble_forecast = ensemble_forecast
        
        logger.info("âœ… Ensemble models trained")
        
        return {
            'models': models,
            'forecasts': forecasts,
            'ensemble': ensemble_forecast,
            'weights': weights
        }
    
    # ========================================================================
    # 7. è©³ç´°æ¤œè¨¼ (1ãƒ¶æœˆç›®/2ãƒ¶æœˆç›®/2ãƒ¶æœˆé–“)
    # ========================================================================
    def validate_forecast(self) -> Dict:
        """
        æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã§äºˆæ¸¬æ€§èƒ½ã‚’è©•ä¾¡
        - 1ãƒ¶æœˆç›®ã® RMSE/MAE/MAPE
        - 2ãƒ¶æœˆç›®ã® RMSE/MAE/MAPE
        - 2ãƒ¶æœˆé–“ã® RMSE/MAE/MAPE
        
        Returns
        -------
        dict
            æ¤œè¨¼çµæœ
        """
        logger.info("ğŸ” Validating forecast on holdout data...")
        
        if self.df_validation is None or len(self.df_validation) == 0:
            logger.warning("âš ï¸  No validation data available")
            return {}
        
        # æ¤œè¨¼æœŸé–“ã®äºˆæ¸¬å€¤ã‚’æŠ½å‡º
        forecast_val = self.ensemble_forecast[
            self.ensemble_forecast['ds'].isin(self.df_validation['ds'])
        ].copy()
        
        # ãƒãƒ¼ã‚¸
        merged = pd.merge(
            self.df_validation[['ds', 'y']], 
            forecast_val[['ds', 'yhat']], 
            on='ds'
        )
        
        if len(merged) == 0:
            logger.warning("âš ï¸  No matching dates in validation period")
            return {}
        
        # æœˆåˆ¥ã«åˆ†å‰²
        merged['year_month'] = merged['ds'].dt.to_period('M')
        months = sorted(merged['year_month'].unique())
        
        validation_metrics = {}
        
        # ------------------------------------------------------------------
        # 1ãƒ¶æœˆç›®ã®è©•ä¾¡
        # ------------------------------------------------------------------
        if len(months) >= 1:
            month1_data = merged[merged['year_month'] == months[0]]
            y_true_m1 = month1_data['y'].values
            y_pred_m1 = month1_data['yhat'].values
            
            rmse_m1 = np.sqrt(mean_squared_error(y_true_m1, y_pred_m1))
            mae_m1 = mean_absolute_error(y_true_m1, y_pred_m1)
            mape_m1 = np.mean(np.abs((y_true_m1 - y_pred_m1) / y_true_m1)) * 100
            
            validation_metrics['month_1'] = {
                'period': str(months[0]),
                'days': len(month1_data),
                'rmse': float(rmse_m1),
                'mae': float(mae_m1),
                'mape': float(mape_m1)
            }
            
            logger.info(f"  ğŸ“Š Month 1 ({months[0]}): RMSE={rmse_m1:.2f}, MAE={mae_m1:.2f}, MAPE={mape_m1:.2f}%")
        
        # ------------------------------------------------------------------
        # 2ãƒ¶æœˆç›®ã®è©•ä¾¡
        # ------------------------------------------------------------------
        if len(months) >= 2:
            month2_data = merged[merged['year_month'] == months[1]]
            y_true_m2 = month2_data['y'].values
            y_pred_m2 = month2_data['yhat'].values
            
            rmse_m2 = np.sqrt(mean_squared_error(y_true_m2, y_pred_m2))
            mae_m2 = mean_absolute_error(y_true_m2, y_pred_m2)
            mape_m2 = np.mean(np.abs((y_true_m2 - y_pred_m2) / y_true_m2)) * 100
            
            validation_metrics['month_2'] = {
                'period': str(months[1]),
                'days': len(month2_data),
                'rmse': float(rmse_m2),
                'mae': float(mae_m2),
                'mape': float(mape_m2)
            }
            
            logger.info(f"  ğŸ“Š Month 2 ({months[1]}): RMSE={rmse_m2:.2f}, MAE={mae_m2:.2f}, MAPE={mape_m2:.2f}%")
        
        # ------------------------------------------------------------------
        # 2ãƒ¶æœˆé–“å…¨ä½“ã®è©•ä¾¡
        # ------------------------------------------------------------------
        y_true_all = merged['y'].values
        y_pred_all = merged['yhat'].values
        
        rmse_all = np.sqrt(mean_squared_error(y_true_all, y_pred_all))
        mae_all = mean_absolute_error(y_true_all, y_pred_all)
        mape_all = np.mean(np.abs((y_true_all - y_pred_all) / y_true_all)) * 100
        
        validation_metrics['overall'] = {
            'period': f"{months[0]} to {months[-1]}" if len(months) > 1 else str(months[0]),
            'days': len(merged),
            'rmse': float(rmse_all),
            'mae': float(mae_all),
            'mape': float(mape_all)
        }
        
        logger.info(f"  ğŸ“Š Overall: RMSE={rmse_all:.2f}, MAE={mae_all:.2f}, MAPE={mape_all:.2f}%")
        
        self.validation_metrics = validation_metrics
        
        # çµæœä¿å­˜
        with open(self.output_dir / 'validation_metrics.json', 'w', encoding='utf-8') as f:
            json.dump(validation_metrics, f, indent=2, ensure_ascii=False)
        
        logger.info(f"âœ… Validation completed. Saved to {self.output_dir / 'validation_metrics.json'}")
        
        return validation_metrics
    
    # ========================================================================
    # 8. å¯è¦–åŒ–
    # ========================================================================
    def create_visualizations(self):
        """
        åŒ…æ‹¬çš„ãªå¯è¦–åŒ–ã‚’ä½œæˆ (æ¤œè¨¼çµæœå«ã‚€)
        """
        logger.info("ğŸ“Š Creating comprehensive visualizations...")
        
        fig = plt.figure(figsize=(24, 20))
        gs = fig.add_gridspec(5, 3, hspace=0.4, wspace=0.3)
        
        # ------------------------------------------------------------------
        # 1. æ™‚ç³»åˆ— + äºˆæ¸¬ + æ¤œè¨¼
        # ------------------------------------------------------------------
        ax1 = fig.add_subplot(gs[0, :])
        
        # å­¦ç¿’ãƒ‡ãƒ¼ã‚¿
        ax1.plot(self.df_train['ds'], self.df_train['y'], 
                label='Training Data', linewidth=1, alpha=0.7, color='blue')
        
        # æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿
        if self.df_validation is not None:
            ax1.plot(self.df_validation['ds'], self.df_validation['y'], 
                    label='Validation Data (Actual)', linewidth=1.5, alpha=0.9, 
                    color='green', marker='o', markersize=3)
        
        # äºˆæ¸¬
        if self.ensemble_forecast is not None:
            forecast = self.ensemble_forecast
            
            # äºˆæ¸¬æœŸé–“ã®ã¿
            forecast_future = forecast[forecast['ds'] > self.df_train['ds'].max()]
            
            ax1.plot(forecast_future['ds'], forecast_future['yhat'], 
                    'r-', label='Forecast', linewidth=2)
            ax1.fill_between(forecast_future['ds'], 
                            forecast_future['yhat_lower'], 
                            forecast_future['yhat_upper'], 
                            alpha=0.2, color='red', label='Uncertainty')
        
        ax1.set_title('Time Series Forecast with Validation', fontsize=14, fontweight='bold')
        ax1.set_xlabel('Date')
        ax1.set_ylabel('Call Volume')
        ax1.legend()
        ax1.grid(alpha=0.3)
        
        # ------------------------------------------------------------------
        # 2. æ¤œè¨¼æœŸé–“ã®æ‹¡å¤§å›³
        # ------------------------------------------------------------------
        ax2 = fig.add_subplot(gs[1, :])
        
        if self.df_validation is not None and self.ensemble_forecast is not None:
            # æ¤œè¨¼æœŸé–“ + å‰å¾Œ1é€±é–“
            val_start = self.df_validation['ds'].min() - timedelta(days=7)
            val_end = self.df_validation['ds'].max() + timedelta(days=7)
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚£ãƒ«ã‚¿
            df_plot = pd.concat([self.df_train, self.df_validation])
            df_plot = df_plot[(df_plot['ds'] >= val_start) & (df_plot['ds'] <= val_end)]
            
            forecast_plot = self.ensemble_forecast[
                (self.ensemble_forecast['ds'] >= val_start) & 
                (self.ensemble_forecast['ds'] <= val_end)
            ]
            
            # ãƒ—ãƒ­ãƒƒãƒˆ
            ax2.plot(df_plot['ds'], df_plot['y'], 
                    label='Actual', linewidth=1.5, alpha=0.8, color='black', marker='o', markersize=4)
            ax2.plot(forecast_plot['ds'], forecast_plot['yhat'], 
                    'r-', label='Forecast', linewidth=2)
            ax2.fill_between(forecast_plot['ds'], 
                            forecast_plot['yhat_lower'], 
                            forecast_plot['yhat_upper'], 
                            alpha=0.2, color='red')
            
            # æ¤œè¨¼æœŸé–“ã‚’å¼·èª¿
            ax2.axvspan(self.df_validation['ds'].min(), 
                       self.df_validation['ds'].max(), 
                       alpha=0.1, color='yellow', label='Validation Period')
            
            ax2.set_title('Validation Period Closeup', fontsize=14, fontweight='bold')
            ax2.set_xlabel('Date')
            ax2.set_ylabel('Call Volume')
            ax2.legend()
            ax2.grid(alpha=0.3)
        
        # ------------------------------------------------------------------
        # 3. æœˆåˆ¥èª¤å·®
        # ------------------------------------------------------------------
        ax3 = fig.add_subplot(gs[2, 0])
        
        if self.validation_metrics:
            metrics_list = []
            for key in ['month_1', 'month_2']:
                if key in self.validation_metrics:
                    metrics_list.append({
                        'Month': self.validation_metrics[key]['period'],
                        'MAE': self.validation_metrics[key]['mae'],
                        'RMSE': self.validation_metrics[key]['rmse'],
                        'MAPE': self.validation_metrics[key]['mape']
                    })
            
            if metrics_list:
                metrics_df = pd.DataFrame(metrics_list)
                x = np.arange(len(metrics_df))
                width = 0.25
                
                ax3.bar(x - width, metrics_df['MAE'], width, label='MAE', alpha=0.8)
                ax3.bar(x, metrics_df['RMSE'], width, label='RMSE', alpha=0.8)
                ax3.bar(x + width, metrics_df['MAPE']*10, width, label='MAPEÃ—10', alpha=0.8)
                
                ax3.set_xlabel('Month')
                ax3.set_ylabel('Error')
                ax3.set_title('Monthly Validation Metrics', fontsize=12, fontweight='bold')
                ax3.set_xticks(x)
                ax3.set_xticklabels(metrics_df['Month'])
                ax3.legend()
                ax3.grid(alpha=0.3)
        
        # ------------------------------------------------------------------
        # 4. æ®‹å·®ãƒ—ãƒ­ãƒƒãƒˆ
        # ------------------------------------------------------------------
        ax4 = fig.add_subplot(gs[2, 1])
        
        if self.df_validation is not None and self.ensemble_forecast is not None:
            forecast_val = self.ensemble_forecast[
                self.ensemble_forecast['ds'].isin(self.df_validation['ds'])
            ]
            merged = pd.merge(self.df_validation[['ds', 'y']], 
                            forecast_val[['ds', 'yhat']], on='ds')
            
            if len(merged) > 0:
                residuals = merged['y'] - merged['yhat']
                
                ax4.scatter(merged['yhat'], residuals, alpha=0.6, s=30)
                ax4.axhline(0, color='red', linestyle='--', linewidth=2)
                ax4.set_xlabel('Predicted')
                ax4.set_ylabel('Residual')
                ax4.set_title('Residual Plot', fontsize=12, fontweight='bold')
                ax4.grid(alpha=0.3)
        
        # ------------------------------------------------------------------
        # 5. å®Ÿæ¸¬ vs äºˆæ¸¬
        # ------------------------------------------------------------------
        ax5 = fig.add_subplot(gs[2, 2])
        
        if self.df_validation is not None and self.ensemble_forecast is not None:
            forecast_val = self.ensemble_forecast[
                self.ensemble_forecast['ds'].isin(self.df_validation['ds'])
            ]
            merged = pd.merge(self.df_validation[['ds', 'y']], 
                            forecast_val[['ds', 'yhat']], on='ds')
            
            if len(merged) > 0:
                ax5.scatter(merged['y'], merged['yhat'], alpha=0.6, s=30)
                
                # å¯¾è§’ç·š
                min_val = min(merged['y'].min(), merged['yhat'].min())
                max_val = max(merged['y'].max(), merged['yhat'].max())
                ax5.plot([min_val, max_val], [min_val, max_val], 
                        'r--', linewidth=2, label='Perfect Prediction')
                
                ax5.set_xlabel('Actual')
                ax5.set_ylabel('Predicted')
                ax5.set_title('Actual vs Predicted', fontsize=12, fontweight='bold')
                ax5.legend()
                ax5.grid(alpha=0.3)
        
        # ------------------------------------------------------------------
        # 6-8. è¨ºæ–­é–¢é€£ (å­¦ç¿’ãƒ‡ãƒ¼ã‚¿)
        # ------------------------------------------------------------------
        y = self.df_train['y'].values
        
        # åˆ†å¸ƒ
        ax6 = fig.add_subplot(gs[3, 0])
        ax6.hist(y, bins=50, alpha=0.7, edgecolor='black', density=True)
        from scipy.stats import gaussian_kde
        kde = gaussian_kde(y)
        x_grid = np.linspace(y.min(), y.max(), 200)
        ax6.plot(x_grid, kde(x_grid), 'r-', linewidth=2, label='KDE')
        ax6.set_title('Distribution', fontsize=12, fontweight='bold')
        ax6.set_xlabel('Call Volume')
        ax6.set_ylabel('Density')
        ax6.legend()
        ax6.grid(alpha=0.3)
        
        # QQ Plot
        ax7 = fig.add_subplot(gs[3, 1])
        stats.probplot(y, dist="norm", plot=ax7)
        ax7.set_title('Q-Q Plot', fontsize=12, fontweight='bold')
        ax7.grid(alpha=0.3)
        
        # Box Plot (æ›œæ—¥åˆ¥)
        ax8 = fig.add_subplot(gs[3, 2])
        df_train_copy = self.df_train.copy()
        df_train_copy['weekday'] = df_train_copy['ds'].dt.dayofweek
        weekday_data = [df_train_copy[df_train_copy['weekday'] == i]['y'].values for i in range(7)]
        ax8.boxplot(weekday_data, labels=['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'])
        ax8.set_title('Box Plot by Weekday', fontsize=12, fontweight='bold')
        ax8.set_ylabel('Call Volume')
        ax8.grid(alpha=0.3)
        
        # ------------------------------------------------------------------
        # 9-11. ACF, PACF, ã‚¹ãƒšã‚¯ãƒˆãƒ«
        # ------------------------------------------------------------------
        ax9 = fig.add_subplot(gs[4, 0])
        plot_acf(y, lags=30, ax=ax9)
        ax9.set_title('ACF', fontsize=12, fontweight='bold')
        
        ax10 = fig.add_subplot(gs[4, 1])
        plot_pacf(y, lags=30, ax=ax10)
        ax10.set_title('PACF', fontsize=12, fontweight='bold')
        
        ax11 = fig.add_subplot(gs[4, 2])
        freqs, psd = signal.periodogram(y, fs=1.0)
        ax11.semilogy(freqs[1:100], psd[1:100])
        ax11.set_title('Periodogram', fontsize=12, fontweight='bold')
        ax11.set_xlabel('Frequency (1/day)')
        ax11.set_ylabel('PSD')
        ax11.grid(alpha=0.3)
        
        plt.savefig(self.output_dir / 'comprehensive_visualizations.png', 
                   dpi=150, bbox_inches='tight')
        plt.close()
        
        logger.info(f"âœ… Visualizations saved to {self.output_dir / 'comprehensive_visualizations.png'}")
    
    # ========================================================================
    # 9. ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    # ========================================================================
    def generate_report(self):
        """
        è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ (æ¤œè¨¼çµæœå«ã‚€)
        """
        logger.info("ğŸ“ Generating comprehensive report...")
        
        report = []
        report.append("=" * 80)
        report.append("Prophet Ultimate Predictor v2.1 - Comprehensive Report")
        report.append("=" * 80)
        report.append(f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append("")
        
        # ------------------------------------------------------------------
        # 1. ãƒ‡ãƒ¼ã‚¿æƒ…å ±
        # ------------------------------------------------------------------
        report.append("1. DATA INFORMATION")
        report.append("-" * 80)
        report.append(f"  Full period: {self.df['ds'].min()} to {self.df['ds'].max()}")
        report.append(f"  Total days: {len(self.df)}")
        report.append("")
        report.append(f"  Training period: {self.df_train['ds'].min()} to {self.df_train['ds'].max()}")
        report.append(f"  Training days: {len(self.df_train)}")
        report.append(f"  Mean: {self.df_train['y'].mean():.1f}, Std: {self.df_train['y'].std():.1f}, CV: {self.df_train['y'].std() / self.df_train['y'].mean():.3f}")
        report.append("")
        
        if self.df_validation is not None:
            report.append(f"  Validation period: {self.df_validation['ds'].min()} to {self.df_validation['ds'].max()}")
            report.append(f"  Validation days: {len(self.df_validation)}")
            val_months = self.df_validation.groupby(self.df_validation['ds'].dt.to_period('M')).size()
            report.append(f"  Validation months: {', '.join(str(m) for m in val_months.index)}")
        report.append("")
        
        # ------------------------------------------------------------------
        # 2. è¨ºæ–­çµæœ
        # ------------------------------------------------------------------
        report.append("2. DIAGNOSTICS (Training Data)")
        report.append("-" * 80)
        
        if self.diagnostics:
            if 'basic_stats' in self.diagnostics:
                stats = self.diagnostics['basic_stats']
                report.append(f"  Mean: {stats['mean']:.1f}, Std: {stats['std']:.1f}, CV: {stats['cv']:.3f}")
                report.append(f"  Skewness: {stats['skewness']:.3f}, Kurtosis: {stats['kurtosis']:.3f}")
            
            if 'normality' in self.diagnostics:
                norm = self.diagnostics['normality']
                report.append(f"  Normality: {'Normal' if norm.get('is_normal') else 'Non-normal'}")
            
            if 'stationarity' in self.diagnostics:
                stat = self.diagnostics['stationarity']
                report.append(f"  Stationarity: {'Stationary' if stat.get('is_stationary') else 'Non-stationary'}")
            
            if 'weekday_effect' in self.diagnostics:
                week = self.diagnostics['weekday_effect']
                report.append(f"  Weekday effect: {'Significant' if week.get('has_effect') else 'Not significant'}")
            
            if 'outliers' in self.diagnostics:
                out = self.diagnostics['outliers']
                report.append(f"  Outliers: {out.get('count', 0)} ({out.get('percentage', 0):.2f}%)")
            
            if 'bimodality' in self.diagnostics:
                bi = self.diagnostics['bimodality']
                report.append(f"  Distribution: {'Bimodal' if bi.get('is_bimodal') else 'Unimodal'}")
        
        report.append("")
        
        # ------------------------------------------------------------------
        # 3. æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        # ------------------------------------------------------------------
        if self.best_params:
            report.append("3. OPTIMIZED HYPERPARAMETERS")
            report.append("-" * 80)
            for key, value in self.best_params.items():
                report.append(f"  {key}: {value}")
            report.append("")
        
        # ------------------------------------------------------------------
        # 4. æ¤œè¨¼çµæœ
        # ------------------------------------------------------------------
        if self.validation_metrics:
            report.append("4. VALIDATION RESULTS")
            report.append("-" * 80)
            
            if 'month_1' in self.validation_metrics:
                m1 = self.validation_metrics['month_1']
                report.append(f"  Month 1 ({m1['period']}):")
                report.append(f"    Days: {m1['days']}")
                report.append(f"    RMSE: {m1['rmse']:.2f}")
                report.append(f"    MAE:  {m1['mae']:.2f}")
                report.append(f"    MAPE: {m1['mape']:.2f}%")
                report.append("")
            
            if 'month_2' in self.validation_metrics:
                m2 = self.validation_metrics['month_2']
                report.append(f"  Month 2 ({m2['period']}):")
                report.append(f"    Days: {m2['days']}")
                report.append(f"    RMSE: {m2['rmse']:.2f}")
                report.append(f"    MAE:  {m2['mae']:.2f}")
                report.append(f"    MAPE: {m2['mape']:.2f}%")
                report.append("")
            
            if 'overall' in self.validation_metrics:
                overall = self.validation_metrics['overall']
                report.append(f"  Overall ({overall['period']}):")
                report.append(f"    Days: {overall['days']}")
                report.append(f"    RMSE: {overall['rmse']:.2f}")
                report.append(f"    MAE:  {overall['mae']:.2f}")
                report.append(f"    MAPE: {overall['mape']:.2f}%")
        
        report.append("")
        
        # ------------------------------------------------------------------
        # 5. æ¨å¥¨äº‹é …
        # ------------------------------------------------------------------
        report.append("5. RECOMMENDATIONS")
        report.append("-" * 80)
        
        cv = self.diagnostics.get('basic_stats', {}).get('cv', 0)
        if cv > 0.5:
            report.append("  âš ï¸  High variability detected (CV > 0.5)")
            report.append("     â†’ Consider regime-separated models")
        
        if self.diagnostics.get('bimodality', {}).get('is_bimodal'):
            report.append("  âš ï¸  Bimodal distribution detected")
            report.append("     â†’ Investigate weekday/weekend split")
        
        if self.diagnostics.get('outliers', {}).get('percentage', 0) > 5:
            report.append("  âš ï¸  High outlier percentage (>5%)")
            report.append("     â†’ Review outlier handling strategy")
        
        if self.validation_metrics:
            overall_mape = self.validation_metrics.get('overall', {}).get('mape', 0)
            if overall_mape > 20:
                report.append("  âš ï¸  High MAPE (>20%)")
                report.append("     â†’ Consider additional features or longer training period")
        
        report.append("")
        report.append("=" * 80)
        
        # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
        report_text = "\n".join(report)
        with open(self.output_dir / 'report.txt', 'w', encoding='utf-8') as f:
            f.write(report_text)
        
        logger.info(f"âœ… Report saved to {self.output_dir / 'report.txt'}")
        
        return report_text
    
    # ========================================================================
    # 10. ãƒ¢ãƒ‡ãƒ«ä¿å­˜ãƒ»ãƒ­ãƒ¼ãƒ‰
    # ========================================================================
    def save_models(self, filepath: str = None):
        """ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜"""
        if filepath is None:
            filepath = self.output_dir / 'models.pkl'
        
        save_obj = {
            'models': self.models,
            'ensemble_forecast': self.ensemble_forecast,
            'best_params': self.best_params,
            'diagnostics': self.diagnostics,
            'regimes': self.regimes,
            'validation_metrics': self.validation_metrics
        }
        
        with open(filepath, 'wb') as f:
            pickle.dump(save_obj, f)
        
        logger.info(f"âœ… Models saved to {filepath}")
    
    def load_models(self, filepath: str):
        """ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰"""
        with open(filepath, 'rb') as f:
            save_obj = pickle.load(f)
        
        self.models = save_obj.get('models', {})
        self.ensemble_forecast = save_obj.get('ensemble_forecast')
        self.best_params = save_obj.get('best_params', {})
        self.diagnostics = save_obj.get('diagnostics', {})
        self.regimes = save_obj.get('regimes', {})
        self.validation_metrics = save_obj.get('validation_metrics', {})
        
        logger.info(f"âœ… Models loaded from {filepath}")
    
    # ========================================================================
    # 11. å®Œå…¨å®Ÿè¡Œãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
    # ========================================================================
    def fit_predict(self, filepath: str, validation_months: int = 2, 
                    quick_mode: bool = True) -> Dict:
        """
        å®Œå…¨å®Ÿè¡Œãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ (2ãƒ¶æœˆå›ºå®šäºˆæ¸¬ + æ¤œè¨¼)
        
        Parameters
        ----------
        filepath : str
            CSVãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        validation_months : int
            æ¤œè¨¼æœŸé–“ (æœˆæ•°ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 2)
        quick_mode : bool
            é«˜é€Ÿãƒ¢ãƒ¼ãƒ‰
        
        Returns
        -------
        dict
            å…¨çµæœã‚’å«ã‚€è¾æ›¸
        """
        logger.info("ğŸš€ Starting Prophet Ultimate Predictor v2.1 pipeline...")
        
        # 1. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ (æ¤œè¨¼åˆ†å‰²)
        self.load_data(filepath, validation_months=validation_months)
        
        # 2. è¨ºæ–­ (å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®ã¿)
        self.run_comprehensive_diagnostics()
        
        # 3. ãƒ¬ã‚¸ãƒ¼ãƒ æ¤œå‡º (é«˜å¤‰å‹•æ™‚ã®ã¿)
        cv = self.diagnostics.get('basic_stats', {}).get('cv', 0)
        if cv > 0.4 or self.diagnostics.get('bimodality', {}).get('is_bimodal'):
            logger.info("ğŸ¯ High variability or bimodality detected. Running regime detection...")
            self.detect_regimes(n_regimes=2, method='kmeans')
        
        # 4. ç¥æ—¥ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ä½œæˆ
        holidays = self.create_holiday_dataframe()
        
        # 5. ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–
        self.optimize_hyperparameters(
            self.df_train, 
            holidays=holidays, 
            quick_mode=quick_mode
        )
        
        # 6. ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«è¨“ç·´ (2ãƒ¶æœˆå›ºå®š)
        self.fit_ensemble_models(self.df_train, holidays=holidays)
        
        # 7. æ¤œè¨¼
        self.validate_forecast()
        
        # 8. å¯è¦–åŒ–
        self.create_visualizations()
        
        # 9. ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        report = self.generate_report()
        
        # 10. ãƒ¢ãƒ‡ãƒ«ä¿å­˜
        self.save_models()
        
        # 11. äºˆæ¸¬çµæœã‚’CSVä¿å­˜
        if self.ensemble_forecast is not None:
            # äºˆæ¸¬æœŸé–“ã®ã¿ (å­¦ç¿’æœŸé–“ã®æœ€å¾Œã®æ—¥ã‚ˆã‚Šå¾Œ)
            forecast_df = self.ensemble_forecast[
                self.ensemble_forecast['ds'] > self.df_train['ds'].max()
            ][['ds', 'yhat', 'yhat_lower', 'yhat_upper']].copy()
            forecast_df.to_csv(self.output_dir / 'forecast.csv', index=False)
            logger.info(f"âœ… Forecast saved to {self.output_dir / 'forecast.csv'}")
        
        logger.info("=" * 80)
        logger.info("ğŸ‰ Pipeline completed successfully!")
        logger.info(f"ğŸ“ All results saved to: {self.output_dir}")
        logger.info("=" * 80)
        
        return {
            'diagnostics': self.diagnostics,
            'best_params': self.best_params,
            'models': self.models,
            'forecast': self.ensemble_forecast,
            'validation_metrics': self.validation_metrics,
            'report': report
        }


# ============================================================================
# ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ
# ============================================================================
if __name__ == '__main__':
    import argparse
    
    parser = argparse.ArgumentParser(
        description='Prophet Ultimate Predictor v2.1 for Call Center Forecasting',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # åŸºæœ¬å®Ÿè¡Œ (2ãƒ¶æœˆäºˆæ¸¬ + æœ€å¾Œã®2ãƒ¶æœˆã§æ¤œè¨¼)
  python prophet_ultimate_predictor.py data.csv
  
  # ã‚«ã‚¹ã‚¿ãƒ å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
  python prophet_ultimate_predictor.py data.csv --output my_results
        """
    )
    
    parser.add_argument('filepath', type=str, help='Path to CSV file (ds, y columns)')
    parser.add_argument('--validation-months', type=int, default=2, 
                        help='Validation period in months (default: 2)')
    parser.add_argument('--output', type=str, default='prophet_ultimate_results', 
                        help='Output directory (default: prophet_ultimate_results)')
    parser.add_argument('--quick', action='store_true', help='Quick mode')
    
    args = parser.parse_args()
    
    # å®Ÿè¡Œ
    predictor = ProphetUltimatePredictor(output_dir=args.output)
    results = predictor.fit_predict(
        args.filepath, 
        validation_months=args.validation_months,
        quick_mode=args.quick
    )
    
    print("\n" + "=" * 80)
    print("ğŸ“Š FINAL RESULTS")
    print("=" * 80)
    print(results['report'])
