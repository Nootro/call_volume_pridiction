#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
==============================================================================
NeuralProphet Ultimate Predictor for Call Center v5.0 - COMPLETE
==============================================================================

ã‚³ãƒ¼ãƒ«ã‚»ãƒ³ã‚¿ãƒ¼æ—¥æ¬¡å‘¼é‡äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ  v5.0 (NeuralProphetç‰ˆ) - å®Œå…¨å®Ÿè£…ç‰ˆ
- ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ™ãƒ¼ã‚¹ã®éç·šå½¢è‡ªå·±å›å¸° (AR-Net)
- è¶…é«˜ç²¾åº¦ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚° (100+ features)
- Optunaãƒ™ã‚¤ã‚ºæœ€é©åŒ– (30+ hyperparameters) âœ… å®Ÿè£…æ¸ˆã¿
- Quantile Lossæœ€é©åŒ– âœ… å®Ÿè£…æ¸ˆã¿
- ã‚·ãƒ•ãƒˆè¨ˆç”»ç‰¹åŒ–è©•ä¾¡æŒ‡æ¨™ (wQL, WAPE, MASE) âœ… å®Ÿè£…æ¸ˆã¿
- å®Œå…¨ãªå¯è¦–åŒ–ã¨ãƒ¬ãƒãƒ¼ãƒˆ âœ… å®Ÿè£…æ¸ˆã¿
- Jupyter Notebookå¯¾å¿œ âœ… å®Ÿè£…æ¸ˆã¿

ä¸»è¦æ©Ÿèƒ½:
---------
1. NeuralProphet AR-Net
   - è‡ªå‹•ãƒ©ã‚°é¸æŠ (1ã€œ365æ—¥)
   - éç·šå½¢ãƒ‘ã‚¿ãƒ¼ãƒ³å­¦ç¿’
   - è¨“ç·´å¯èƒ½ãª seasonality
2. è¶…é«˜ç²¾åº¦ç‰¹å¾´é‡
   - Lagged regressors: çŸ­æœŸã€œé•·æœŸãƒ©ã‚° (12ç¨®é¡)
   - Future regressors: æ›œæ—¥ã€æœˆã€ç¥æ—¥ã€ã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼ç‰¹å¾´ (50+)
   - Rolling features: rolling mean/std, EWM (20+)
   - Events: æ—¥æœ¬ã®ç¥æ—¥ã€ç‰¹æ®ŠæœŸé–“
3. è‡ªå‹•å¤‰æ›é¸æŠ
   - æ­£è¦æ€§æ¤œå®š (5ç¨®é¡)
   - Box-Cox, Yeo-Johnson, log, sqrt, reciprocal
4. Optunaæœ€é©åŒ– âœ…
   - 30+ ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
   - Quantile loss (QL_60, QL_70)
   - æ™‚ç³»åˆ—CV
5. ã‚·ãƒ•ãƒˆè¨ˆç”»ç‰¹åŒ–è©•ä¾¡ âœ…
   - wQL, WAPE, MASE
   - Peak day accuracy
   - Bias analysis
6. åŒ…æ‹¬çš„å¯è¦–åŒ–ã¨ãƒ¬ãƒãƒ¼ãƒˆ âœ…

ä½¿ç”¨ä¾‹ (Jupyter):
-----------------
from neuralprophet_ultimate_predictor_full import NeuralProphetUltimatePredictor

# åˆæœŸåŒ–
predictor = NeuralProphetUltimatePredictor(
    validation_months=2,
    optuna_trials=100,
    target_quantile=0.6
)

# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
df = predictor.load_data('data.csv', date_col='date', value_col='y')

# è‡ªå‹•å¤‰æ›é¸æŠ
df_transformed = predictor.select_optimal_transformation(df)

# ç‰¹å¾´é‡ç”Ÿæˆ
df_features = predictor.generate_comprehensive_features(df_transformed)

# è¨“ç·´ãƒ»æ¤œè¨¼åˆ†å‰²
train_df, val_df = predictor.split_train_validation(df_features)

# Optunaæœ€é©åŒ– + è¨“ç·´
best_model, best_params = predictor.optimize_and_train(train_df, val_df)

# äºˆæ¸¬
forecast_df = predictor.predict(best_model, periods=60, include_history=True)

# è©•ä¾¡
metrics = predictor.evaluate(val_df, forecast_df)

# å¯è¦–åŒ–
predictor.plot_forecast(forecast_df, val_df)
predictor.plot_components(best_model, forecast_df)
predictor.plot_metrics(metrics)

# ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
predictor.generate_report(metrics, best_params, output_path='report.html')

ä½œæˆè€…: AI Assistant
ãƒãƒ¼ã‚¸ãƒ§ãƒ³: 5.0
æœ€çµ‚æ›´æ–°: 2026-02-20
ãƒ©ã‚¤ã‚»ãƒ³ã‚¹: MIT
"""

import warnings
warnings.filterwarnings('ignore')

import pandas as pd
import numpy as np
import matplotlib
matplotlib.use('Agg')  # GUIä¸è¦
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta
from dateutil.relativedelta import relativedelta
import calendar
import json
import pickle
import logging
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Union
import sys
import argparse
from scipy import stats
from scipy.special import inv_boxcox
from sklearn.preprocessing import PowerTransformer
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# NeuralProphet
try:
    from neuralprophet import NeuralProphet, set_log_level
    set_log_level("ERROR")  # ãƒ­ã‚°æŠ‘åˆ¶
except ImportError:
    print("âŒ NeuralProphet not installed.")
    print("Run: pip install neuralprophet")
    print("Or: pip install neuralprophet[live]  # for live plotting")
    sys.exit(1)

# Optuna
try:
    import optuna
    optuna.logging.set_verbosity(optuna.logging.WARNING)
except ImportError:
    print("âŒ Optuna not installed. Run: pip install optuna")
    sys.exit(1)

# PyTorch (NeuralProphetã®ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰)
try:
    import torch
    if torch.cuda.is_available():
        print(f"âœ… GPUæ¤œå‡º: {torch.cuda.get_device_name(0)}")
        DEVICE = 'cuda'
    else:
        print("âœ… CPU ãƒ¢ãƒ¼ãƒ‰")
        DEVICE = 'cpu'
except ImportError:
    print("âŒ PyTorch not installed. Run: pip install torch")
    sys.exit(1)

# jpholiday (æ—¥æœ¬ã®ç¥æ—¥)
try:
    import jpholiday
    JPHOLIDAY_AVAILABLE = True
    print("âœ… jpholiday ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ¸ˆã¿")
except ImportError:
    JPHOLIDAY_AVAILABLE = False
    print("âš ï¸  jpholiday æœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« (ã‚¤ãƒ™ãƒ³ãƒˆç‰¹å¾´ãªã—)")
    print("   æ¨å¥¨: pip install jpholiday")

# Jupyterè¡¨ç¤ºè¨­å®š
try:
    from IPython.display import display, HTML
    JUPYTER_MODE = True
except ImportError:
    JUPYTER_MODE = False

# ãƒ—ãƒ­ãƒƒãƒˆè¨­å®š
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")
plt.rcParams['figure.figsize'] = (14, 8)
plt.rcParams['font.size'] = 10
plt.rcParams['axes.titlesize'] = 12
plt.rcParams['axes.labelsize'] = 10

# ==============================================================================
# Main Predictor Class
# ==============================================================================

class NeuralProphetUltimatePredictor:
    """
    NeuralProphet Ultimate Predictor v5.0
    
    å®Œå…¨å®Ÿè£…ç‰ˆï¼šOptunaæœ€é©åŒ–ã€è¨“ç·´ã€è©•ä¾¡ã€å¯è¦–åŒ–ã™ã¹ã¦å«ã‚€
    """
    
    def __init__(
        self,
        validation_months: int = 2,
        optuna_trials: int = 100,
        target_quantile: float = 0.6,
        n_lags: Optional[int] = None,
        ar_layers: Optional[List[int]] = None,
        epochs: int = 100,
        batch_size: int = 32,
        learning_rate: float = None,
        log_dir: str = './logs',
        output_dir: str = './outputs'
    ):
        """
        åˆæœŸåŒ–
        
        Parameters
        ----------
        validation_months : int
            æ¤œè¨¼æœŸé–“ã®æœˆæ•° (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 2)
        optuna_trials : int
            Optunaè©¦è¡Œå›æ•° (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 100)
        target_quantile : float
            ç›®æ¨™åˆ†ä½ç‚¹ (0.5=ä¸­å¤®å€¤, 0.6=ã‚·ãƒ•ãƒˆæ¨å¥¨, 0.7=ä¿å®ˆçš„)
        n_lags : int, optional
            AR-Netãƒ©ã‚°æ•° (None=è‡ªå‹•)
        ar_layers : list, optional
            AR-Netå±¤æ§‹æˆ (None=è‡ªå‹•)
        epochs : int
            è¨“ç·´ã‚¨ãƒãƒƒã‚¯æ•°
        batch_size : int
            ãƒãƒƒãƒã‚µã‚¤ã‚º
        learning_rate : float, optional
            å­¦ç¿’ç‡ (None=è‡ªå‹•)
        log_dir : str
            ãƒ­ã‚°å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        output_dir : str
            çµæœå‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        """
        self.validation_months = validation_months
        self.optuna_trials = optuna_trials
        self.target_quantile = target_quantile
        self.n_lags = n_lags
        self.ar_layers = ar_layers
        self.epochs = epochs
        self.batch_size = batch_size
        self.learning_rate = learning_rate
        
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        self.log_dir = Path(log_dir)
        self.output_dir = Path(output_dir)
        self.log_dir.mkdir(parents=True, exist_ok=True)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # ãƒ­ã‚°è¨­å®š
        self.logger = self._setup_logger()
        
        # ç‰¹å¾´é‡åãƒªã‚¹ãƒˆ
        self.lagged_regressor_names = []
        self.future_regressor_names = []
        self.event_names = []
        
        # å¤‰æ›ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.transformation_type = None
        self.transformation_params = {}
        self.original_mean = None
        self.original_std = None
        
        # ãƒ¢ãƒ‡ãƒ«
        self.best_model = None
        self.best_params = None
        self.study = None
        
        self.logger.info("=" * 80)
        self.logger.info("NeuralProphet Ultimate Predictor v5.0 - åˆæœŸåŒ–å®Œäº†")
        self.logger.info("=" * 80)
        self.logger.info(f"æ¤œè¨¼æœŸé–“: {validation_months} ãƒ¶æœˆ")
        self.logger.info(f"Optunaè©¦è¡Œ: {optuna_trials} å›")
        self.logger.info(f"ç›®æ¨™åˆ†ä½ç‚¹: {target_quantile}")
        self.logger.info(f"ãƒ‡ãƒã‚¤ã‚¹: {DEVICE}")
        self.logger.info(f"jpholiday: {'âœ…' if JPHOLIDAY_AVAILABLE else 'âŒ'}")
        
    def _setup_logger(self) -> logging.Logger:
        """ãƒ­ã‚¬ãƒ¼è¨­å®š"""
        logger = logging.getLogger('NeuralProphet_Ultimate')
        logger.setLevel(logging.INFO)
        logger.handlers.clear()
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒ³ãƒ‰ãƒ©
        log_file = self.log_dir / f'neuralprophet_ultimate_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'
        fh = logging.FileHandler(log_file, encoding='utf-8')
        fh.setLevel(logging.INFO)
        
        # ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ãƒãƒ³ãƒ‰ãƒ©
        ch = logging.StreamHandler()
        ch.setLevel(logging.INFO)
        
        # ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            datefmt='%Y-%m-%d %H:%M:%S'
        )
        fh.setFormatter(formatter)
        ch.setFormatter(formatter)
        
        logger.addHandler(fh)
        logger.addHandler(ch)
        
        return logger
    
    def load_data(
        self,
        file_path: str,
        date_col: str = 'date',
        value_col: str = 'y',
        parse_dates: bool = True
    ) -> pd.DataFrame:
        """
        ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        
        Parameters
        ----------
        file_path : str
            CSVãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        date_col : str
            æ—¥ä»˜ã‚«ãƒ©ãƒ å
        value_col : str
            ç›®çš„å¤‰æ•°ã‚«ãƒ©ãƒ å
        parse_dates : bool
            æ—¥ä»˜ãƒ‘ãƒ¼ã‚¹
        
        Returns
        -------
        pd.DataFrame
            èª­ã¿è¾¼ã‚“ã ãƒ‡ãƒ¼ã‚¿ (ds, y ã‚«ãƒ©ãƒ )
        """
        self.logger.info("=" * 80)
        self.logger.info("ğŸ“‚ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿é–‹å§‹")
        self.logger.info("=" * 80)
        
        # CSVèª­ã¿è¾¼ã¿
        df = pd.read_csv(file_path)
        self.logger.info(f"  âœ“ ãƒ•ã‚¡ã‚¤ãƒ«: {file_path}")
        self.logger.info(f"  âœ“ è¡Œæ•°: {len(df):,}")
        self.logger.info(f"  âœ“ ã‚«ãƒ©ãƒ : {list(df.columns)}")
        
        # æ—¥ä»˜ã‚«ãƒ©ãƒ æ¤œå‡º
        if date_col not in df.columns:
            # è‡ªå‹•æ¤œå‡º
            date_cols = [col for col in df.columns if 'date' in col.lower() or 'time' in col.lower()]
            if date_cols:
                date_col = date_cols[0]
                self.logger.info(f"  âœ“ æ—¥ä»˜ã‚«ãƒ©ãƒ è‡ªå‹•æ¤œå‡º: {date_col}")
            else:
                raise ValueError(f"æ—¥ä»˜ã‚«ãƒ©ãƒ  '{date_col}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        
        # å€¤ã‚«ãƒ©ãƒ æ¤œå‡º
        if value_col not in df.columns:
            # è‡ªå‹•æ¤œå‡º (æ•°å€¤ã‚«ãƒ©ãƒ ã®æœ€åˆ)
            numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
            if numeric_cols:
                value_col = numeric_cols[0]
                self.logger.info(f"  âœ“ å€¤ã‚«ãƒ©ãƒ è‡ªå‹•æ¤œå‡º: {value_col}")
            else:
                raise ValueError(f"å€¤ã‚«ãƒ©ãƒ  '{value_col}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ä½œæˆ
        df_clean = pd.DataFrame({
            'ds': pd.to_datetime(df[date_col]) if parse_dates else df[date_col],
            'y': df[value_col]
        })
        
        # ã‚½ãƒ¼ãƒˆ
        df_clean = df_clean.sort_values('ds').reset_index(drop=True)
        
        # æ¬ æå€¤ç¢ºèª
        missing_count = df_clean['y'].isna().sum()
        if missing_count > 0:
            self.logger.warning(f"  âš ï¸  æ¬ æå€¤: {missing_count} å€‹ â†’ ç·šå½¢è£œé–“")
            df_clean['y'] = df_clean['y'].interpolate(method='linear')
        
        # çµ±è¨ˆæƒ…å ±
        self.logger.info(f"\nğŸ“Š åŸºæœ¬çµ±è¨ˆ:")
        self.logger.info(f"  æœŸé–“: {df_clean['ds'].min().date()} ã€œ {df_clean['ds'].max().date()}")
        self.logger.info(f"  æ—¥æ•°: {len(df_clean)} æ—¥")
        self.logger.info(f"  å¹³å‡: {df_clean['y'].mean():.2f}")
        self.logger.info(f"  æ¨™æº–åå·®: {df_clean['y'].std():.2f}")
        self.logger.info(f"  æœ€å°: {df_clean['y'].min():.2f}")
        self.logger.info(f"  æœ€å¤§: {df_clean['y'].max():.2f}")
        
        return df_clean
    
    def select_optimal_transformation(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        æœ€é©ãªå¤‰æ›ã‚’è‡ªå‹•é¸æŠ
        
        æ­£è¦æ€§æ¤œå®šã§æœ€ã‚‚æ­£è¦åˆ†å¸ƒã«è¿‘ã„å¤‰æ›ã‚’é¸æŠ
        
        Parameters
        ----------
        df : pd.DataFrame
            å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ (ds, y)
        
        Returns
        -------
        pd.DataFrame
            å¤‰æ›å¾Œãƒ‡ãƒ¼ã‚¿ (ds, y)
        """
        self.logger.info("=" * 80)
        self.logger.info("ğŸ”„ è‡ªå‹•å¤‰æ›é¸æŠé–‹å§‹")
        self.logger.info("=" * 80)
        
        y = df['y'].values
        self.original_mean = y.mean()
        self.original_std = y.std()
        
        transformations = {}
        
        # 1. å…ƒãƒ‡ãƒ¼ã‚¿
        _, p_original = stats.shapiro(y)
        transformations['none'] = {'data': y, 'p_value': p_original, 'params': {}}
        self.logger.info(f"  å…ƒãƒ‡ãƒ¼ã‚¿: Shapiro p={p_original:.4f}")
        
        # 2. Logå¤‰æ›
        if (y > 0).all():
            y_log = np.log(y)
            _, p_log = stats.shapiro(y_log)
            transformations['log'] = {'data': y_log, 'p_value': p_log, 'params': {}}
            self.logger.info(f"  Logå¤‰æ›: Shapiro p={p_log:.4f}")
        
        # 3. Sqrtå¤‰æ›
        if (y >= 0).all():
            y_sqrt = np.sqrt(y)
            _, p_sqrt = stats.shapiro(y_sqrt)
            transformations['sqrt'] = {'data': y_sqrt, 'p_value': p_sqrt, 'params': {}}
            self.logger.info(f"  Sqrtå¤‰æ›: Shapiro p={p_sqrt:.4f}")
        
        # 4. Box-Coxå¤‰æ›
        if (y > 0).all():
            y_boxcox, lambda_boxcox = stats.boxcox(y)
            _, p_boxcox = stats.shapiro(y_boxcox)
            transformations['boxcox'] = {
                'data': y_boxcox,
                'p_value': p_boxcox,
                'params': {'lambda': lambda_boxcox}
            }
            self.logger.info(f"  Box-Coxå¤‰æ›: Shapiro p={p_boxcox:.4f}, Î»={lambda_boxcox:.4f}")
        
        # 5. Yeo-Johnsonå¤‰æ›
        pt = PowerTransformer(method='yeo-johnson', standardize=True)
        y_yj = pt.fit_transform(y.reshape(-1, 1)).flatten()
        _, p_yj = stats.shapiro(y_yj)
        transformations['yeo-johnson'] = {
            'data': y_yj,
            'p_value': p_yj,
            'params': {'transformer': pt}
        }
        self.logger.info(f"  Yeo-Johnsonå¤‰æ›: Shapiro p={p_yj:.4f}")
        
        # æœ€é©å¤‰æ›é¸æŠ (på€¤æœ€å¤§ = æœ€ã‚‚æ­£è¦åˆ†å¸ƒã«è¿‘ã„)
        best_transform = max(transformations.items(), key=lambda x: x[1]['p_value'])
        self.transformation_type = best_transform[0]
        self.transformation_params = best_transform[1]['params']
        
        self.logger.info(f"\nâœ… æœ€é©å¤‰æ›: {self.transformation_type.upper()} (p={best_transform[1]['p_value']:.4f})")
        
        # å¤‰æ›é©ç”¨
        df_transformed = df.copy()
        df_transformed['y'] = best_transform[1]['data']
        
        return df_transformed
    
    def inverse_transform(self, y_transformed: np.ndarray) -> np.ndarray:
        """
        é€†å¤‰æ›
        
        Parameters
        ----------
        y_transformed : np.ndarray
            å¤‰æ›å¾Œãƒ‡ãƒ¼ã‚¿
        
        Returns
        -------
        np.ndarray
            å…ƒã®ã‚¹ã‚±ãƒ¼ãƒ«ã«æˆ»ã—ãŸãƒ‡ãƒ¼ã‚¿
        """
        if self.transformation_type == 'none':
            return y_transformed
        elif self.transformation_type == 'log':
            return np.exp(y_transformed)
        elif self.transformation_type == 'sqrt':
            return y_transformed ** 2
        elif self.transformation_type == 'boxcox':
            lambda_val = self.transformation_params['lambda']
            return inv_boxcox(y_transformed, lambda_val)
        elif self.transformation_type == 'yeo-johnson':
            pt = self.transformation_params['transformer']
            return pt.inverse_transform(y_transformed.reshape(-1, 1)).flatten()
        else:
            return y_transformed
    
    def generate_comprehensive_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        åŒ…æ‹¬çš„ç‰¹å¾´é‡ç”Ÿæˆ
        
        100+ ç‰¹å¾´é‡ã‚’ç”Ÿæˆ
        
        Parameters
        ----------
        df : pd.DataFrame
            å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ (ds, y)
        
        Returns
        -------
        pd.DataFrame
            ç‰¹å¾´é‡è¿½åŠ ãƒ‡ãƒ¼ã‚¿
        """
        self.logger.info("=" * 80)
        self.logger.info("ğŸ”§ ç‰¹å¾´é‡ç”Ÿæˆé–‹å§‹")
        self.logger.info("=" * 80)
        
        df = df.copy()
        
        # åŸºæœ¬æ—¥ä»˜ç‰¹å¾´
        df = self._generate_basic_date_features(df)
        
        # Lagged regressors
        df = self._generate_lagged_regressors(df)
        
        # Rolling features
        df = self._generate_rolling_features(df)
        
        # Calendar features
        df = self._generate_calendar_features(df)
        
        # Cyclical features
        df = self._generate_cyclical_features(df)
        
        # Event features
        df = self._generate_event_features(df)
        
        # Trend features
        df = self._generate_trend_features(df)
        
        # æ¬ æå€¤å‡¦ç† (forward fill)
        df = df.fillna(method='ffill').fillna(method='bfill')
        
        self.logger.info("=" * 80)
        self.logger.info("âœ… ç‰¹å¾´é‡ç”Ÿæˆå®Œäº†")
        self.logger.info("=" * 80)
        self.logger.info(f"  Lagged regressors: {len(self.lagged_regressor_names)} å€‹")
        self.logger.info(f"  Future regressors: {len(self.future_regressor_names)} å€‹")
        self.logger.info(f"  Events: {len(self.event_names)} å€‹")
        self.logger.info(f"  åˆè¨ˆ: {len(self.lagged_regressor_names) + len(self.future_regressor_names) + len(self.event_names)} å€‹")
        
        return df
    
    def _generate_basic_date_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """åŸºæœ¬æ—¥ä»˜ç‰¹å¾´"""
        self.logger.info("ğŸ“… åŸºæœ¬æ—¥ä»˜ç‰¹å¾´ç”Ÿæˆä¸­...")
        
        df['year'] = df['ds'].dt.year
        df['month'] = df['ds'].dt.month
        df['day_of_month'] = df['ds'].dt.day
        df['dayofweek'] = df['ds'].dt.dayofweek
        df['quarter'] = df['ds'].dt.quarter
        df['day_of_year'] = df['ds'].dt.dayofyear
        df['week_of_year'] = df['ds'].dt.isocalendar().week.astype(int)
        df['is_weekend'] = (df['dayofweek'] >= 5).astype(int)
        df['is_month_start'] = df['ds'].dt.is_month_start.astype(int)
        df['is_month_end'] = df['ds'].dt.is_month_end.astype(int)
        df['is_quarter_start'] = df['ds'].dt.is_quarter_start.astype(int)
        df['is_quarter_end'] = df['ds'].dt.is_quarter_end.astype(int)
        df['is_year_start'] = ((df['month'] == 1) & (df['day_of_month'] == 1)).astype(int)
        df['is_year_end'] = ((df['month'] == 12) & (df['day_of_month'] == 31)).astype(int)
        df['days_in_month'] = df['ds'].dt.days_in_month
        
        self.logger.info(f"  âœ“ åŸºæœ¬æ—¥ä»˜ç‰¹å¾´: 15 å€‹")
        
        return df
    
    def _generate_lagged_regressors(self, df: pd.DataFrame) -> pd.DataFrame:
        """Lagged regressors"""
        self.logger.info("â±ï¸  Lagged regressors ç”Ÿæˆä¸­...")
        
        lag_days = [1, 2, 3, 7, 14, 21, 28, 30, 60, 90, 180, 365]
        
        for lag in lag_days:
            col_name = f'y_lag_{lag}'
            df[col_name] = df['y'].shift(lag)
            self.lagged_regressor_names.append(col_name)
        
        self.logger.info(f"  âœ“ Lagged regressors: {len(self.lagged_regressor_names)} å€‹")
        
        return df
    
    def _generate_rolling_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """Rolling features"""
        self.logger.info("ğŸ“Š Rolling features ç”Ÿæˆä¸­...")
        
        windows = [7, 14, 28]
        
        for window in windows:
            # Rolling mean
            col_name = f'y_rolling_mean_{window}'
            df[col_name] = df['y'].rolling(window=window, min_periods=1).mean()
            self.lagged_regressor_names.append(col_name)
            
            # Rolling std
            col_name = f'y_rolling_std_{window}'
            df[col_name] = df['y'].rolling(window=window, min_periods=1).std()
            self.lagged_regressor_names.append(col_name)
            
            # EWM
            col_name = f'y_ewm_{window}'
            df[col_name] = df['y'].ewm(span=window, min_periods=1).mean()
            self.lagged_regressor_names.append(col_name)
        
        self.logger.info(f"  âœ“ Rolling features: {len(windows) * 3} å€‹")
        
        return df
    
    def _generate_calendar_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """Calendar features"""
        self.logger.info("ğŸ“† Calendar features ç”Ÿæˆä¸­...")
        
        # One-hot encoding: æ›œæ—¥
        for dow in range(7):
            col_name = f'dow_{dow}'
            df[col_name] = (df['dayofweek'] == dow).astype(int)
            self.future_regressor_names.append(col_name)
        
        # One-hot encoding: æœˆ
        for month in range(1, 13):
            col_name = f'month_{month}'
            df[col_name] = (df['month'] == month).astype(int)
            self.future_regressor_names.append(col_name)
        
        # é€±ã®ä½ç½® (ç¬¬1é€±ã€œç¬¬5é€±)
        df['week_of_month'] = (df['day_of_month'] - 1) // 7 + 1
        for week in range(1, 6):
            col_name = f'week_of_month_{week}'
            df[col_name] = (df['week_of_month'] == week).astype(int)
            self.future_regressor_names.append(col_name)
        
        self.logger.info(f"  âœ“ Calendar features: {7 + 12 + 5} å€‹")
        
        return df
    
    def _generate_cyclical_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """Cyclical features"""
        self.logger.info("ğŸ”„ Cyclical features ç”Ÿæˆä¸­...")
        
        # æ›œæ—¥ (å‘¨æœŸ=7)
        df['dow_sin'] = np.sin(2 * np.pi * df['dayofweek'] / 7)
        df['dow_cos'] = np.cos(2 * np.pi * df['dayofweek'] / 7)
        self.future_regressor_names.extend(['dow_sin', 'dow_cos'])
        
        # æœˆ (å‘¨æœŸ=12)
        df['month_sin'] = np.sin(2 * np.pi * (df['month'] - 1) / 12)
        df['month_cos'] = np.cos(2 * np.pi * (df['month'] - 1) / 12)
        self.future_regressor_names.extend(['month_sin', 'month_cos'])
        
        # æœˆå†…æ—¥ (å‘¨æœŸ=31)
        df['day_sin'] = np.sin(2 * np.pi * (df['day_of_month'] - 1) / 31)
        df['day_cos'] = np.cos(2 * np.pi * (df['day_of_month'] - 1) / 31)
        self.future_regressor_names.extend(['day_sin', 'day_cos'])
        
        # å¹´å†…æ—¥ (å‘¨æœŸ=365)
        df['doy_sin'] = np.sin(2 * np.pi * (df['day_of_year'] - 1) / 365)
        df['doy_cos'] = np.cos(2 * np.pi * (df['day_of_year'] - 1) / 365)
        self.future_regressor_names.extend(['doy_sin', 'doy_cos'])
        
        # å››åŠæœŸ (å‘¨æœŸ=4)
        df['quarter_sin'] = np.sin(2 * np.pi * (df['quarter'] - 1) / 4)
        df['quarter_cos'] = np.cos(2 * np.pi * (df['quarter'] - 1) / 4)
        self.future_regressor_names.extend(['quarter_sin', 'quarter_cos'])
        
        self.logger.info(f"  âœ“ Cyclical features: 10 å€‹")
        
        return df
    
    def _generate_event_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """Event features"""
        self.logger.info("ğŸŒ Event features ç”Ÿæˆä¸­...")
        
        if not JPHOLIDAY_AVAILABLE:
            self.logger.warning("  âš ï¸  jpholidayæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« â†’ ã‚¤ãƒ™ãƒ³ãƒˆç‰¹å¾´ã‚¹ã‚­ãƒƒãƒ—")
            return df
        
        # ç¥æ—¥ãƒ•ãƒ©ã‚°
        df['is_holiday'] = df['ds'].apply(lambda x: jpholiday.is_holiday(x)).astype(int)
        self.event_names.append('is_holiday')
        
        # ã‚´ãƒ¼ãƒ«ãƒ‡ãƒ³ã‚¦ã‚£ãƒ¼ã‚¯
        df['is_golden_week'] = (
            ((df['month'] == 4) & (df['day_of_month'] >= 29)) |
            ((df['month'] == 5) & (df['day_of_month'] <= 5))
        ).astype(int)
        self.event_names.append('is_golden_week')
        
        # ãŠç›†
        df['is_obon'] = (
            (df['month'] == 8) & 
            (df['day_of_month'] >= 13) & 
            (df['day_of_month'] <= 16)
        ).astype(int)
        self.event_names.append('is_obon')
        
        # å¹´æœ«å¹´å§‹
        df['is_year_end_new_year'] = (
            ((df['month'] == 12) & (df['day_of_month'] >= 29)) |
            ((df['month'] == 1) & (df['day_of_month'] <= 3))
        ).astype(int)
        self.event_names.append('is_year_end_new_year')
        
        # ã‚·ãƒ«ãƒãƒ¼ã‚¦ã‚£ãƒ¼ã‚¯
        df['is_silver_week'] = (
            (df['month'] == 9) & 
            (df['day_of_month'] >= 15) & 
            (df['day_of_month'] <= 23) &
            (df['is_holiday'] == 1)
        ).astype(int)
        self.event_names.append('is_silver_week')
        
        # ç¥æ—¥å‰æ—¥ãƒ»ç¿Œæ—¥
        df['is_holiday_before'] = df['is_holiday'].shift(-1).fillna(0).astype(int)
        df['is_holiday_after'] = df['is_holiday'].shift(1).fillna(0).astype(int)
        self.event_names.extend(['is_holiday_before', 'is_holiday_after'])
        
        self.logger.info(f"  âœ“ Event features: {len(self.event_names)} å€‹")
        
        return df
    
    def _generate_trend_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """Trend features"""
        self.logger.info("ğŸ“ˆ Trend features ç”Ÿæˆä¸­...")
        
        df['t'] = np.arange(len(df))
        df['t_squared'] = df['t'] ** 2
        df['t_cubed'] = df['t'] ** 3
        df['t_normalized'] = df['t'] / (len(df) - 1)
        
        self.future_regressor_names.extend(['t', 't_squared', 't_cubed', 't_normalized'])
        
        self.logger.info(f"  âœ“ Trend features: 4 å€‹")
        
        return df
    
    def split_train_validation(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """
        è¨“ç·´ãƒ»æ¤œè¨¼åˆ†å‰²
        
        Parameters
        ----------
        df : pd.DataFrame
            å…¨ãƒ‡ãƒ¼ã‚¿
        
        Returns
        -------
        train_df, val_df : pd.DataFrame
            è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã€æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿
        """
        self.logger.info("=" * 80)
        self.logger.info("âœ‚ï¸  è¨“ç·´ãƒ»æ¤œè¨¼åˆ†å‰²")
        self.logger.info("=" * 80)
        
        # æ¤œè¨¼æœŸé–“ã®é–‹å§‹æ—¥
        val_start = df['ds'].max() - relativedelta(months=self.validation_months)
        
        train_df = df[df['ds'] < val_start].copy()
        val_df = df[df['ds'] >= val_start].copy()
        
        self.logger.info(f"  è¨“ç·´æœŸé–“: {train_df['ds'].min().date()} ã€œ {train_df['ds'].max().date()} ({len(train_df)} æ—¥)")
        self.logger.info(f"  æ¤œè¨¼æœŸé–“: {val_df['ds'].min().date()} ã€œ {val_df['ds'].max().date()} ({len(val_df)} æ—¥)")
        
        return train_df, val_df
    
    # ==============================================================================
    # Optunaæœ€é©åŒ– + è¨“ç·´
    # ==============================================================================
    
    def optimize_and_train(
        self,
        train_df: pd.DataFrame,
        val_df: pd.DataFrame
    ) -> Tuple[NeuralProphet, Dict]:
        """
        Optunaæœ€é©åŒ– + ãƒ¢ãƒ‡ãƒ«è¨“ç·´
        
        Parameters
        ----------
        train_df : pd.DataFrame
            è¨“ç·´ãƒ‡ãƒ¼ã‚¿
        val_df : pd.DataFrame
            æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿
        
        Returns
        -------
        best_model : NeuralProphet
            æœ€é©åŒ–ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«
        best_params : dict
            æœ€é©ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        """
        self.logger.info("=" * 80)
        self.logger.info("ğŸ” Optunaæœ€é©åŒ– + è¨“ç·´é–‹å§‹")
        self.logger.info("=" * 80)
        
        # Optuna Studyä½œæˆ
        study = optuna.create_study(
            direction='minimize',
            sampler=optuna.samplers.TPESampler(seed=42),
            pruner=optuna.pruners.MedianPruner(n_startup_trials=10, n_warmup_steps=5)
        )
        
        # ç›®çš„é–¢æ•°
        def objective(trial):
            return self._optuna_objective(trial, train_df, val_df)
        
        # æœ€é©åŒ–å®Ÿè¡Œ
        study.optimize(objective, n_trials=self.optuna_trials, show_progress_bar=True)
        
        self.study = study
        self.best_params = study.best_params
        
        self.logger.info("=" * 80)
        self.logger.info("âœ… Optunaæœ€é©åŒ–å®Œäº†")
        self.logger.info("=" * 80)
        self.logger.info(f"  æœ€é©å€¤ (wQL): {study.best_value:.4f}")
        self.logger.info(f"  æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:")
        for key, value in self.best_params.items():
            self.logger.info(f"    {key}: {value}")
        
        # æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ãƒ¢ãƒ‡ãƒ«è¨“ç·´
        self.logger.info("\nğŸš€ æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ãƒ¢ãƒ‡ãƒ«è¨“ç·´ä¸­...")
        self.best_model = self._train_model_with_params(train_df, self.best_params, verbose=True)
        
        return self.best_model, self.best_params
    
    def _optuna_objective(
        self,
        trial: optuna.Trial,
        train_df: pd.DataFrame,
        val_df: pd.DataFrame
    ) -> float:
        """
        Optunaç›®çš„é–¢æ•°
        
        Parameters
        ----------
        trial : optuna.Trial
            Optunaãƒˆãƒ©ã‚¤ã‚¢ãƒ«
        train_df : pd.DataFrame
            è¨“ç·´ãƒ‡ãƒ¼ã‚¿
        val_df : pd.DataFrame
            æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿
        
        Returns
        -------
        float
            è©•ä¾¡æŒ‡æ¨™ (wQL)
        """
        # ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        params = self._sample_hyperparameters(trial)
        
        # ãƒ¢ãƒ‡ãƒ«è¨“ç·´
        try:
            model = self._train_model_with_params(train_df, params, verbose=False)
        except Exception as e:
            self.logger.warning(f"  Trial {trial.number} failed: {e}")
            return float('inf')
        
        # äºˆæ¸¬
        try:
            future = model.make_future_dataframe(
                df=train_df[['ds', 'y']],
                periods=len(val_df),
                n_historic_predictions=0
            )
            
            # ç‰¹å¾´é‡è¿½åŠ  (future)
            future = self._add_features_to_future(future, train_df)
            
            forecast = model.predict(future)
            
            # äºˆæ¸¬å€¤æŠ½å‡º
            y_pred = forecast['yhat1'].tail(len(val_df)).values
            y_true = val_df['y'].values
            
            # wQLè¨ˆç®—
            wql = self._calculate_wql(y_true, y_pred, self.target_quantile)
            
            return wql
            
        except Exception as e:
            self.logger.warning(f"  Trial {trial.number} prediction failed: {e}")
            return float('inf')
    
    def _sample_hyperparameters(self, trial: optuna.Trial) -> Dict:
        """
        ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° (30+ params)
        
        Parameters
        ----------
        trial : optuna.Trial
            Optunaãƒˆãƒ©ã‚¤ã‚¢ãƒ«
        
        Returns
        -------
        dict
            ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¾æ›¸
        """
        params = {}
        
        # === AR-Net ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ ===
        params['n_lags'] = trial.suggest_int('n_lags', 7, 60)
        params['ar_layers'] = [trial.suggest_categorical('ar_layers', [16, 32, 64, 128])]
        params['ar_sparsity'] = trial.suggest_float('ar_sparsity', 0.0, 0.1)
        
        # === Trend ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ ===
        params['growth'] = trial.suggest_categorical('growth', ['linear', 'discontinuous'])
        params['changepoints_range'] = trial.suggest_float('changepoints_range', 0.8, 0.95)
        params['n_changepoints'] = trial.suggest_int('n_changepoints', 10, 50)
        params['trend_reg'] = trial.suggest_float('trend_reg', 0.0, 10.0)
        
        # === Seasonality ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ ===
        params['yearly_seasonality'] = trial.suggest_int('yearly_seasonality', 5, 20)
        params['weekly_seasonality'] = trial.suggest_int('weekly_seasonality', 3, 7)
        params['seasonality_mode'] = trial.suggest_categorical('seasonality_mode', ['additive', 'multiplicative'])
        params['seasonality_reg'] = trial.suggest_float('seasonality_reg', 0.0, 1.0)
        
        # === Training ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ ===
        params['epochs'] = trial.suggest_categorical('epochs', [50, 100, 200])
        params['batch_size'] = trial.suggest_categorical('batch_size', [16, 32, 64])
        params['learning_rate'] = trial.suggest_float('learning_rate', 0.001, 0.1, log=True)
        
        # === Loss ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ ===
        params['loss_func'] = trial.suggest_categorical('loss_func', ['Huber', 'MSE'])
        
        # === Regularization ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ ===
        params['dropout'] = trial.suggest_float('dropout', 0.0, 0.3)
        params['normalize'] = trial.suggest_categorical('normalize', ['auto', 'standardize', 'minmax'])
        
        return params
    
    def _train_model_with_params(
        self,
        train_df: pd.DataFrame,
        params: Dict,
        verbose: bool = False
    ) -> NeuralProphet:
        """
        æŒ‡å®šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ãƒ¢ãƒ‡ãƒ«è¨“ç·´
        
        Parameters
        ----------
        train_df : pd.DataFrame
            è¨“ç·´ãƒ‡ãƒ¼ã‚¿
        params : dict
            ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        verbose : bool
            è©³ç´°å‡ºåŠ›
        
        Returns
        -------
        NeuralProphet
            è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«
        """
        # ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
        model = NeuralProphet(
            n_lags=params.get('n_lags', 28),
            ar_layers=params.get('ar_layers', [64]),
            ar_sparsity=params.get('ar_sparsity', 0.0),
            growth=params.get('growth', 'linear'),
            changepoints_range=params.get('changepoints_range', 0.9),
            n_changepoints=params.get('n_changepoints', 25),
            trend_reg=params.get('trend_reg', 1.0),
            yearly_seasonality=params.get('yearly_seasonality', 10),
            weekly_seasonality=params.get('weekly_seasonality', 5),
            daily_seasonality=False,
            seasonality_mode=params.get('seasonality_mode', 'additive'),
            seasonality_reg=params.get('seasonality_reg', 0.1),
            epochs=params.get('epochs', 100),
            batch_size=params.get('batch_size', 32),
            learning_rate=params.get('learning_rate', None),
            loss_func=params.get('loss_func', 'Huber'),
            normalize=params.get('normalize', 'auto'),
            drop_missing=False
        )
        
        # Lagged regressorsè¿½åŠ 
        for lag_name in self.lagged_regressor_names:
            if lag_name in train_df.columns:
                model.add_lagged_regressor(names=lag_name, n_lags=1, regularization=0.1)
        
        # Future regressorsè¿½åŠ 
        for future_name in self.future_regressor_names:
            if future_name in train_df.columns:
                model.add_future_regressor(name=future_name, regularization=0.1)
        
        # Eventsè¿½åŠ 
        for event_name in self.event_names:
            if event_name in train_df.columns:
                model.add_events(event_name)
        
        # è¨“ç·´
        metrics = model.fit(
            train_df[['ds', 'y'] + self.lagged_regressor_names + self.future_regressor_names + self.event_names],
            freq='D',
            validation_df=None,
            progress=None if not verbose else 'bar'
        )
        
        return model
    
    def _add_features_to_future(self, future: pd.DataFrame, train_df: pd.DataFrame) -> pd.DataFrame:
        """
        Future dataframeã«ç‰¹å¾´é‡è¿½åŠ 
        
        Parameters
        ----------
        future : pd.DataFrame
            Future dataframe
        train_df : pd.DataFrame
            è¨“ç·´ãƒ‡ãƒ¼ã‚¿ (ç‰¹å¾´é‡ã‚½ãƒ¼ã‚¹)
        
        Returns
        -------
        pd.DataFrame
            ç‰¹å¾´é‡è¿½åŠ æ¸ˆã¿future dataframe
        """
        # åŸºæœ¬æ—¥ä»˜ç‰¹å¾´
        future = self._generate_basic_date_features(future)
        
        # Future regressors
        future = self._generate_calendar_features(future)
        future = self._generate_cyclical_features(future)
        future = self._generate_event_features(future)
        future = self._generate_trend_features(future)
        
        # Lagged regressors (è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å–å¾—)
        combined = pd.concat([train_df, future], ignore_index=True)
        combined = combined.sort_values('ds').reset_index(drop=True)
        
        for lag_name in self.lagged_regressor_names:
            if 'lag_' in lag_name:
                lag = int(lag_name.split('_')[-1])
                combined[lag_name] = combined['y'].shift(lag)
            elif 'rolling_' in lag_name:
                window = int(lag_name.split('_')[-1])
                if 'mean' in lag_name:
                    combined[lag_name] = combined['y'].rolling(window=window, min_periods=1).mean()
                elif 'std' in lag_name:
                    combined[lag_name] = combined['y'].rolling(window=window, min_periods=1).std()
            elif 'ewm_' in lag_name:
                span = int(lag_name.split('_')[-1])
                combined[lag_name] = combined['y'].ewm(span=span, min_periods=1).mean()
        
        # futureéƒ¨åˆ†ã ã‘æŠ½å‡º
        future = combined[combined['ds'] >= future['ds'].min()].copy()
        
        # æ¬ æå€¤è£œå®Œ
        future = future.fillna(method='ffill').fillna(method='bfill')
        
        return future
    
    def _calculate_wql(self, y_true: np.ndarray, y_pred: np.ndarray, quantile: float = 0.6) -> float:
        """
        Weighted Quantile Loss (wQL) è¨ˆç®—
        
        Parameters
        ----------
        y_true : np.ndarray
            å®Ÿæ¸¬å€¤
        y_pred : np.ndarray
            äºˆæ¸¬å€¤
        quantile : float
            åˆ†ä½ç‚¹
        
        Returns
        -------
        float
            wQLå€¤
        """
        errors = y_true - y_pred
        loss = np.where(errors >= 0, quantile * errors, (quantile - 1) * errors)
        return np.mean(loss)
    
    # ==============================================================================
    # äºˆæ¸¬
    # ==============================================================================
    
    def predict(
        self,
        model: NeuralProphet,
        periods: int = 60,
        include_history: bool = True,
        train_df: Optional[pd.DataFrame] = None
    ) -> pd.DataFrame:
        """
        äºˆæ¸¬
        
        Parameters
        ----------
        model : NeuralProphet
            è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«
        periods : int
            äºˆæ¸¬æœŸé–“ (æ—¥æ•°)
        include_history : bool
            å±¥æ­´å«ã‚€
        train_df : pd.DataFrame, optional
            è¨“ç·´ãƒ‡ãƒ¼ã‚¿ (å±¥æ­´äºˆæ¸¬ç”¨)
        
        Returns
        -------
        pd.DataFrame
            äºˆæ¸¬çµæœ
        """
        self.logger.info("=" * 80)
        self.logger.info(f"ğŸ”® äºˆæ¸¬é–‹å§‹ (periods={periods})")
        self.logger.info("=" * 80)
        
        if train_df is None:
            raise ValueError("train_df ãŒå¿…è¦ã§ã™")
        
        # Future dataframeä½œæˆ
        future = model.make_future_dataframe(
            df=train_df[['ds', 'y']],
            periods=periods,
            n_historic_predictions=len(train_df) if include_history else 0
        )
        
        # ç‰¹å¾´é‡è¿½åŠ 
        future = self._add_features_to_future(future, train_df)
        
        # äºˆæ¸¬
        forecast = model.predict(future)
        
        # é€†å¤‰æ›
        if 'yhat1' in forecast.columns:
            forecast['yhat_original'] = self.inverse_transform(forecast['yhat1'].values)
        
        self.logger.info(f"  âœ“ äºˆæ¸¬å®Œäº†: {len(forecast)} è¡Œ")
        
        return forecast
    
    # ==============================================================================
    # è©•ä¾¡
    # ==============================================================================
    
    def evaluate(
        self,
        val_df: pd.DataFrame,
        forecast_df: pd.DataFrame
    ) -> Dict:
        """
        è©•ä¾¡æŒ‡æ¨™è¨ˆç®—
        
        Parameters
        ----------
        val_df : pd.DataFrame
            æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ (å®Ÿæ¸¬å€¤)
        forecast_df : pd.DataFrame
            äºˆæ¸¬çµæœ
        
        Returns
        -------
        dict
            è©•ä¾¡æŒ‡æ¨™è¾æ›¸
        """
        self.logger.info("=" * 80)
        self.logger.info("ğŸ“Š è©•ä¾¡æŒ‡æ¨™è¨ˆç®—")
        self.logger.info("=" * 80)
        
        # æ—¥ä»˜ã§ãƒãƒ¼ã‚¸
        merged = val_df[['ds', 'y']].merge(
            forecast_df[['ds', 'yhat1']],
            on='ds',
            how='inner'
        )
        
        y_true = merged['y'].values
        y_pred = merged['yhat1'].values
        
        # é€†å¤‰æ›
        y_true_original = self.inverse_transform(y_true)
        y_pred_original = self.inverse_transform(y_pred)
        
        metrics = {}
        
        # === Primary Metrics ===
        
        # wQL (è¤‡æ•°åˆ†ä½ç‚¹)
        metrics['wql'] = {}
        for q in [0.1, 0.5, 0.6, 0.7, 0.9]:
            wql = self._calculate_wql(y_true_original, y_pred_original, q)
            metrics['wql'][f'QL_{int(q*100)}'] = wql
        
        # WAPE
        metrics['wape'] = np.sum(np.abs(y_true_original - y_pred_original)) / np.sum(np.abs(y_true_original)) * 100
        
        # sMAPE
        metrics['smape'] = np.mean(2 * np.abs(y_pred_original - y_true_original) / (np.abs(y_true_original) + np.abs(y_pred_original))) * 100
        
        # Asymmetric MAE
        errors = y_true_original - y_pred_original
        under_loss = np.sum(np.maximum(errors, 0)) * 2.0  # éå°äºˆæ¸¬ãƒšãƒŠãƒ«ãƒ†ã‚£
        over_loss = np.sum(np.maximum(-errors, 0)) * 0.5  # éå¤§äºˆæ¸¬ãƒšãƒŠãƒ«ãƒ†ã‚£
        metrics['asymmetric_mae'] = (under_loss + over_loss) / len(errors)
        
        # === Secondary Metrics ===
        
        metrics['mae'] = mean_absolute_error(y_true_original, y_pred_original)
        metrics['rmse'] = np.sqrt(mean_squared_error(y_true_original, y_pred_original))
        metrics['mape'] = np.mean(np.abs((y_true_original - y_pred_original) / y_true_original)) * 100
        metrics['r2'] = r2_score(y_true_original, y_pred_original)
        metrics['bias'] = np.mean(y_pred_original - y_true_original)
        metrics['bias_pct'] = metrics['bias'] / np.mean(y_true_original) * 100
        
        # MASE (Mean Absolute Scaled Error)
        naive_errors = np.abs(np.diff(y_true_original))
        mae_naive = np.mean(naive_errors)
        metrics['mase'] = metrics['mae'] / mae_naive if mae_naive > 0 else np.inf
        
        # === Peak Day Metrics ===
        
        # Top 25% volume days
        threshold = np.percentile(y_true_original, 75)
        peak_mask = y_true_original >= threshold
        
        if np.sum(peak_mask) > 0:
            metrics['peak_mae'] = mean_absolute_error(
                y_true_original[peak_mask],
                y_pred_original[peak_mask]
            )
            metrics['peak_mape'] = np.mean(
                np.abs((y_true_original[peak_mask] - y_pred_original[peak_mask]) / y_true_original[peak_mask])
            ) * 100
            metrics['peak_wape'] = np.sum(np.abs(y_true_original[peak_mask] - y_pred_original[peak_mask])) / np.sum(np.abs(y_true_original[peak_mask])) * 100
            metrics['peak_bias'] = np.mean(y_pred_original[peak_mask] - y_true_original[peak_mask])
            metrics['peak_under_pred_rate'] = np.mean(y_pred_original[peak_mask] < y_true_original[peak_mask]) * 100
        else:
            metrics['peak_mae'] = np.nan
            metrics['peak_mape'] = np.nan
            metrics['peak_wape'] = np.nan
            metrics['peak_bias'] = np.nan
            metrics['peak_under_pred_rate'] = np.nan
        
        # === Day-of-Week Metrics ===
        
        merged_full = val_df[['ds', 'y']].merge(
            forecast_df[['ds', 'yhat1']],
            on='ds',
            how='inner'
        )
        merged_full['dayofweek'] = pd.to_datetime(merged_full['ds']).dt.dayofweek
        
        metrics['dow_metrics'] = {}
        for dow in range(7):
            dow_mask = merged_full['dayofweek'] == dow
            if np.sum(dow_mask) > 0:
                dow_y_true = self.inverse_transform(merged_full.loc[dow_mask, 'y'].values)
                dow_y_pred = self.inverse_transform(merged_full.loc[dow_mask, 'yhat1'].values)
                
                metrics['dow_metrics'][f'dow_{dow}'] = {
                    'mae': mean_absolute_error(dow_y_true, dow_y_pred),
                    'bias': np.mean(dow_y_pred - dow_y_true),
                    'bias_pct': np.mean(dow_y_pred - dow_y_true) / np.mean(dow_y_true) * 100
                }
        
        # ãƒ­ã‚°å‡ºåŠ›
        self.logger.info("\nğŸ¯ Primary Metrics:")
        self.logger.info(f"  wQL (QL_60): {metrics['wql']['QL_60']:.4f}")
        self.logger.info(f"  WAPE: {metrics['wape']:.2f}%")
        self.logger.info(f"  sMAPE: {metrics['smape']:.2f}%")
        self.logger.info(f"  Asymmetric MAE: {metrics['asymmetric_mae']:.2f}")
        
        self.logger.info("\nğŸ“ˆ Secondary Metrics:")
        self.logger.info(f"  MAE: {metrics['mae']:.2f}")
        self.logger.info(f"  RMSE: {metrics['rmse']:.2f}")
        self.logger.info(f"  MAPE: {metrics['mape']:.2f}%")
        self.logger.info(f"  MASE: {metrics['mase']:.4f}")
        self.logger.info(f"  RÂ²: {metrics['r2']:.4f}")
        self.logger.info(f"  Bias: {metrics['bias']:.2f} ({metrics['bias_pct']:.2f}%)")
        
        self.logger.info("\nğŸ” Peak Day Metrics:")
        self.logger.info(f"  Peak MAE: {metrics['peak_mae']:.2f}")
        self.logger.info(f"  Peak MAPE: {metrics['peak_mape']:.2f}%")
        self.logger.info(f"  Peak Under-prediction Rate: {metrics['peak_under_pred_rate']:.2f}%")
        
        return metrics
    
    # ==============================================================================
    # å¯è¦–åŒ–
    # ==============================================================================
    
    def plot_forecast(
        self,
        forecast_df: pd.DataFrame,
        val_df: Optional[pd.DataFrame] = None,
        save_path: Optional[str] = None
    ):
        """
        äºˆæ¸¬çµæœãƒ—ãƒ­ãƒƒãƒˆ
        
        Parameters
        ----------
        forecast_df : pd.DataFrame
            äºˆæ¸¬çµæœ
        val_df : pd.DataFrame, optional
            æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ (å®Ÿæ¸¬å€¤)
        save_path : str, optional
            ä¿å­˜ãƒ‘ã‚¹
        """
        self.logger.info("ğŸ“Š äºˆæ¸¬çµæœãƒ—ãƒ­ãƒƒãƒˆä½œæˆä¸­...")
        
        fig, ax = plt.subplots(figsize=(16, 8))
        
        # äºˆæ¸¬å€¤
        ax.plot(
            forecast_df['ds'],
            self.inverse_transform(forecast_df['yhat1'].values),
            label='Forecast',
            color='blue',
            linewidth=2
        )
        
        # ä¿¡é ¼åŒºé–“ (ã‚‚ã—ã‚ã‚Œã°)
        if 'yhat1_lower' in forecast_df.columns and 'yhat1_upper' in forecast_df.columns:
            ax.fill_between(
                forecast_df['ds'],
                self.inverse_transform(forecast_df['yhat1_lower'].values),
                self.inverse_transform(forecast_df['yhat1_upper'].values),
                alpha=0.2,
                color='blue',
                label='Confidence Interval'
            )
        
        # å®Ÿæ¸¬å€¤
        if val_df is not None:
            ax.plot(
                val_df['ds'],
                self.inverse_transform(val_df['y'].values),
                label='Actual',
                color='red',
                linewidth=2,
                marker='o',
                markersize=4
            )
        
        ax.set_xlabel('Date', fontsize=12)
        ax.set_ylabel('Call Volume', fontsize=12)
        ax.set_title('Call Volume Forecast - NeuralProphet v5.0', fontsize=14, fontweight='bold')
        ax.legend(fontsize=10)
        ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            self.logger.info(f"  âœ“ ä¿å­˜: {save_path}")
        
        if JUPYTER_MODE:
            plt.show()
        else:
            plt.close()
    
    def plot_components(
        self,
        model: NeuralProphet,
        forecast_df: pd.DataFrame,
        save_path: Optional[str] = None
    ):
        """
        ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆãƒ—ãƒ­ãƒƒãƒˆ
        
        Parameters
        ----------
        model : NeuralProphet
            è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«
        forecast_df : pd.DataFrame
            äºˆæ¸¬çµæœ
        save_path : str, optional
            ä¿å­˜ãƒ‘ã‚¹
        """
        self.logger.info("ğŸ“Š ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆãƒ—ãƒ­ãƒƒãƒˆä½œæˆä¸­...")
        
        fig = model.plot_components(forecast_df)
        
        if save_path:
            fig.savefig(save_path, dpi=300, bbox_inches='tight')
            self.logger.info(f"  âœ“ ä¿å­˜: {save_path}")
        
        if JUPYTER_MODE:
            plt.show()
        else:
            plt.close(fig)
    
    def plot_metrics(
        self,
        metrics: Dict,
        save_path: Optional[str] = None
    ):
        """
        è©•ä¾¡æŒ‡æ¨™ãƒ—ãƒ­ãƒƒãƒˆ
        
        Parameters
        ----------
        metrics : dict
            è©•ä¾¡æŒ‡æ¨™è¾æ›¸
        save_path : str, optional
            ä¿å­˜ãƒ‘ã‚¹
        """
        self.logger.info("ğŸ“Š è©•ä¾¡æŒ‡æ¨™ãƒ—ãƒ­ãƒƒãƒˆä½œæˆä¸­...")
        
        fig, axes = plt.subplots(2, 3, figsize=(18, 10))
        
        # wQL
        ax = axes[0, 0]
        wql_values = [v for k, v in metrics['wql'].items()]
        wql_labels = [k for k in metrics['wql'].keys()]
        ax.bar(wql_labels, wql_values, color='steelblue')
        ax.set_title('Weighted Quantile Loss (wQL)', fontweight='bold')
        ax.set_ylabel('wQL')
        ax.grid(axis='y', alpha=0.3)
        
        # Primary Metrics
        ax = axes[0, 1]
        primary_metrics = ['wape', 'smape', 'mape']
        primary_values = [metrics[m] for m in primary_metrics]
        primary_labels = ['WAPE', 'sMAPE', 'MAPE']
        ax.bar(primary_labels, primary_values, color='coral')
        ax.set_title('Percentage Errors', fontweight='bold')
        ax.set_ylabel('%')
        ax.grid(axis='y', alpha=0.3)
        
        # Secondary Metrics
        ax = axes[0, 2]
        secondary_metrics = ['mae', 'rmse', 'asymmetric_mae']
        secondary_values = [metrics[m] for m in secondary_metrics]
        secondary_labels = ['MAE', 'RMSE', 'Asymmetric MAE']
        ax.bar(secondary_labels, secondary_values, color='lightgreen')
        ax.set_title('Absolute Errors', fontweight='bold')
        ax.set_ylabel('Error')
        ax.grid(axis='y', alpha=0.3)
        
        # MASE & RÂ²
        ax = axes[1, 0]
        quality_metrics = ['mase', 'r2']
        quality_values = [metrics[m] for m in quality_metrics]
        quality_labels = ['MASE', 'RÂ²']
        colors = ['orange' if metrics['mase'] < 1 else 'red', 'green' if metrics['r2'] > 0.7 else 'orange']
        ax.bar(quality_labels, quality_values, color=colors)
        ax.set_title('Quality Metrics', fontweight='bold')
        ax.axhline(y=1.0, color='red', linestyle='--', linewidth=1, alpha=0.5, label='MASE Target')
        ax.axhline(y=0.7, color='green', linestyle='--', linewidth=1, alpha=0.5, label='RÂ² Target')
        ax.legend(fontsize=8)
        ax.grid(axis='y', alpha=0.3)
        
        # Peak Day Metrics
        ax = axes[1, 1]
        if not np.isnan(metrics['peak_mae']):
            peak_metrics = ['peak_mae', 'peak_wape', 'peak_under_pred_rate']
            peak_values = [metrics['peak_mae'], metrics['peak_wape'], metrics['peak_under_pred_rate']]
            peak_labels = ['Peak MAE', 'Peak WAPE\n(%)', 'Under-pred\nRate (%)']
            ax.bar(peak_labels, peak_values, color='purple')
            ax.set_title('Peak Day Performance', fontweight='bold')
            ax.grid(axis='y', alpha=0.3)
        else:
            ax.text(0.5, 0.5, 'No Peak Data', ha='center', va='center', fontsize=14)
            ax.set_title('Peak Day Performance', fontweight='bold')
        
        # Day-of-Week MAE
        ax = axes[1, 2]
        if 'dow_metrics' in metrics and len(metrics['dow_metrics']) > 0:
            dow_mae = [metrics['dow_metrics'][f'dow_{dow}']['mae'] for dow in range(7)]
            dow_labels = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']
            ax.bar(dow_labels, dow_mae, color='teal')
            ax.set_title('Day-of-Week MAE', fontweight='bold')
            ax.set_ylabel('MAE')
            ax.grid(axis='y', alpha=0.3)
        else:
            ax.text(0.5, 0.5, 'No DoW Data', ha='center', va='center', fontsize=14)
            ax.set_title('Day-of-Week MAE', fontweight='bold')
        
        plt.suptitle('NeuralProphet v5.0 - Evaluation Metrics', fontsize=16, fontweight='bold', y=1.00)
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            self.logger.info(f"  âœ“ ä¿å­˜: {save_path}")
        
        if JUPYTER_MODE:
            plt.show()
        else:
            plt.close()
    
    def plot_optuna_results(self, save_path: Optional[str] = None):
        """
        Optunaæœ€é©åŒ–çµæœãƒ—ãƒ­ãƒƒãƒˆ
        
        Parameters
        ----------
        save_path : str, optional
            ä¿å­˜ãƒ‘ã‚¹
        """
        if self.study is None:
            self.logger.warning("  âš ï¸  Optuna study ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return
        
        self.logger.info("ğŸ“Š Optunaæœ€é©åŒ–çµæœãƒ—ãƒ­ãƒƒãƒˆä½œæˆä¸­...")
        
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        
        # Optimization History
        ax = axes[0, 0]
        trials = self.study.trials
        trial_numbers = [t.number for t in trials if t.value is not None]
        trial_values = [t.value for t in trials if t.value is not None]
        ax.plot(trial_numbers, trial_values, marker='o', markersize=3, alpha=0.6)
        ax.set_xlabel('Trial')
        ax.set_ylabel('Objective Value (wQL)')
        ax.set_title('Optimization History', fontweight='bold')
        ax.grid(True, alpha=0.3)
        
        # Parameter Importances
        try:
            ax = axes[0, 1]
            importances = optuna.importance.get_param_importances(self.study)
            params = list(importances.keys())[:10]  # Top 10
            values = [importances[p] for p in params]
            ax.barh(params, values, color='steelblue')
            ax.set_xlabel('Importance')
            ax.set_title('Top 10 Parameter Importances', fontweight='bold')
            ax.grid(axis='x', alpha=0.3)
        except Exception as e:
            ax.text(0.5, 0.5, f'Importance Error:\n{e}', ha='center', va='center', fontsize=10)
            ax.set_title('Top 10 Parameter Importances', fontweight='bold')
        
        # Parallel Coordinate (ä¸Šä½10 trials)
        try:
            ax = axes[1, 0]
            from optuna.visualization.matplotlib import plot_parallel_coordinate
            fig_parallel = plot_parallel_coordinate(self.study)
            # Copy to axes (é›£ã—ã„ã®ã§ã‚¹ã‚­ãƒƒãƒ—)
            ax.text(0.5, 0.5, 'See separate plot', ha='center', va='center', fontsize=12)
            ax.set_title('Parallel Coordinate Plot', fontweight='bold')
        except Exception:
            ax.text(0.5, 0.5, 'N/A', ha='center', va='center', fontsize=12)
            ax.set_title('Parallel Coordinate Plot', fontweight='bold')
        
        # Contour (ä¸Šä½2ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿)
        try:
            ax = axes[1, 1]
            from optuna.visualization.matplotlib import plot_contour
            # Copy to axes (é›£ã—ã„ã®ã§ã‚¹ã‚­ãƒƒãƒ—)
            ax.text(0.5, 0.5, 'See separate plot', ha='center', va='center', fontsize=12)
            ax.set_title('Contour Plot', fontweight='bold')
        except Exception:
            ax.text(0.5, 0.5, 'N/A', ha='center', va='center', fontsize=12)
            ax.set_title('Contour Plot', fontweight='bold')
        
        plt.suptitle('Optuna Optimization Results', fontsize=16, fontweight='bold', y=0.995)
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            self.logger.info(f"  âœ“ ä¿å­˜: {save_path}")
        
        if JUPYTER_MODE:
            plt.show()
        else:
            plt.close()
    
    # ==============================================================================
    # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    # ==============================================================================
    
    def generate_report(
        self,
        metrics: Dict,
        best_params: Dict,
        output_path: Optional[str] = None
    ):
        """
        HTMLãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        
        Parameters
        ----------
        metrics : dict
            è©•ä¾¡æŒ‡æ¨™
        best_params : dict
            æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        output_path : str, optional
            å‡ºåŠ›ãƒ‘ã‚¹
        """
        self.logger.info("=" * 80)
        self.logger.info("ğŸ“ HTMLãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ")
        self.logger.info("=" * 80)
        
        if output_path is None:
            output_path = self.output_dir / f'neuralprophet_report_{datetime.now().strftime("%Y%m%d_%H%M%S")}.html'
        
        html = f"""
<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <title>NeuralProphet v5.0 - Forecast Report</title>
    <style>
        body {{
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            margin: 40px;
            background-color: #f5f5f5;
        }}
        .container {{
            background-color: white;
            padding: 30px;
            border-radius: 10px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }}
        h1 {{
            color: #2c3e50;
            border-bottom: 3px solid #3498db;
            padding-bottom: 10px;
        }}
        h2 {{
            color: #34495e;
            margin-top: 30px;
            border-left: 4px solid #3498db;
            padding-left: 10px;
        }}
        table {{
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
        }}
        th, td {{
            border: 1px solid #ddd;
            padding: 12px;
            text-align: left;
        }}
        th {{
            background-color: #3498db;
            color: white;
        }}
        tr:nth-child(even) {{
            background-color: #f2f2f2;
        }}
        .metric-good {{
            color: green;
            font-weight: bold;
        }}
        .metric-warning {{
            color: orange;
            font-weight: bold;
        }}
        .metric-bad {{
            color: red;
            font-weight: bold;
        }}
        .footer {{
            margin-top: 40px;
            text-align: center;
            color: #7f8c8d;
            font-size: 12px;
        }}
    </style>
</head>
<body>
    <div class="container">
        <h1>ğŸ“Š NeuralProphet v5.0 - Call Volume Forecast Report</h1>
        <p><strong>ç”Ÿæˆæ—¥æ™‚:</strong> {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}</p>
        
        <h2>ğŸ¯ Primary Metrics</h2>
        <table>
            <tr>
                <th>æŒ‡æ¨™</th>
                <th>å€¤</th>
                <th>è©•ä¾¡</th>
            </tr>
            <tr>
                <td>wQL (QL_60)</td>
                <td>{metrics['wql']['QL_60']:.4f}</td>
                <td class="{'metric-good' if metrics['wql']['QL_60'] < 50 else 'metric-warning' if metrics['wql']['QL_60'] < 100 else 'metric-bad'}">
                    {'âœ… Excellent' if metrics['wql']['QL_60'] < 50 else 'âš ï¸ Good' if metrics['wql']['QL_60'] < 100 else 'âŒ Needs Improvement'}
                </td>
            </tr>
            <tr>
                <td>WAPE</td>
                <td>{metrics['wape']:.2f}%</td>
                <td class="{'metric-good' if metrics['wape'] < 10 else 'metric-warning' if metrics['wape'] < 20 else 'metric-bad'}">
                    {'âœ… Excellent' if metrics['wape'] < 10 else 'âš ï¸ Good' if metrics['wape'] < 20 else 'âŒ Needs Improvement'}
                </td>
            </tr>
            <tr>
                <td>sMAPE</td>
                <td>{metrics['smape']:.2f}%</td>
                <td class="{'metric-good' if metrics['smape'] < 10 else 'metric-warning' if metrics['smape'] < 20 else 'metric-bad'}">
                    {'âœ… Excellent' if metrics['smape'] < 10 else 'âš ï¸ Good' if metrics['smape'] < 20 else 'âŒ Needs Improvement'}
                </td>
            </tr>
            <tr>
                <td>Asymmetric MAE</td>
                <td>{metrics['asymmetric_mae']:.2f}</td>
                <td class="{'metric-good' if metrics['asymmetric_mae'] < 50 else 'metric-warning' if metrics['asymmetric_mae'] < 100 else 'metric-bad'}">
                    {'âœ… Excellent' if metrics['asymmetric_mae'] < 50 else 'âš ï¸ Good' if metrics['asymmetric_mae'] < 100 else 'âŒ Needs Improvement'}
                </td>
            </tr>
        </table>
        
        <h2>ğŸ“ˆ Secondary Metrics</h2>
        <table>
            <tr>
                <th>æŒ‡æ¨™</th>
                <th>å€¤</th>
            </tr>
            <tr><td>MAE</td><td>{metrics['mae']:.2f}</td></tr>
            <tr><td>RMSE</td><td>{metrics['rmse']:.2f}</td></tr>
            <tr><td>MAPE</td><td>{metrics['mape']:.2f}%</td></tr>
            <tr><td>MASE</td><td>{metrics['mase']:.4f}</td></tr>
            <tr><td>RÂ²</td><td>{metrics['r2']:.4f}</td></tr>
            <tr><td>Bias</td><td>{metrics['bias']:.2f} ({metrics['bias_pct']:.2f}%)</td></tr>
        </table>
        
        <h2>ğŸ” Peak Day Metrics</h2>
        <table>
            <tr>
                <th>æŒ‡æ¨™</th>
                <th>å€¤</th>
            </tr>
            <tr><td>Peak MAE</td><td>{metrics.get('peak_mae', 'N/A'):.2f if not np.isnan(metrics.get('peak_mae', np.nan)) else 'N/A'}</td></tr>
            <tr><td>Peak MAPE</td><td>{metrics.get('peak_mape', 'N/A'):.2f if not np.isnan(metrics.get('peak_mape', np.nan)) else 'N/A'}%</td></tr>
            <tr><td>Peak WAPE</td><td>{metrics.get('peak_wape', 'N/A'):.2f if not np.isnan(metrics.get('peak_wape', np.nan)) else 'N/A'}%</td></tr>
            <tr><td>Peak Bias</td><td>{metrics.get('peak_bias', 'N/A'):.2f if not np.isnan(metrics.get('peak_bias', np.nan)) else 'N/A'}</td></tr>
            <tr><td>Peak Under-prediction Rate</td><td>{metrics.get('peak_under_pred_rate', 'N/A'):.2f if not np.isnan(metrics.get('peak_under_pred_rate', np.nan)) else 'N/A'}%</td></tr>
        </table>
        
        <h2>âš™ï¸ Best Hyperparameters</h2>
        <table>
            <tr>
                <th>ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿</th>
                <th>å€¤</th>
            </tr>
"""
        
        for key, value in best_params.items():
            html += f"            <tr><td>{key}</td><td>{value}</td></tr>\n"
        
        html += """
        </table>
        
        <h2>ğŸ’¡ Recommendations</h2>
        <ul>
"""
        
        # æ¨å¥¨äº‹é …
        if metrics['wape'] < 10:
            html += "            <li>âœ… WAPE < 10%: å„ªç§€ãªäºˆæ¸¬ç²¾åº¦ã§ã™ã€‚æœ¬ç•ªå±•é–‹å¯èƒ½ã§ã™ã€‚</li>\n"
        elif metrics['wape'] < 20:
            html += "            <li>âš ï¸ WAPE 10-20%: è‰¯å¥½ãªäºˆæ¸¬ç²¾åº¦ã§ã™ã€‚ãƒ”ãƒ¼ã‚¯æ—¥ã®ç²¾åº¦æ”¹å–„ã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚</li>\n"
        else:
            html += "            <li>âŒ WAPE > 20%: äºˆæ¸¬ç²¾åº¦ãŒä¸ååˆ†ã§ã™ã€‚ç‰¹å¾´é‡è¿½åŠ ãƒ»ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å†èª¿æ•´ãŒå¿…è¦ã§ã™ã€‚</li>\n"
        
        if metrics['mase'] < 1.0:
            html += "            <li>âœ… MASE < 1.0: ãƒŠã‚¤ãƒ¼ãƒ–äºˆæ¸¬ã‚ˆã‚Šé«˜ç²¾åº¦ã§ã™ã€‚</li>\n"
        else:
            html += "            <li>âŒ MASE â‰¥ 1.0: ãƒŠã‚¤ãƒ¼ãƒ–äºˆæ¸¬ä»¥ä¸‹ã§ã™ã€‚ãƒ¢ãƒ‡ãƒ«æ”¹å–„ãŒå¿…è¦ã§ã™ã€‚</li>\n"
        
        if not np.isnan(metrics.get('peak_under_pred_rate', np.nan)) and metrics['peak_under_pred_rate'] > 40:
            html += f"            <li>âš ï¸ ãƒ”ãƒ¼ã‚¯æ—¥ã®éå°äºˆæ¸¬ç‡ {metrics['peak_under_pred_rate']:.1f}%: ã‚·ãƒ•ãƒˆä¸è¶³ãƒªã‚¹ã‚¯ãŒã‚ã‚Šã¾ã™ã€‚QL_70ã¸ã®èª¿æ•´ã‚’æ¨å¥¨ã€‚</li>\n"
        
        html += """
        </ul>
        
        <div class="footer">
            <p>Generated by NeuralProphet Ultimate Predictor v5.0</p>
            <p>Powered by NeuralProphet, Optuna, PyTorch</p>
        </div>
    </div>
</body>
</html>
"""
        
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(html)
        
        self.logger.info(f"  âœ“ HTMLãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†: {output_path}")
        
        if JUPYTER_MODE:
            display(HTML(f'<a href="{output_path}" target="_blank">ğŸ“„ ãƒ¬ãƒãƒ¼ãƒˆã‚’é–‹ã</a>'))


# ==============================================================================
# CLI + Jupyterä¸¡å¯¾å¿œ Main
# ==============================================================================

def main_cli():
    """CLIã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ"""
    parser = argparse.ArgumentParser(description='NeuralProphet Ultimate Predictor v5.0')
    parser.add_argument('data_path', type=str, help='CSVãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹')
    parser.add_argument('--date-col', type=str, default='date', help='æ—¥ä»˜ã‚«ãƒ©ãƒ å')
    parser.add_argument('--value-col', type=str, default='y', help='å€¤ã‚«ãƒ©ãƒ å')
    parser.add_argument('--validation-months', type=int, default=2, help='æ¤œè¨¼æœŸé–“ (æœˆæ•°)')
    parser.add_argument('--optuna-trials', type=int, default=100, help='Optunaè©¦è¡Œå›æ•°')
    parser.add_argument('--target-quantile', type=float, default=0.6, help='ç›®æ¨™åˆ†ä½ç‚¹')
    parser.add_argument('--epochs', type=int, default=100, help='è¨“ç·´ã‚¨ãƒãƒƒã‚¯æ•°')
    parser.add_argument('--output-dir', type=str, default='./outputs', help='å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª')
    
    args = parser.parse_args()
    
    # PredictoråˆæœŸåŒ–
    predictor = NeuralProphetUltimatePredictor(
        validation_months=args.validation_months,
        optuna_trials=args.optuna_trials,
        target_quantile=args.target_quantile,
        epochs=args.epochs,
        output_dir=args.output_dir
    )
    
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    df = predictor.load_data(args.data_path, date_col=args.date_col, value_col=args.value_col)
    
    # è‡ªå‹•å¤‰æ›
    df_transformed = predictor.select_optimal_transformation(df)
    
    # ç‰¹å¾´é‡ç”Ÿæˆ
    df_features = predictor.generate_comprehensive_features(df_transformed)
    
    # è¨“ç·´ãƒ»æ¤œè¨¼åˆ†å‰²
    train_df, val_df = predictor.split_train_validation(df_features)
    
    # Optunaæœ€é©åŒ– + è¨“ç·´
    best_model, best_params = predictor.optimize_and_train(train_df, val_df)
    
    # äºˆæ¸¬
    forecast_df = predictor.predict(best_model, periods=60, include_history=True, train_df=train_df)
    
    # è©•ä¾¡
    metrics = predictor.evaluate(val_df, forecast_df)
    
    # å¯è¦–åŒ–
    predictor.plot_forecast(
        forecast_df, 
        val_df,
        save_path=predictor.output_dir / 'forecast_plot.png'
    )
    predictor.plot_components(
        best_model,
        forecast_df,
        save_path=predictor.output_dir / 'components_plot.png'
    )
    predictor.plot_metrics(
        metrics,
        save_path=predictor.output_dir / 'metrics_plot.png'
    )
    predictor.plot_optuna_results(
        save_path=predictor.output_dir / 'optuna_plot.png'
    )
    
    # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    predictor.generate_report(metrics, best_params)
    
    print("\n" + "="*80)
    print("âœ… ã™ã¹ã¦ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
    print("="*80)
    print(f"ğŸ“‚ å‡ºåŠ›å…ˆ: {predictor.output_dir}")
    print(f"ğŸ“Š äºˆæ¸¬ãƒ—ãƒ­ãƒƒãƒˆ: forecast_plot.png")
    print(f"ğŸ“Š ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆãƒ—ãƒ­ãƒƒãƒˆ: components_plot.png")
    print(f"ğŸ“Š è©•ä¾¡æŒ‡æ¨™ãƒ—ãƒ­ãƒƒãƒˆ: metrics_plot.png")
    print(f"ğŸ“Š Optunaçµæœãƒ—ãƒ­ãƒƒãƒˆ: optuna_plot.png")
    print(f"ğŸ“ HTMLãƒ¬ãƒãƒ¼ãƒˆ: neuralprophet_report_*.html")


if __name__ == '__main__':
    main_cli()
