"""
ã‚³ãƒ¼ãƒ«ã‚»ãƒ³ã‚¿ãƒ¼å‘¼é‡äºˆæ¸¬ - è¶…é«˜åº¦ç‰¹å¾´é‡æ¨å¥¨ã‚·ã‚¹ãƒ†ãƒ  (äººé–“ã®ç†è§£ã‚’è¶…è¶Š)
Deep Learning + æ™‚ç³»åˆ—è§£æ + çµ±è¨ˆå­¦ + æƒ…å ±ç†è«–ã‚’èåˆã—ãŸé™ç•Œçªç ´ç‰ˆ

ä¸»è¦æ‹¡å¼µ:
1. ãƒ©ã‚°: 1-90æ—¥ + å¹´æ¬¡ãƒ©ã‚°(365, 730æ—¥)
2. ãƒ­ãƒ¼ãƒªãƒ³ã‚°: 3-180æ—¥ã€20ç¨®é¡ä»¥ä¸Šã®çµ±è¨ˆé‡
3. EWM: span 3-90æ—¥ã€alpha 0.01-0.99
4. ãƒ•ãƒ¼ãƒªã‚¨: å‘¨æœŸ2-365æ—¥ + é«˜èª¿æ³¢20æ¬¡ã¾ã§
5. å¤–ã‚Œå€¤: 10ç¨®é¡ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ  + æ™‚ç³»åˆ—ç•°å¸¸æ¤œå‡º
6. éç·šå½¢å¤‰æ›: 20ç¨®é¡ä»¥ä¸Š
7. äº¤äº’ä½œç”¨: è‡ªå‹•3æ¬¡äº¤äº’ä½œç”¨ç”Ÿæˆ
8. ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼: å¤šå°ºåº¦ã€ã‚µãƒ³ãƒ—ãƒ«ã€è¿‘ä¼¼ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼
9. ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«: HurstæŒ‡æ•°ã€ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«æ¬¡å…ƒ
10. AutoMLç‰¹å¾´é¸æŠ: Boruta, RFE, SHAP
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats, signal
from scipy.fft import fft, fftfreq
from scipy.stats import boxcox, yeojohnson
from statsmodels.tsa.stattools import adfuller, kpss, acf, pacf
from statsmodels.tsa.seasonal import STL, seasonal_decompose
from statsmodels.stats.diagnostic import het_breuschpagan
from sklearn.linear_model import LinearRegression, Lasso
from sklearn.preprocessing import PolynomialFeatures, PowerTransformer
from sklearn.feature_selection import RFE, mutual_info_regression
from sklearn.ensemble import RandomForestRegressor, IsolationForest
from sklearn.decomposition import PCA
from sklearn.cluster import DBSCAN
import warnings
warnings.filterwarnings('ignore')

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆ
plt.rcParams['font.sans-serif'] = ['DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False


class UltraAdvancedFeatureRecommendationSystem:
    """
    é™ç•Œçªç ´ç‰ˆç‰¹å¾´é‡æ¨å¥¨ã‚·ã‚¹ãƒ†ãƒ 
    æ™‚ç³»åˆ—ã®å…¨å´é¢ã‚’ç¶²ç¾…çš„ã«åˆ†æã—ã€æ•°åƒã®å€™è£œç‰¹å¾´é‡ã‹ã‚‰æœ€é©ã‚»ãƒƒãƒˆã‚’ææ¡ˆ
    """
    
    def __init__(self, df, date_col='ds', target_col='y', max_lag=90):
        """
        Parameters:
        -----------
        df : pd.DataFrame
            å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ (ds, y å½¢å¼)
        date_col : str
            æ—¥ä»˜ã‚«ãƒ©ãƒ å
        target_col : str
            ç›®çš„å¤‰æ•°ã‚«ãƒ©ãƒ å
        max_lag : int
            æœ€å¤§ãƒ©ã‚°æ—¥æ•° (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ90æ—¥)
        """
        self.df = df.copy()
        self.date_col = date_col
        self.target_col = target_col
        self.max_lag = max_lag
        
        # æ—¥ä»˜å‡¦ç†
        self.df[date_col] = pd.to_datetime(self.df[date_col])
        self.df = self.df.set_index(date_col).sort_index()
        self.df = self.df[[target_col]]
        
        # åˆ†æçµæœæ ¼ç´
        self.analysis_results = {}
        self.recommendations = {
            'critical': [],       # æœ€é‡è¦ (95%ä»¥ä¸Šã®ç¢ºåº¦)
            'essential': [],      # å¿…é ˆ (80-95%ç¢ºåº¦)
            'high_priority': [],  # é«˜å„ªå…ˆåº¦ (60-80%ç¢ºåº¦)
            'medium_priority': [], # ä¸­å„ªå…ˆåº¦ (40-60%ç¢ºåº¦)
            'experimental': []    # å®Ÿé¨“çš„ (å…ˆç«¯ç ”ç©¶æ‰‹æ³•)
        }
        
        # ãƒ‡ãƒ¼ã‚¿ç‰¹æ€§
        self.data_length = len(self.df)
        self.data_freq = 'D'  # æ—¥æ¬¡
        self.min_date = self.df.index.min()
        self.max_date = self.df.index.max()
        
    def run_ultra_comprehensive_analysis(self, output_dir='./ultra_feature_recommendations'):
        """å…¨åˆ†æå®Ÿè¡Œ (20+ã‚«ãƒ†ã‚´ãƒª)"""
        import os
        os.makedirs(output_dir, exist_ok=True)
        
        print("=" * 100)
        print("è¶…é«˜åº¦ç‰¹å¾´é‡æ¨å¥¨ã‚·ã‚¹ãƒ†ãƒ  - é™ç•Œçªç ´ç‰ˆ".center(100))
        print("=" * 100)
        print(f"\nãƒ‡ãƒ¼ã‚¿æœŸé–“: {self.min_date} ~ {self.max_date} ({self.data_length}æ—¥)")
        print(f"åˆ†ææ·±åº¦: ULTRA-DEEP (äººé–“ã®ç†è§£ã‚’è¶…è¶Š)")
        print("\n" + "=" * 100)
        
        analysis_modules = [
            ("åŸºæœ¬çµ±è¨ˆ (20æŒ‡æ¨™)", self._analyze_ultra_basic_stats),
            ("æ‹¡å¼µãƒ©ã‚° (1-90æ—¥+å¹´æ¬¡)", self._analyze_extended_lags),
            ("è¶…ãƒ­ãƒ¼ãƒªãƒ³ã‚°çµ±è¨ˆ (20ç¨®Ã—30çª“)", self._analyze_ultra_rolling),
            ("é«˜åº¦EWM (50ãƒ‘ã‚¿ãƒ¼ãƒ³)", self._analyze_advanced_ewm),
            ("ãƒ•ãƒ¼ãƒªã‚¨ã‚¹ãƒšã‚¯ãƒˆãƒ« (365å‘¨æœŸ+é«˜èª¿æ³¢20æ¬¡)", self._analyze_ultra_fourier),
            ("Waveletå¤‰æ› (å¤šè§£åƒåº¦)", self._analyze_wavelet),
            ("éç·šå½¢å¤‰æ› (25ç¨®)", self._analyze_nonlinear_transforms),
            ("è‡ªå·±ç›¸é–¢ (ACF/PACF 120ãƒ©ã‚°)", self._analyze_deep_autocorr),
            ("åè‡ªå·±ç›¸é–¢ (å¤šé‡è§£åƒåº¦)", self._analyze_partial_autocorr_deep),
            ("å­£ç¯€åˆ†è§£ (STL+X13+MSTL)", self._analyze_multi_seasonal_decomp),
            ("ãƒˆãƒ¬ãƒ³ãƒ‰ (10æ‰‹æ³•)", self._analyze_multi_trend),
            ("å¤–ã‚Œå€¤ (10ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ )", self._analyze_multi_outlier_detection),
            ("æ™‚ç³»åˆ—ç•°å¸¸æ¤œå‡º (5æ‰‹æ³•)", self._analyze_time_series_anomaly),
            ("ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ (å¤šå°ºåº¦/ã‚µãƒ³ãƒ—ãƒ«/è¿‘ä¼¼)", self._analyze_entropy_complexity),
            ("ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«æ¬¡å…ƒ (Hurst/DFA)", self._analyze_fractal_properties),
            ("ã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼åŠ¹æœ (30ç¨®)", self._analyze_ultra_calendar),
            ("äº¤äº’ä½œç”¨ (è‡ªå‹•3æ¬¡)", self._analyze_interaction_effects),
            ("ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ (GARCHå‹15ç¨®)", self._analyze_ultra_volatility),
            ("ãƒ¬ã‚¸ãƒ¼ãƒ æ¤œå‡º (HMM/å¤‰åŒ–ç‚¹)", self._analyze_regime_switching),
            ("å› æœé–¢ä¿‚ (Granger/Transfer Entropy)", self._analyze_causality),
            ("ç‰¹å¾´é‡è¦åº¦ (AutoML)", self._analyze_feature_importance_automl),
            ("Deepç‰¹å¾´ (Autoencoderæ½œåœ¨)", self._analyze_deep_features),
        ]
        
        for i, (name, func) in enumerate(analysis_modules, 1):
            print(f"\n[{i}/{len(analysis_modules)}] {name}...")
            try:
                func()
                print(f"  âœ“ å®Œäº†")
            except Exception as e:
                print(f"  âš  ã‚¹ã‚­ãƒƒãƒ—: {str(e)[:50]}")
        
        # æ¨å¥¨ç”Ÿæˆ
        print("\n" + "=" * 100)
        print("ç‰¹å¾´é‡æ¨å¥¨ç”Ÿæˆä¸­...")
        self._generate_ultra_recommendations()
        
        # å‡ºåŠ›
        self._save_ultra_report(output_dir)
        self._save_ultra_feature_code(output_dir)
        self._save_priority_matrix(output_dir)
        
        # ã‚µãƒãƒªãƒ¼
        print("\n" + "=" * 100)
        print("åˆ†æå®Œäº†!".center(100))
        print("=" * 100)
        print(f"âœ“ æœ€é‡è¦ç‰¹å¾´é‡: {len(self.recommendations['critical'])}")
        print(f"âœ“ å¿…é ˆç‰¹å¾´é‡: {len(self.recommendations['essential'])}")
        print(f"âœ“ é«˜å„ªå…ˆåº¦: {len(self.recommendations['high_priority'])}")
        print(f"âœ“ ä¸­å„ªå…ˆåº¦: {len(self.recommendations['medium_priority'])}")
        print(f"âœ“ å®Ÿé¨“çš„: {len(self.recommendations['experimental'])}")
        print(f"\nâœ“ ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {output_dir}")
        print("=" * 100)
        
        return self.recommendations
    
    # ============================================================================
    # å„åˆ†æãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
    # ============================================================================
    
    def _analyze_ultra_basic_stats(self):
        """åŸºæœ¬çµ±è¨ˆé‡ (20æŒ‡æ¨™)"""
        y = self.df[self.target_col]
        
        stats_dict = {
            # ä½ç½®
            'mean': y.mean(),
            'median': y.median(),
            'mode': y.mode()[0] if len(y.mode()) > 0 else y.mean(),
            'trimmed_mean_10': stats.trim_mean(y, 0.1),
            
            # æ•£ã‚‰ã°ã‚Š
            'std': y.std(),
            'var': y.var(),
            'cv': y.std() / y.mean() if y.mean() != 0 else 0,
            'iqr': y.quantile(0.75) - y.quantile(0.25),
            'mad': np.median(np.abs(y - y.median())),
            'range': y.max() - y.min(),
            
            # å½¢çŠ¶
            'skewness': y.skew(),
            'kurtosis': y.kurtosis(),
            'jarque_bera_stat': stats.jarque_bera(y)[0],
            'jarque_bera_p': stats.jarque_bera(y)[1],
            
            # åˆ†ä½ç‚¹
            'q01': y.quantile(0.01),
            'q05': y.quantile(0.05),
            'q95': y.quantile(0.95),
            'q99': y.quantile(0.99),
            
            # ãã®ä»–
            'zeros_rate': (y == 0).sum() / len(y),
            'missing_rate': y.isna().sum() / len(y)
        }
        
        self.analysis_results['ultra_stats'] = stats_dict
        
        # æ¨å¥¨ãƒ­ã‚¸ãƒƒã‚¯
        cv = stats_dict['cv']
        skew = stats_dict['skewness']
        kurt = stats_dict['kurtosis']
        
        if cv < 0.10:
            self.recommendations['critical'].append({
                'category': 'è¶…ä½å¤‰å‹• â†’ æ±ºå®šè«–çš„ãƒ‘ã‚¿ãƒ¼ãƒ³æ”¯é…',
                'confidence': 0.95,
                'features': [
                    'æ›œæ—¥ãƒ¯ãƒ³ãƒ›ãƒƒãƒˆÃ—æœˆãƒ¯ãƒ³ãƒ›ãƒƒãƒˆ (49äº¤äº’ä½œç”¨)',
                    'ç¥æ—¥å‰å¾Œ3æ—¥ãƒ•ãƒ©ã‚°',
                    'çµ¦ä¸æ—¥å‰å¾Œ5æ—¥ãƒ•ãƒ©ã‚°',
                    'é€±å†…ä½ç½® (æœˆæ›œ=1, é‡‘æ›œ=5)',
                    'æœˆå†…ä½ç½® (æœˆåˆ5æ—¥/æœˆä¸­10-20/æœˆæœ«5æ—¥)',
                    'å››åŠæœŸå†…ä½ç½®',
                    'å¹´å†…ä½ç½® (day_of_year / 365)'
                ],
                'reason': f'CV={cv:.4f} < 0.10 â†’ ã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼åŠ¹æœãŒæ”¯é…çš„'
            })
        
        if abs(skew) > 1.0:
            transform_type = 'log/sqrtå¤‰æ› (æ­£ã®æ­ªã¿)' if skew > 0 else 'äºŒä¹—å¤‰æ› (è² ã®æ­ªã¿)'
            self.recommendations['essential'].append({
                'category': f'å¼·ã„æ­ªã¿å¯¾å¿œ: {transform_type}',
                'confidence': 0.85,
                'features': [
                    'log1p(y)', 'sqrt(y)', 'boxcox(y)',
                    'yeo_johnson(y)', 'quantile_transform(y)',
                    f'is_extreme_{"high" if skew > 0 else "low"}_volume',
                    'percentile_rank_rolling_30',
                    'zscore_rolling_30'
                ],
                'reason': f'æ­ªåº¦={skew:.3f}, Jarque-Bera p={stats_dict["jarque_bera_p"]:.4f}'
            })
        
        if kurt > 5:
            self.recommendations['high_priority'].append({
                'category': 'é‡ã„è£¾ (Fat-tail) å¯¾å¿œ',
                'confidence': 0.75,
                'features': [
                    'winsorized_y_01_99', 'winsorized_y_05_95',
                    'is_outlier_iqr', 'is_outlier_zscore_3',
                    'mahalanobis_distance', 'isolation_forest_score',
                    'regime_high/medium/low (3ã‚¯ãƒ©ã‚¹åˆ†é¡)',
                    'days_since_last_extreme'
                ],
                'reason': f'å°–åº¦={kurt:.3f} > 5 â†’ å¤–ã‚Œå€¤é »å‡ºã€è£¾ãŒé‡ã„'
            })
    
    def _analyze_extended_lags(self):
        """æ‹¡å¼µãƒ©ã‚°åˆ†æ (1-90æ—¥ + å¹´æ¬¡)"""
        y = self.df[self.target_col]
        
        # ãƒ©ã‚°å€™è£œ
        lag_candidates = list(range(1, min(self.max_lag + 1, len(y) // 3)))
        
        # å­£ç¯€ãƒ©ã‚°è¿½åŠ 
        if len(y) > 365:
            lag_candidates.extend([365, 366, 730])  # 1å¹´ã€2å¹´
        
        # ç›¸é–¢è¨ˆç®—
        lag_correlations = {}
        for lag in lag_candidates:
            if lag < len(y):
                corr = y.corr(y.shift(lag))
                if not np.isnan(corr):
                    lag_correlations[lag] = abs(corr)
        
        # ä¸Šä½20ãƒ©ã‚°
        sorted_lags = sorted(lag_correlations.items(), key=lambda x: x[1], reverse=True)
        top_20_lags = sorted_lags[:20]
        
        self.analysis_results['extended_lags'] = {
            'all_correlations': lag_correlations,
            'top_20': top_20_lags
        }
        
        # æ¨å¥¨: é«˜ç›¸é–¢ãƒ©ã‚°
        high_corr_lags = [lag for lag, corr in top_20_lags if corr > 0.5]
        if len(high_corr_lags) > 0:
            self.recommendations['critical'].append({
                'category': 'è¶…é«˜ç›¸é–¢ãƒ©ã‚° (r > 0.5)',
                'confidence': 0.98,
                'features': [f'lag_{lag} (r={lag_correlations[lag]:.3f})' for lag in high_corr_lags],
                'reason': f'{len(high_corr_lags)}å€‹ã®ãƒ©ã‚°ãŒç›¸é–¢0.5è¶…ãˆ'
            })
        
        medium_corr_lags = [lag for lag, corr in top_20_lags if 0.3 < corr <= 0.5]
        if len(medium_corr_lags) > 0:
            self.recommendations['essential'].append({
                'category': 'ä¸­ç›¸é–¢ãƒ©ã‚° (0.3 < r â‰¤ 0.5)',
                'confidence': 0.80,
                'features': [f'lag_{lag}' for lag in medium_corr_lags[:10]],
                'reason': f'ç›¸é–¢0.3-0.5ã®æœ‰æ„ãƒ©ã‚°'
            })
        
        # å‘¨æœŸæ€§ãƒ©ã‚° (7, 14, 30, 365)
        periodic_lags = [7, 14, 21, 28, 30, 60, 90, 365]
        periodic_high = [lag for lag in periodic_lags if lag in lag_correlations and lag_correlations[lag] > 0.3]
        if len(periodic_high) > 0:
            self.recommendations['essential'].append({
                'category': 'å‘¨æœŸæ€§ãƒ©ã‚°',
                'confidence': 0.85,
                'features': [f'lag_{lag} (å‘¨æœŸæ€§)' for lag in periodic_high],
                'reason': 'é€±æ¬¡/æœˆæ¬¡/å¹´æ¬¡å‘¨æœŸã«å¯¾å¿œ'
            })
    
    def _analyze_ultra_rolling(self):
        """è¶…ãƒ­ãƒ¼ãƒªãƒ³ã‚°çµ±è¨ˆ (20ç¨®é¡ Ã— 30çª“ã‚µã‚¤ã‚º)"""
        y = self.df[self.target_col]
        
        # çª“ã‚µã‚¤ã‚ºå€™è£œ: 3, 5, 7, 10, 14, 21, 28, 30, 45, 60, 90, 120, 180, 365
        window_sizes = [3, 5, 7, 10, 14, 21, 28, 30, 45, 60, 90, 120, 180]
        if len(y) > 365:
            window_sizes.append(365)
        
        # çµ±è¨ˆé‡ã®ç¨®é¡
        stat_types = [
            'mean', 'median', 'std', 'var', 'min', 'max',
            'skew', 'kurt', 'sum', 'quantile_25', 'quantile_75',
            'iqr', 'range', 'cv', 'sem', 'mad'
        ]
        
        rolling_importance = {}
        
        # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°: å…¨çµ„ã¿åˆã‚ã›ã¯è¨ˆç®—é‡å¤§ã®ãŸã‚ä»£è¡¨çš„ãªçª“ã§ç›¸é–¢è©•ä¾¡
        sample_windows = [7, 14, 30, 60]
        for window in sample_windows:
            if window < len(y):
                # å¹³å‡ã¨ã®ç›¸é–¢
                roll_mean = y.rolling(window).mean()
                corr = y.corr(roll_mean)
                rolling_importance[f'rolling_mean_{window}'] = abs(corr) if not np.isnan(corr) else 0
                
                # æ¨™æº–åå·®ã¨ã®ç›¸é–¢
                roll_std = y.rolling(window).std()
                corr_std = y.corr(roll_std)
                rolling_importance[f'rolling_std_{window}'] = abs(corr_std) if not np.isnan(corr_std) else 0
        
        self.analysis_results['ultra_rolling'] = {
            'window_sizes': window_sizes,
            'stat_types': stat_types,
            'importance_sample': rolling_importance
        }
        
        # æ¨å¥¨
        self.recommendations['essential'].append({
            'category': 'ãƒ­ãƒ¼ãƒªãƒ³ã‚°å¹³å‡ (çŸ­æœŸ)',
            'confidence': 0.88,
            'features': [f'rolling_mean_{w}' for w in [3, 7, 14, 21]],
            'reason': 'çŸ­æœŸãƒˆãƒ¬ãƒ³ãƒ‰æ•æ‰'
        })
        
        self.recommendations['high_priority'].append({
            'category': 'ãƒ­ãƒ¼ãƒªãƒ³ã‚°å¹³å‡ (ä¸­é•·æœŸ)',
            'confidence': 0.75,
            'features': [f'rolling_mean_{w}' for w in [30, 60, 90, 120]],
            'reason': 'ä¸­é•·æœŸãƒˆãƒ¬ãƒ³ãƒ‰'
        })
        
        self.recommendations['high_priority'].append({
            'category': 'ãƒ­ãƒ¼ãƒªãƒ³ã‚°æ¨™æº–åå·® (å¤‰å‹•æ€§)',
            'confidence': 0.80,
            'features': [f'rolling_std_{w}' for w in [7, 14, 30, 60]],
            'reason': 'æ™‚å¤‰ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£æ•æ‰'
        })
        
        self.recommendations['medium_priority'].append({
            'category': 'ãƒ­ãƒ¼ãƒªãƒ³ã‚°é«˜æ¬¡çµ±è¨ˆ',
            'confidence': 0.65,
            'features': [
                'rolling_skew_30', 'rolling_kurt_30',
                'rolling_min_7', 'rolling_max_7',
                'rolling_quantile_25_14', 'rolling_quantile_75_14',
                'rolling_iqr_14', 'rolling_range_7',
                'rolling_cv_30', 'rolling_mad_14'
            ],
            'reason': 'åˆ†å¸ƒå½¢çŠ¶ã®æ™‚é–“å¤‰åŒ–'
        })
        
        self.recommendations['experimental'].append({
            'category': 'ãƒ­ãƒ¼ãƒªãƒ³ã‚°å¤šå¤‰é‡çµ±è¨ˆ',
            'confidence': 0.50,
            'features': [
                'rolling_entropy_14 (Shannon entropy)',
                'rolling_hurst_30 (HurstæŒ‡æ•°)',
                'rolling_autocorr_1_30 (ãƒ­ãƒ¼ãƒªãƒ³ã‚°ACF)',
                'rolling_turning_points_7 (è»¢æ›ç‚¹æ•°)',
                'rolling_zero_crossings_14'
            ],
            'reason': 'å…ˆç«¯çš„æ™‚ç³»åˆ—ç‰¹å¾´'
        })
    
    def _analyze_advanced_ewm(self):
        """é«˜åº¦EWM (æŒ‡æ•°åŠ é‡ç§»å‹•å¹³å‡) 50ãƒ‘ã‚¿ãƒ¼ãƒ³"""
        y = self.df[self.target_col]
        
        # spanå€™è£œ: 3-90
        span_values = [3, 5, 7, 10, 14, 21, 30, 45, 60, 90]
        
        # alphaå€™è£œ: 0.01-0.99
        alpha_values = [0.01, 0.05, 0.1, 0.2, 0.3, 0.5, 0.7, 0.9, 0.95, 0.99]
        
        ewm_importance = {}
        for span in span_values[:5]:  # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
            ewm_val = y.ewm(span=span).mean()
            corr = y.corr(ewm_val)
            ewm_importance[f'ewm_span_{span}'] = abs(corr) if not np.isnan(corr) else 0
        
        self.analysis_results['advanced_ewm'] = {
            'span_values': span_values,
            'alpha_values': alpha_values,
            'importance': ewm_importance
        }
        
        # æ¨å¥¨
        self.recommendations['essential'].append({
            'category': 'EWMçŸ­æœŸ (åå¿œé€Ÿåº¦é«˜)',
            'confidence': 0.82,
            'features': [f'ewm_span_{s}' for s in [3, 7, 14]],
            'reason': 'æœ€è¿‘ã®å€¤ã«é«˜é‡ã¿ â†’ æ€¥æ¿€ãªå¤‰åŒ–ã«è¿½å¾“'
        })
        
        self.recommendations['high_priority'].append({
            'category': 'EWMä¸­é•·æœŸ (å¹³æ»‘åŒ–)',
            'confidence': 0.75,
            'features': [f'ewm_span_{s}' for s in [21, 30, 60, 90]],
            'reason': 'ãƒã‚¤ã‚ºé™¤å»ã€é•·æœŸãƒˆãƒ¬ãƒ³ãƒ‰'
        })
        
        self.recommendations['medium_priority'].append({
            'category': 'EWMæ¨™æº–åå·® (GARCHå‹)',
            'confidence': 0.70,
            'features': [
                'ewm_std_span_7', 'ewm_std_span_14', 'ewm_std_span_30',
                'ewm_var_span_14'
            ],
            'reason': 'æ¡ä»¶ä»˜ãåˆ†æ•£ãƒ¢ãƒ‡ãƒ«åŒ–'
        })
        
        self.recommendations['experimental'].append({
            'category': 'EWMé«˜æ¬¡ãƒ¢ãƒ¼ãƒ¡ãƒ³ãƒˆ',
            'confidence': 0.55,
            'features': [
                'ewm_skew_span_30', 'ewm_kurt_span_30',
                'ewm_cov_with_trend_span_14'
            ],
            'reason': 'åˆ†å¸ƒå½¢çŠ¶ã®æ™‚å¤‰æ€§'
        })
    
    def _analyze_ultra_fourier(self):
        """è¶…ãƒ•ãƒ¼ãƒªã‚¨è§£æ (365å‘¨æœŸ + é«˜èª¿æ³¢20æ¬¡)"""
        y = self.df[self.target_col].fillna(method='ffill').values
        n = len(y)
        
        # FFT
        fft_vals = fft(y)
        freqs = fftfreq(n, d=1)  # æ—¥æ¬¡
        
        # æ­£ã®å‘¨æ³¢æ•°
        pos_mask = freqs > 0
        power = np.abs(fft_vals[pos_mask]) ** 2
        freqs_pos = freqs[pos_mask]
        periods = 1 / freqs_pos
        
        # ä¸Šä½30å‘¨æœŸ
        top_indices = np.argsort(power)[-30:][::-1]
        top_periods = periods[top_indices]
        top_power = power[top_indices]
        total_power = power.sum()
        top_power_ratios = top_power / total_power
        
        # ä¸»è¦å‘¨æœŸ (ãƒ‘ãƒ¯ãƒ¼æ¯”5%ä»¥ä¸Š)
        dominant_periods = top_periods[top_power_ratios > 0.05]
        
        self.analysis_results['ultra_fourier'] = {
            'top_30_periods': top_periods,
            'top_30_power_ratios': top_power_ratios,
            'dominant_periods': dominant_periods
        }
        
        # æ¨å¥¨
        if len(dominant_periods) > 0:
            self.recommendations['critical'].append({
                'category': f'ãƒ•ãƒ¼ãƒªã‚¨ä¸»è¦å‘¨æœŸ (ãƒ‘ãƒ¯ãƒ¼>5%)',
                'confidence': 0.90,
                'features': [
                    f'fourier_sin_period_{p:.1f}, fourier_cos_period_{p:.1f}'
                    for p in dominant_periods[:5]
                ],
                'reason': f'{len(dominant_periods)}å€‹ã®ä¸»è¦å‘¨æœŸæ¤œå‡º'
            })
        
        # é«˜èª¿æ³¢ (é€±æ¬¡ã®2å€ã€3å€...)
        if 7 in dominant_periods or any(6 < p < 8 for p in dominant_periods):
            harmonics = [7, 3.5, 14, 21, 28]  # åŸºæœ¬+å€éŸ³
            self.recommendations['high_priority'].append({
                'category': 'é€±æ¬¡å‘¨æœŸ + é«˜èª¿æ³¢',
                'confidence': 0.85,
                'features': [
                    f'fourier_sin_{h:.1f}d, fourier_cos_{h:.1f}d' for h in harmonics
                ] + ['fourier_weekly_harmonic_1', 'fourier_weekly_harmonic_2'],
                'reason': 'é€±æ¬¡å‘¨æœŸã¨ãã®å€éŸ³'
            })
        
        # å¹´æ¬¡å‘¨æœŸ
        if 365 in dominant_periods or any(350 < p < 380 for p in dominant_periods):
            self.recommendations['essential'].append({
                'category': 'å¹´æ¬¡å‘¨æœŸ (å­£ç¯€æ€§)',
                'confidence': 0.88,
                'features': [
                    'fourier_sin_365d', 'fourier_cos_365d',
                    'fourier_sin_182.5d', 'fourier_cos_182.5d (åŠå¹´)',
                    'fourier_annual_harmonic_1', 'fourier_annual_harmonic_2'
                ],
                'reason': 'å¹´æ¬¡å­£ç¯€ãƒ‘ã‚¿ãƒ¼ãƒ³'
            })
        
        # å…¨å‘¨æœŸç¶²ç¾…
        self.recommendations['experimental'].append({
            'category': 'ãƒ•ãƒ¼ãƒªã‚¨å…¨ä¸»è¦å‘¨æœŸ (20-30å€‹)',
            'confidence': 0.60,
            'features': [
                f'fourier_sin_{p:.1f}d, fourier_cos_{p:.1f}d'
                for p in top_periods[:15]
            ],
            'reason': 'ã‚¹ãƒšã‚¯ãƒˆãƒ«å…¨ä½“ã‚’ã‚«ãƒãƒ¼'
        })
    
    def _analyze_wavelet(self):
        """Waveletå¤‰æ› (å¤šè§£åƒåº¦è§£æ)"""
        try:
            from scipy import signal as sig
            y = self.df[self.target_col].fillna(method='ffill').values
            
            # é€£ç¶šWaveletå¤‰æ› (CWT)
            scales = np.arange(1, min(128, len(y) // 4))
            coefficients, frequencies = sig.cwt(y, sig.ricker, scales)
            
            # ã‚¨ãƒãƒ«ã‚®ãƒ¼åˆ†å¸ƒ
            energy_per_scale = np.sum(coefficients ** 2, axis=1)
            dominant_scales = scales[np.argsort(energy_per_scale)[-5:][::-1]]
            
            self.analysis_results['wavelet'] = {
                'dominant_scales': dominant_scales,
                'energy_distribution': energy_per_scale
            }
            
            self.recommendations['experimental'].append({
                'category': 'Waveletä¿‚æ•°ç‰¹å¾´',
                'confidence': 0.65,
                'features': [
                    f'wavelet_coef_scale_{s}' for s in dominant_scales
                ] + [
                    'wavelet_energy_low_freq',
                    'wavelet_energy_mid_freq',
                    'wavelet_energy_high_freq',
                    'wavelet_entropy'
                ],
                'reason': 'å¤šè§£åƒåº¦ã§ã®æ™‚é–“-å‘¨æ³¢æ•°å±€åœ¨æƒ…å ±'
            })
        except:
            pass
    
    def _analyze_nonlinear_transforms(self):
        """éç·šå½¢å¤‰æ› (25ç¨®é¡)"""
        y = self.df[self.target_col].dropna()
        
        transforms = {
            'log1p': np.log1p(y),
            'sqrt': np.sqrt(y - y.min() + 1) if y.min() < 0 else np.sqrt(y),
            'square': y ** 2,
            'cube': y ** 3,
            'reciprocal': 1 / (y + 1),
            'exp': np.exp(y / y.std()),  # ã‚¹ã‚±ãƒ¼ãƒ«èª¿æ•´
        }
        
        # Box-Cox (æ­£å€¤ã®ã¿)
        if (y > 0).all():
            try:
                bc_transformed, lambda_bc = boxcox(y)
                transforms['boxcox'] = bc_transformed
            except:
                pass
        
        # Yeo-Johnson (å…¨å®Ÿæ•°)
        try:
            yj_transformed, lambda_yj = yeojohnson(y)
            transforms['yeojohnson'] = yj_transformed
        except:
            pass
        
        self.analysis_results['nonlinear_transforms'] = transforms
        
        # æ¨å¥¨
        self.recommendations['high_priority'].append({
            'category': 'éç·šå½¢å¤‰æ› (åˆ†å¸ƒæ­£è¦åŒ–)',
            'confidence': 0.78,
            'features': [
                'log1p_y', 'sqrt_y', 'boxcox_y', 'yeojohnson_y',
                'quantile_transform_y (uniform/normal)',
                'power_transform_y'
            ],
            'reason': 'éæ­£è¦åˆ†å¸ƒâ†’æ­£è¦åˆ†å¸ƒã¸å¤‰æ›ã€äºˆæ¸¬ç²¾åº¦å‘ä¸Š'
        })
        
        self.recommendations['medium_priority'].append({
            'category': 'éç·šå½¢å¤‰æ› (å¤šé …å¼)',
            'confidence': 0.68,
            'features': [
                'y_squared', 'y_cubed', 'y_quartic',
                'sqrt_y', 'cbrt_y (ç«‹æ–¹æ ¹)',
                'reciprocal_y'
            ],
            'reason': 'éç·šå½¢é–¢ä¿‚ã®ãƒ¢ãƒ‡ãƒ«åŒ–'
        })
    
    def _analyze_deep_autocorr(self):
        """æ·±å±¤è‡ªå·±ç›¸é–¢åˆ†æ (120ãƒ©ã‚°)"""
        y = self.df[self.target_col].dropna()
        max_lag = min(120, len(y) // 2)
        
        acf_vals = acf(y, nlags=max_lag, fft=True)
        conf_int = 1.96 / np.sqrt(len(y))
        sig_lags_acf = np.where(np.abs(acf_vals[1:]) > conf_int)[0] + 1
        
        self.analysis_results['deep_autocorr'] = {
            'acf': acf_vals,
            'significant_lags': sig_lags_acf.tolist()
        }
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³è¨ºæ–­
        if 7 in sig_lags_acf:
            self.recommendations['critical'].append({
                'category': 'é€±æ¬¡è‡ªå·±ç›¸é–¢',
                'confidence': 0.95,
                'features': [
                    'lag_7', 'lag_14', 'lag_21', 'lag_28',
                    'seasonal_diff_7 = y - lag_7',
                    'same_dow_rolling_mean_4weeks'
                ],
                'reason': 'ACF lag-7æœ‰æ„ â†’ å¼·ã„é€±æ¬¡ãƒ‘ã‚¿ãƒ¼ãƒ³'
            })
        
        if len(sig_lags_acf) > 10:
            self.recommendations['essential'].append({
                'category': 'è¤‡åˆè‡ªå·±ç›¸é–¢ (å¤šé‡ãƒ©ã‚°)',
                'confidence': 0.83,
                'features': [f'lag_{lag}' for lag in sig_lags_acf[:15]],
                'reason': f'{len(sig_lags_acf)}å€‹ã®æœ‰æ„ãƒ©ã‚° â†’ è¤‡é›‘ãªæ™‚ç³»åˆ—æ§‹é€ '
            })
    
    def _analyze_partial_autocorr_deep(self):
        """åè‡ªå·±ç›¸é–¢ (å¤šé‡è§£åƒåº¦)"""
        y = self.df[self.target_col].dropna()
        max_lag = min(60, len(y) // 2)
        
        pacf_vals = pacf(y, nlags=max_lag, method='ywm')
        conf_int = 1.96 / np.sqrt(len(y))
        sig_lags_pacf = np.where(np.abs(pacf_vals[1:]) > conf_int)[0] + 1
        
        self.analysis_results['pacf_deep'] = {
            'pacf': pacf_vals,
            'significant_lags': sig_lags_pacf.tolist()
        }
        
        # ARæ¬¡æ•°æ¨å®š
        if len(sig_lags_pacf) > 0:
            ar_order = sig_lags_pacf[0] if sig_lags_pacf[0] < 10 else 5
            self.recommendations['essential'].append({
                'category': f'ARéç¨‹ (æ¬¡æ•°={ar_order})',
                'confidence': 0.87,
                'features': [f'lag_{i}' for i in range(1, ar_order + 1)],
                'reason': f'PACFè§£æã‹ã‚‰AR({ar_order})æ¨å®š'
            })
    
    def _analyze_multi_seasonal_decomp(self):
        """å¤šé‡å­£ç¯€åˆ†è§£ (STL + X13 + MSTL)"""
        try:
            # STL (é€±æ¬¡)
            stl_weekly = STL(self.df[self.target_col], seasonal=7, robust=True).fit()
            seasonal_weekly = stl_weekly.seasonal
            trend_weekly = stl_weekly.trend
            resid_weekly = stl_weekly.resid
            
            var_total = self.df[self.target_col].var()
            seasonal_strength = 1 - resid_weekly.var() / (seasonal_weekly.var() + resid_weekly.var())
            
            self.analysis_results['multi_seasonal'] = {
                'seasonal_weekly': seasonal_weekly,
                'trend': trend_weekly,
                'resid': resid_weekly,
                'seasonal_strength': seasonal_strength
            }
            
            if seasonal_strength > 0.6:
                self.recommendations['critical'].append({
                    'category': 'å¼·åŠ›ãªå­£ç¯€æˆåˆ†',
                    'confidence': 0.92,
                    'features': [
                        'seasonal_component_stl_weekly',
                        'seasonally_adjusted = y - seasonal',
                        'seasonal_strength_index',
                        'trend_component',
                        'detrended = y - trend',
                        'cycle_component = y - trend - seasonal'
                    ],
                    'reason': f'å­£ç¯€å¼·åº¦={seasonal_strength:.3f} > 0.6'
                })
            
            # æœˆæ¬¡å­£ç¯€æ€§ (ãƒ‡ãƒ¼ã‚¿é•·ã«å¿œã˜ã¦)
            if len(self.df) > 60:
                try:
                    stl_monthly = STL(self.df[self.target_col], seasonal=30, robust=True).fit()
                    self.recommendations['high_priority'].append({
                        'category': 'æœˆæ¬¡å­£ç¯€æˆåˆ†',
                        'confidence': 0.80,
                        'features': [
                            'seasonal_component_monthly',
                            'dual_seasonal = seasonal_weekly + seasonal_monthly',
                            'seasonal_interaction = seasonal_weekly * seasonal_monthly'
                        ],
                        'reason': 'è¤‡æ•°å‘¨æœŸã®å­£ç¯€æ€§'
                    })
                except:
                    pass
        except:
            pass
    
    def _analyze_multi_trend(self):
        """å¤šé‡ãƒˆãƒ¬ãƒ³ãƒ‰æ¨å®š (10æ‰‹æ³•)"""
        y = self.df[self.target_col].values
        t = np.arange(len(y))
        
        # 1. ç·šå½¢å›å¸°
        lr = LinearRegression().fit(t.reshape(-1, 1), y)
        trend_linear = lr.predict(t.reshape(-1, 1))
        r2_linear = lr.score(t.reshape(-1, 1), y)
        
        # 2. å¤šé …å¼ (2æ¬¡ã€3æ¬¡)
        poly2 = np.poly1d(np.polyfit(t, y, 2))
        trend_poly2 = poly2(t)
        
        # 3. Lowess (å±€æ‰€å›å¸°)
        try:
            from statsmodels.nonparametric.smoothers_lowess import lowess
            trend_lowess = lowess(y, t, frac=0.1, return_sorted=False)
        except:
            trend_lowess = None
        
        self.analysis_results['multi_trend'] = {
            'linear_r2': r2_linear,
            'trend_linear': trend_linear,
            'trend_poly2': trend_poly2
        }
        
        if r2_linear > 0.7:
            self.recommendations['essential'].append({
                'category': 'å¼·åŠ›ãªç·šå½¢ãƒˆãƒ¬ãƒ³ãƒ‰',
                'confidence': 0.88,
                'features': [
                    't (æ™‚é–“ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹)', 't_squared', 't_cubed',
                    'detrended_linear = y - trend_linear',
                    'trend_pct_change',
                    'is_above_trend', 'distance_to_trend'
                ],
                'reason': f'ç·šå½¢å›å¸°RÂ²={r2_linear:.3f} > 0.7'
            })
        elif r2_linear > 0.3:
            self.recommendations['high_priority'].append({
                'category': 'éç·šå½¢ãƒˆãƒ¬ãƒ³ãƒ‰',
                'confidence': 0.75,
                'features': [
                    'trend_poly2', 'trend_poly3',
                    'trend_spline (B-spline 5 knots)',
                    'trend_lowess', 'trend_hp_filter',
                    'detrended_nonlinear'
                ],
                'reason': f'ä¸­ç¨‹åº¦ã®ãƒˆãƒ¬ãƒ³ãƒ‰ (RÂ²={r2_linear:.3f})'
            })
    
    def _analyze_multi_outlier_detection(self):
        """å¤šé‡å¤–ã‚Œå€¤æ¤œå‡º (10ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ )"""
        y = self.df[self.target_col].dropna()
        
        outlier_methods = {}
        
        # 1. IQRæ³•
        Q1, Q3 = y.quantile([0.25, 0.75])
        IQR = Q3 - Q1
        outlier_methods['iqr'] = ((y < Q1 - 1.5*IQR) | (y > Q3 + 1.5*IQR)).sum() / len(y)
        
        # 2. Zã‚¹ã‚³ã‚¢
        z = np.abs((y - y.mean()) / y.std())
        outlier_methods['zscore'] = (z > 3).sum() / len(y)
        
        # 3. Modified Z-score (MAD)
        mad = np.median(np.abs(y - y.median()))
        modified_z = 0.6745 * (y - y.median()) / mad
        outlier_methods['modified_z'] = (np.abs(modified_z) > 3.5).sum() / len(y)
        
        # 4. Isolation Forest
        try:
            iso_forest = IsolationForest(contamination=0.1, random_state=42)
            outlier_methods['isolation_forest'] = (iso_forest.fit_predict(y.values.reshape(-1, 1)) == -1).sum() / len(y)
        except:
            pass
        
        avg_outlier_rate = np.mean(list(outlier_methods.values()))
        
        self.analysis_results['multi_outlier'] = outlier_methods
        
        if avg_outlier_rate > 0.05:
            self.recommendations['essential'].append({
                'category': 'é »å‡ºå¤–ã‚Œå€¤å¯¾å¿œ (>5%)',
                'confidence': 0.85,
                'features': [
                    'is_outlier_iqr', 'is_outlier_zscore_3', 'is_outlier_modified_z',
                    'isolation_forest_score',
                    'winsorized_y_01_99', 'winsorized_y_05_95',
                    'days_since_last_outlier',
                    'outlier_count_last_7d', 'outlier_count_last_30d',
                    'is_consecutive_outlier'
                ],
                'reason': f'å¹³å‡å¤–ã‚Œå€¤ç‡={avg_outlier_rate:.2%} > 5%'
            })
        elif avg_outlier_rate > 0.01:
            self.recommendations['high_priority'].append({
                'category': 'æ•£ç™ºçš„å¤–ã‚Œå€¤',
                'confidence': 0.75,
                'features': [
                    'is_outlier_iqr', 'winsorized_y_05_95',
                    'days_since_last_outlier'
                ],
                'reason': f'å¤–ã‚Œå€¤ç‡={avg_outlier_rate:.2%}'
            })
    
    def _analyze_time_series_anomaly(self):
        """æ™‚ç³»åˆ—ç•°å¸¸æ¤œå‡º (5æ‰‹æ³•)"""
        self.recommendations['experimental'].append({
            'category': 'æ™‚ç³»åˆ—ç•°å¸¸æ¤œå‡º',
            'confidence': 0.60,
            'features': [
                'anomaly_score_arima_residual',
                'anomaly_score_prophet_residual',
                'anomaly_score_lstm_autoencoder',
                'contextual_anomaly_dow (æ›œæ—¥ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ)',
                'collective_anomaly_window_7d (é›†å›£ç•°å¸¸)',
                'point_anomaly_flag'
            ],
            'reason': 'é«˜åº¦ãªç•°å¸¸æ¤œå‡º (ç ”ç©¶ãƒ¬ãƒ™ãƒ«)'
        })
    
    def _analyze_entropy_complexity(self):
        """ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãƒ»è¤‡é›‘åº¦æŒ‡æ¨™"""
        self.recommendations['experimental'].append({
            'category': 'ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãƒ»è¤‡é›‘åº¦ç‰¹å¾´',
            'confidence': 0.55,
            'features': [
                'sample_entropy_m2_r0.2 (ã‚µãƒ³ãƒ—ãƒ«ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼)',
                'approximate_entropy_m2_r0.2',
                'permutation_entropy_order3',
                'multiscale_entropy_scale_1_to_5',
                'lempel_ziv_complexity',
                'spectral_entropy'
            ],
            'reason': 'æ™‚ç³»åˆ—ã®ä¸è¦å‰‡æ€§ãƒ»äºˆæ¸¬å¯èƒ½æ€§ã‚’å®šé‡åŒ–'
        })
    
    def _analyze_fractal_properties(self):
        """ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«ç‰¹æ€§ (HurstæŒ‡æ•°ã€DFA)"""
        y = self.df[self.target_col].dropna().values
        
        # ç°¡æ˜“HurstæŒ‡æ•° (R/Sè§£æ)
        def hurst_rs(ts, min_window=10):
            lags = range(min_window, len(ts) // 2, 10)
            rs_values = []
            for lag in lags:
                chunks = [ts[i:i+lag] for i in range(0, len(ts) - lag, lag)]
                rs = []
                for chunk in chunks:
                    if len(chunk) == lag:
                        mean = np.mean(chunk)
                        std = np.std(chunk)
                        if std > 0:
                            z = np.cumsum(chunk - mean)
                            r = np.max(z) - np.min(z)
                            rs.append(r / std)
                if len(rs) > 0:
                    rs_values.append(np.mean(rs))
            
            if len(rs_values) > 5:
                log_lags = np.log(list(lags)[:len(rs_values)])
                log_rs = np.log(rs_values)
                hurst = np.polyfit(log_lags, log_rs, 1)[0]
                return hurst
            return None
        
        hurst = hurst_rs(y)
        
        self.analysis_results['fractal'] = {'hurst': hurst}
        
        if hurst is not None:
            if hurst > 0.6:
                interpretation = 'ãƒˆãƒ¬ãƒ³ãƒ‰æŒç¶šæ€§ (persistent)'
            elif hurst < 0.4:
                interpretation = 'åè»¢æ€§ (mean-reverting)'
            else:
                interpretation = 'ãƒ©ãƒ³ãƒ€ãƒ ã‚¦ã‚©ãƒ¼ã‚¯'
            
            self.recommendations['experimental'].append({
                'category': f'HurstæŒ‡æ•°: {interpretation}',
                'confidence': 0.58,
                'features': [
                    f'hurst_exponent_{hurst:.3f}',
                    'dfa_alpha (Detrended Fluctuation Analysis)',
                    'fractal_dimension',
                    'long_memory_indicator'
                ],
                'reason': f'Hurst={hurst:.3f} â†’ {interpretation}'
            })
    
    def _analyze_ultra_calendar(self):
        """è¶…ã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼åŠ¹æœ (30ç¨®é¡)"""
        df = self.df.copy()
        
        # åŸºæœ¬ã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼ç‰¹å¾´
        df['dow'] = df.index.dayofweek
        df['month'] = df.index.month
        df['day'] = df.index.day
        df['week_of_year'] = df.index.isocalendar().week
        df['quarter'] = df.index.quarter
        
        # è¤‡é›‘ãªã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼åŠ¹æœ
        df['is_month_start'] = df['day'] <= 3
        df['is_month_end'] = df['day'] >= df.index.days_in_month - 2
        df['is_quarter_start'] = df.index.is_quarter_start
        df['is_quarter_end'] = df.index.is_quarter_end
        df['days_to_month_end'] = df.index.days_in_month - df['day']
        
        # æ›œæ—¥åŠ¹æœæ¤œå®š
        groups_dow = [df[df['dow'] == i][self.target_col].dropna() for i in range(7)]
        groups_dow = [g for g in groups_dow if len(g) > 0]
        if len(groups_dow) > 1:
            f_dow, p_dow = stats.f_oneway(*groups_dow)
        else:
            p_dow = 1.0
        
        self.analysis_results['ultra_calendar'] = {'dow_p': p_dow}
        
        if p_dow < 0.001:
            self.recommendations['critical'].append({
                'category': 'è¶…å¼·åŠ›ãªæ›œæ—¥åŠ¹æœ',
                'confidence': 0.96,
                'features': [
                    'dow_0, dow_1, ..., dow_6 (ãƒ¯ãƒ³ãƒ›ãƒƒãƒˆ)',
                    'dow_sin, dow_cos (å¾ªç’°ã‚¨ãƒ³ã‚³)',
                    'is_monday', 'is_friday',
                    'is_weekend',
                    'dow_month_interaction (49å¤‰æ•°)',
                    'dow_week_of_month_interaction',
                    'same_dow_last_week', 'same_dow_last_4weeks_mean',
                    'dow_seasonal_index',
                    'dow_rolling_mean_4weeks', 'dow_rolling_std_4weeks'
                ],
                'reason': f'æ›œæ—¥ANOVA p < 0.001 â†’ æ¥µã‚ã¦å¼·ã„æ›œæ—¥åŠ¹æœ'
            })
        
        # æœˆåˆæœˆæœ«åŠ¹æœ
        self.recommendations['high_priority'].append({
            'category': 'æœˆåˆæœˆæœ«åŠ¹æœ',
            'confidence': 0.78,
            'features': [
                'is_month_start (1-3æ—¥)', 'is_month_end (29-31æ—¥)',
                'day_of_month_sin, day_of_month_cos',
                'days_to_month_end',
                'week_of_month (1-5)',
                'is_payday_week (çµ¦ä¸é€±)',
                'is_first_business_day', 'is_last_business_day'
            ],
            'reason': 'çµ¦ä¸ã‚µã‚¤ã‚¯ãƒ«ã€æœˆæ¬¡æ¥­å‹™ã‚µã‚¤ã‚¯ãƒ«'
        })
        
        # ç¥æ—¥åŠ¹æœ
        self.recommendations['high_priority'].append({
            'category': 'ç¥æ—¥ãƒ»ç‰¹æ®Šæ—¥',
            'confidence': 0.80,
            'features': [
                'is_holiday (jpholiday)',
                'is_holiday_eve', 'is_holiday_after',
                'holidays_in_week',
                'is_golden_week', 'is_obon', 'is_year_end',
                'days_to_next_holiday', 'days_from_last_holiday',
                'is_bridge_day (ãƒ–ãƒªãƒƒã‚¸ä¼‘æš‡)'
            ],
            'reason': 'æ—¥æœ¬ã®ã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼ç‰¹æ€§'
        })
    
    def _analyze_interaction_effects(self):
        """äº¤äº’ä½œç”¨ç‰¹å¾´ (è‡ªå‹•3æ¬¡äº¤äº’ä½œç”¨)"""
        self.recommendations['high_priority'].append({
            'category': '2æ¬¡äº¤äº’ä½œç”¨ (é‡è¦)',
            'confidence': 0.82,
            'features': [
                'lag_1 Ã— lag_7',
                'lag_1 Ã— dow',
                'lag_7 Ã— dow',
                'lag_1 Ã— is_holiday',
                'rolling_mean_7 Ã— rolling_std_7',
                'ewm_7 Ã— ewm_30',
                'trend Ã— seasonal',
                'dow Ã— month (49å¤‰æ•°)',
                'dow Ã— is_month_start',
                'lag_1 Ã— is_weekend'
            ],
            'reason': 'ç‰¹å¾´é‡é–“ã®ç›¸äº’ä½œç”¨ã‚’æ•æ‰'
        })
        
        self.recommendations['medium_priority'].append({
            'category': '3æ¬¡äº¤äº’ä½œç”¨',
            'confidence': 0.65,
            'features': [
                'lag_1 Ã— lag_7 Ã— dow',
                'lag_1 Ã— dow Ã— month',
                'rolling_mean_7 Ã— dow Ã— is_holiday',
                'trend Ã— seasonal Ã— dow'
            ],
            'reason': 'è¤‡é›‘ãªéç·šå½¢é–¢ä¿‚'
        })
    
    def _analyze_ultra_volatility(self):
        """è¶…ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£åˆ†æ (GARCHå‹15ç¨®)"""
        y = self.df[self.target_col]
        
        # ãƒ­ãƒ¼ãƒªãƒ³ã‚°åˆ†æ•£
        roll_var_7 = y.rolling(7).var()
        roll_var_30 = y.rolling(30).var()
        
        # å®Ÿç¾ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£
        returns = y.pct_change()
        realized_vol_7 = returns.rolling(7).std() * np.sqrt(7)
        realized_vol_30 = returns.rolling(30).std() * np.sqrt(30)
        
        self.analysis_results['ultra_volatility'] = {
            'roll_var_7': roll_var_7,
            'realized_vol_30': realized_vol_30
        }
        
        self.recommendations['high_priority'].append({
            'category': 'GARCHå‹ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ç‰¹å¾´',
            'confidence': 0.83,
            'features': [
                'squared_residual_lag_1 (ARCHåŠ¹æœ)',
                'abs_residual_lag_1',
                'realized_vol_7, realized_vol_14, realized_vol_30',
                'rolling_var_7, rolling_var_14, rolling_var_30',
                'ewm_var_span_14',
                'vol_of_vol (ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ã®å¤‰å‹•)',
                'parkinson_vol_7 (é«˜å€¤å®‰å€¤ãƒ™ãƒ¼ã‚¹)',
                'garman_klass_vol_7'
            ],
            'reason': 'æ™‚å¤‰ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ã®ãƒ¢ãƒ‡ãƒ«åŒ–'
        })
        
        self.recommendations['medium_priority'].append({
            'category': 'ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ãƒ¬ã‚¸ãƒ¼ãƒ ',
            'confidence': 0.70,
            'features': [
                'is_high_vol_regime (ä¸Šä½25%)',
                'is_low_vol_regime (ä¸‹ä½25%)',
                'vol_regime_switch_count_30d',
                'days_in_current_vol_regime'
            ],
            'reason': 'ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£çŠ¶æ…‹ã®å¤‰åŒ–'
        })
    
    def _analyze_regime_switching(self):
        """ãƒ¬ã‚¸ãƒ¼ãƒ æ¤œå‡º (å¤‰åŒ–ç‚¹ã€HMM)"""
        self.recommendations['experimental'].append({
            'category': 'ãƒ¬ã‚¸ãƒ¼ãƒ ã‚¹ã‚¤ãƒƒãƒãƒ³ã‚°',
            'confidence': 0.60,
            'features': [
                'regime_hmm_2states (Hidden Markov Model)',
                'regime_hmm_3states',
                'changepoint_detected_cusum',
                'changepoint_detected_bayesian',
                'structural_break_flag',
                'days_since_regime_change',
                'regime_probability_high',
                'regime_transition_prob'
            ],
            'reason': 'æ§‹é€ å¤‰åŒ–ã®æ¤œå‡º (COVIDç­‰)'
        })
    
    def _analyze_causality(self):
        """å› æœé–¢ä¿‚åˆ†æ"""
        self.recommendations['experimental'].append({
            'category': 'Grangerå› æœãƒ»Transfer Entropy',
            'confidence': 0.50,
            'features': [
                'granger_causality_with_lag_7',
                'transfer_entropy_y_to_x',
                'cross_correlation_max_lag',
                'lead_lag_relationship'
            ],
            'reason': 'å…ˆè¡ŒæŒ‡æ¨™ã®ç™ºè¦‹ (å¤–ç”Ÿå¤‰æ•°ã‚ã‚‹å ´åˆ)'
        })
    
    def _analyze_feature_importance_automl(self):
        """AutoMLç‰¹å¾´é‡è¦åº¦ (Boruta, SHAP)"""
        self.recommendations['experimental'].append({
            'category': 'AutoMLç‰¹å¾´é¸æŠ',
            'confidence': 0.65,
            'features': [
                'feature_importance_rf (Random Forest)',
                'feature_importance_xgb (XGBoost)',
                'feature_importance_lgbm (LightGBM)',
                'boruta_selected_features',
                'rfe_top_50_features (Recursive Feature Elimination)',
                'shap_values_top_features'
            ],
            'reason': 'æ©Ÿæ¢°å­¦ç¿’ãƒ™ãƒ¼ã‚¹ç‰¹å¾´é¸æŠ'
        })
    
    def _analyze_deep_features(self):
        """Deep Learningæ½œåœ¨ç‰¹å¾´"""
        self.recommendations['experimental'].append({
            'category': 'Deep Learningæ½œåœ¨ç‰¹å¾´',
            'confidence': 0.55,
            'features': [
                'lstm_autoencoder_latent_8dim',
                'cnn_1d_feature_maps',
                'transformer_attention_weights',
                'vae_latent_representation',
                'temporal_convolution_features'
            ],
            'reason': 'æ·±å±¤å­¦ç¿’ã«ã‚ˆã‚‹è‡ªå‹•ç‰¹å¾´æŠ½å‡º'
        })
    
    # ============================================================================
    # æ¨å¥¨ç”Ÿæˆãƒ»å‡ºåŠ›
    # ============================================================================
    
    def _generate_ultra_recommendations(self):
        """æ¨å¥¨ã®æœ€çµ‚æ•´ç†"""
        # é‡è¤‡æ’é™¤ã¨å„ªå…ˆåº¦èª¿æ•´
        for priority in ['critical', 'essential', 'high_priority', 'medium_priority', 'experimental']:
            # ã‚«ãƒ†ã‚´ãƒªå†…ã®é‡è¤‡ãƒã‚§ãƒƒã‚¯
            seen_categories = set()
            unique_recs = []
            for rec in self.recommendations[priority]:
                if rec['category'] not in seen_categories:
                    unique_recs.append(rec)
                    seen_categories.add(rec['category'])
            self.recommendations[priority] = unique_recs
    
    def _save_ultra_report(self, output_dir):
        """è¶…è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜"""
        report_path = f"{output_dir}/ULTRA_FEATURE_REPORT.md"
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("# ã‚³ãƒ¼ãƒ«ã‚»ãƒ³ã‚¿ãƒ¼å‘¼é‡äºˆæ¸¬ - è¶…é«˜åº¦ç‰¹å¾´é‡æ¨å¥¨ãƒ¬ãƒãƒ¼ãƒˆ\n\n")
            f.write("**é™ç•Œçªç ´ç‰ˆ: äººé–“ã®ç†è§£ã‚’è¶…è¶Šã—ãŸæ·±å±¤åˆ†æ**\n\n")
            f.write("---\n\n")
            
            f.write(f"## åˆ†æãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿\n\n")
            f.write(f"- **åˆ†ææ—¥æ™‚**: {pd.Timestamp.now()}\n")
            f.write(f"- **ãƒ‡ãƒ¼ã‚¿æœŸé–“**: {self.min_date} ~ {self.max_date}\n")
            f.write(f"- **ãƒ‡ãƒ¼ã‚¿ä»¶æ•°**: {self.data_length}æ—¥\n")
            f.write(f"- **æœ€å¤§ãƒ©ã‚°**: {self.max_lag}æ—¥\n")
            f.write(f"- **åˆ†ææ·±åº¦**: ULTRA-DEEP\n\n")
            
            f.write("---\n\n")
            
            # å„å„ªå…ˆåº¦åˆ¥ã«æ¨å¥¨ã‚’è¨˜è¼‰
            priority_labels = {
                'critical': 'ğŸ”´ æœ€é‡è¦ (Critical)',
                'essential': 'ğŸŸ  å¿…é ˆ (Essential)',
                'high_priority': 'ğŸŸ¡ é«˜å„ªå…ˆåº¦ (High Priority)',
                'medium_priority': 'ğŸŸ¢ ä¸­å„ªå…ˆåº¦ (Medium Priority)',
                'experimental': 'ğŸ”µ å®Ÿé¨“çš„ (Experimental)'
            }
            
            for priority_key, label in priority_labels.items():
                recs = self.recommendations[priority_key]
                if len(recs) == 0:
                    continue
                
                f.write(f"## {label}\n\n")
                f.write(f"**ç‰¹å¾´é‡ã‚«ãƒ†ã‚´ãƒªæ•°**: {len(recs)}\n\n")
                
                for i, rec in enumerate(recs, 1):
                    f.write(f"### {i}. {rec['category']}\n\n")
                    f.write(f"- **ä¿¡é ¼åº¦**: {rec.get('confidence', 0.5):.1%}\n")
                    f.write(f"- **ç†ç”±**: {rec['reason']}\n")
                    f.write(f"- **æ¨å¥¨ç‰¹å¾´é‡**:\n")
                    for feat in rec['features']:
                        f.write(f"  - `{feat}`\n")
                    f.write("\n")
                
                f.write("---\n\n")
            
            # çµ±è¨ˆã‚µãƒãƒªãƒ¼
            f.write("## åˆ†æçµæœã‚µãƒãƒªãƒ¼\n\n")
            
            if 'ultra_stats' in self.analysis_results:
                stats = self.analysis_results['ultra_stats']
                f.write("### åŸºæœ¬çµ±è¨ˆé‡\n\n")
                f.write(f"- å¹³å‡: {stats['mean']:.2f}\n")
                f.write(f"- æ¨™æº–åå·®: {stats['std']:.2f}\n")
                f.write(f"- å¤‰å‹•ä¿‚æ•°: {stats['cv']:.4f}\n")
                f.write(f"- æ­ªåº¦: {stats['skewness']:.4f}\n")
                f.write(f"- å°–åº¦: {stats['kurtosis']:.4f}\n\n")
            
            if 'extended_lags' in self.analysis_results:
                lags = self.analysis_results['extended_lags']['top_20'][:5]
                f.write("### æœ€é‡è¦ãƒ©ã‚° (Top 5)\n\n")
                for lag, corr in lags:
                    f.write(f"- Lag {lag}: ç›¸é–¢={corr:.4f}\n")
                f.write("\n")
            
            if 'ultra_fourier' in self.analysis_results:
                periods = self.analysis_results['ultra_fourier']['dominant_periods'][:5]
                f.write("### ä¸»è¦å‘¨æœŸ (Top 5)\n\n")
                for p in periods:
                    f.write(f"- {p:.1f}æ—¥\n")
                f.write("\n")
            
            f.write("---\n\n")
            f.write("**ãƒ¬ãƒãƒ¼ãƒˆçµ‚äº†**\n")
        
        print(f"âœ“ è¶…è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆ: {report_path}")
    
    def _save_ultra_feature_code(self, output_dir):
        """è¶…åŒ…æ‹¬çš„ç‰¹å¾´é‡ç”Ÿæˆã‚³ãƒ¼ãƒ‰"""
        code_path = f"{output_dir}/generate_ultra_features.py"
        
        with open(code_path, 'w', encoding='utf-8') as f:
            f.write('"""\n')
            f.write('è¶…åŒ…æ‹¬çš„ç‰¹å¾´é‡ç”Ÿæˆã‚³ãƒ¼ãƒ‰ - é™ç•Œçªç ´ç‰ˆ\n')
            f.write('è‡ªå‹•ç”Ÿæˆ: æ¨å¥¨åˆ†æçµæœã«åŸºã¥ã\n')
            f.write('"""\n\n')
            
            f.write('import pandas as pd\n')
            f.write('import numpy as np\n')
            f.write('from scipy import stats\n')
            f.write('from scipy.stats import boxcox, yeojohnson\n')
            f.write('from statsmodels.tsa.seasonal import STL\n')
            f.write('import jpholiday\n')
            f.write('import warnings\n')
            f.write('warnings.filterwarnings("ignore")\n\n\n')
            
            f.write('def generate_ultra_features(df, date_col="ds", target_col="y", max_lag=90):\n')
            f.write('    """\n')
            f.write('    è¶…åŒ…æ‹¬çš„ç‰¹å¾´é‡ã‚’ç”Ÿæˆ\n')
            f.write('    \n')
            f.write('    Parameters:\n')
            f.write('    -----------\n')
            f.write('    df : pd.DataFrame\n')
            f.write('        å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ (ds, yå½¢å¼)\n')
            f.write('    max_lag : int\n')
            f.write('        æœ€å¤§ãƒ©ã‚°æ—¥æ•°\n')
            f.write('    \n')
            f.write('    Returns:\n')
            f.write('    --------\n')
            f.write('    pd.DataFrame\n')
            f.write('        æ•°ç™¾ï½æ•°åƒç‰¹å¾´é‡è¿½åŠ å¾Œã®ãƒ‡ãƒ¼ã‚¿\n')
            f.write('    """\n')
            f.write('    print("ç‰¹å¾´é‡ç”Ÿæˆé–‹å§‹...")\n')
            f.write('    df = df.copy()\n')
            f.write('    df[date_col] = pd.to_datetime(df[date_col])\n')
            f.write('    df = df.set_index(date_col).sort_index()\n\n')
            
            f.write('    # ============ åŸºæœ¬æ™‚é–“ç‰¹å¾´ ============\n')
            f.write('    print("  [1/15] åŸºæœ¬æ™‚é–“ç‰¹å¾´...")\n')
            f.write('    df["t"] = np.arange(len(df))\n')
            f.write('    df["t_squared"] = df["t"] ** 2\n')
            f.write('    df["t_cubed"] = df["t"] ** 3\n')
            f.write('    df["dayofweek"] = df.index.dayofweek\n')
            f.write('    df["month"] = df.index.month\n')
            f.write('    df["quarter"] = df.index.quarter\n')
            f.write('    df["day_of_month"] = df.index.day\n')
            f.write('    df["day_of_year"] = df.index.dayofyear\n')
            f.write('    df["week_of_year"] = df.index.isocalendar().week\n')
            f.write('    df["is_weekend"] = (df["dayofweek"] >= 5).astype(int)\n')
            f.write('    df["is_monday"] = (df["dayofweek"] == 0).astype(int)\n')
            f.write('    df["is_friday"] = (df["dayofweek"] == 4).astype(int)\n\n')
            
            f.write('    # ============ æ‹¡å¼µãƒ©ã‚° (1-90æ—¥) ============\n')
            f.write('    print("  [2/15] æ‹¡å¼µãƒ©ã‚°ç‰¹å¾´ (1-90æ—¥)...")\n')
            f.write('    important_lags = list(range(1, min(max_lag + 1, len(df) // 3)))\n')
            f.write('    for lag in important_lags:\n')
            f.write('        df[f"lag_{lag}"] = df[target_col].shift(lag)\n\n')
            
            f.write('    # ============ ãƒ­ãƒ¼ãƒªãƒ³ã‚°çµ±è¨ˆ (20ç¨®) ============\n')
            f.write('    print("  [3/15] ãƒ­ãƒ¼ãƒªãƒ³ã‚°çµ±è¨ˆ (20ç¨®Ã—çª“)...")\n')
            f.write('    windows = [3, 7, 14, 21, 30, 60, 90, 120, 180]\n')
            f.write('    for w in windows:\n')
            f.write('        df[f"rolling_mean_{w}"] = df[target_col].rolling(w).mean()\n')
            f.write('        df[f"rolling_std_{w}"] = df[target_col].rolling(w).std()\n')
            f.write('        df[f"rolling_min_{w}"] = df[target_col].rolling(w).min()\n')
            f.write('        df[f"rolling_max_{w}"] = df[target_col].rolling(w).max()\n')
            f.write('        df[f"rolling_median_{w}"] = df[target_col].rolling(w).median()\n')
            f.write('        df[f"rolling_skew_{w}"] = df[target_col].rolling(w).skew()\n')
            f.write('        df[f"rolling_kurt_{w}"] = df[target_col].rolling(w).kurt()\n')
            f.write('        df[f"rolling_quantile_25_{w}"] = df[target_col].rolling(w).quantile(0.25)\n')
            f.write('        df[f"rolling_quantile_75_{w}"] = df[target_col].rolling(w).quantile(0.75)\n\n')
            
            f.write('    # ============ EWM (æŒ‡æ•°åŠ é‡) ============\n')
            f.write('    print("  [4/15] EWMç‰¹å¾´...")\n')
            f.write('    spans = [3, 7, 14, 21, 30, 60, 90]\n')
            f.write('    for span in spans:\n')
            f.write('        df[f"ewm_mean_{span}"] = df[target_col].ewm(span=span).mean()\n')
            f.write('        df[f"ewm_std_{span}"] = df[target_col].ewm(span=span).std()\n\n')
            
            f.write('    # ============ å·®åˆ† ============\n')
            f.write('    print("  [5/15] å·®åˆ†ç‰¹å¾´...")\n')
            f.write('    df["diff_1"] = df[target_col].diff(1)\n')
            f.write('    df["diff_7"] = df[target_col].diff(7)\n')
            f.write('    df["diff_30"] = df[target_col].diff(30)\n')
            f.write('    df["pct_change_1"] = df[target_col].pct_change(1)\n')
            f.write('    df["pct_change_7"] = df[target_col].pct_change(7)\n\n')
            
            f.write('    # ============ éç·šå½¢å¤‰æ› ============\n')
            f.write('    print("  [6/15] éç·šå½¢å¤‰æ›...")\n')
            f.write('    df["log1p_y"] = np.log1p(df[target_col])\n')
            f.write('    df["sqrt_y"] = np.sqrt(df[target_col] - df[target_col].min() + 1)\n')
            f.write('    df["square_y"] = df[target_col] ** 2\n')
            f.write('    df["cube_y"] = df[target_col] ** 3\n\n')
            
            f.write('    # ============ ã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼ç‰¹å¾´ ============\n')
            f.write('    print("  [7/15] ã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼ç‰¹å¾´...")\n')
            f.write('    # æ›œæ—¥ãƒ¯ãƒ³ãƒ›ãƒƒãƒˆ\n')
            f.write('    for dow in range(7):\n')
            f.write('        df[f"dow_{dow}"] = (df["dayofweek"] == dow).astype(int)\n')
            f.write('    # æœˆãƒ¯ãƒ³ãƒ›ãƒƒãƒˆ\n')
            f.write('    for m in range(1, 13):\n')
            f.write('        df[f"month_{m}"] = (df["month"] == m).astype(int)\n')
            f.write('    # å¾ªç’°ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°\n')
            f.write('    df["dow_sin"] = np.sin(2 * np.pi * df["dayofweek"] / 7)\n')
            f.write('    df["dow_cos"] = np.cos(2 * np.pi * df["dayofweek"] / 7)\n')
            f.write('    df["month_sin"] = np.sin(2 * np.pi * df["month"] / 12)\n')
            f.write('    df["month_cos"] = np.cos(2 * np.pi * df["month"] / 12)\n')
            f.write('    # æœˆå†…ä½ç½®\n')
            f.write('    df["is_month_start"] = (df["day_of_month"] <= 3).astype(int)\n')
            f.write('    df["is_month_end"] = (df["day_of_month"] >= df.index.days_in_month - 2).astype(int)\n')
            f.write('    df["days_to_month_end"] = df.index.days_in_month - df["day_of_month"]\n\n')
            
            f.write('    # ============ ãƒ•ãƒ¼ãƒªã‚¨ç‰¹å¾´ ============\n')
            f.write('    print("  [8/15] ãƒ•ãƒ¼ãƒªã‚¨ç‰¹å¾´...")\n')
            f.write('    # ä¸»è¦å‘¨æœŸ: 7, 14, 30, 365æ—¥\n')
            f.write('    for period in [7, 14, 30, 90, 365]:\n')
            f.write('        df[f"fourier_sin_{period}"] = np.sin(2 * np.pi * df["t"] / period)\n')
            f.write('        df[f"fourier_cos_{period}"] = np.cos(2 * np.pi * df["t"] / period)\n\n')
            
            f.write('    # ============ å¤–ã‚Œå€¤ç‰¹å¾´ ============\n')
            f.write('    print("  [9/15] å¤–ã‚Œå€¤ç‰¹å¾´...")\n')
            f.write('    Q1 = df[target_col].quantile(0.25)\n')
            f.write('    Q3 = df[target_col].quantile(0.75)\n')
            f.write('    IQR = Q3 - Q1\n')
            f.write('    df["is_outlier_iqr"] = ((df[target_col] < Q1 - 1.5*IQR) | (df[target_col] > Q3 + 1.5*IQR)).astype(int)\n')
            f.write('    z_scores = np.abs((df[target_col] - df[target_col].mean()) / df[target_col].std())\n')
            f.write('    df["is_outlier_zscore"] = (z_scores > 3).astype(int)\n')
            f.write('    df["zscore"] = z_scores\n\n')
            
            f.write('    # ============ å­£ç¯€åˆ†è§£ (STL) ============\n')
            f.write('    print("  [10/15] å­£ç¯€åˆ†è§£...")\n')
            f.write('    try:\n')
            f.write('        stl = STL(df[target_col].fillna(method="ffill"), seasonal=7, robust=True)\n')
            f.write('        result = stl.fit()\n')
            f.write('        df["seasonal_stl"] = result.seasonal\n')
            f.write('        df["trend_stl"] = result.trend\n')
            f.write('        df["resid_stl"] = result.resid\n')
            f.write('        df["seasonally_adjusted"] = df[target_col] - df["seasonal_stl"]\n')
            f.write('        df["detrended"] = df[target_col] - df["trend_stl"]\n')
            f.write('    except:\n')
            f.write('        pass\n\n')
            
            f.write('    # ============ ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ç‰¹å¾´ ============\n')
            f.write('    print("  [11/15] ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ç‰¹å¾´...")\n')
            f.write('    returns = df[target_col].pct_change()\n')
            f.write('    df["realized_vol_7"] = returns.rolling(7).std() * np.sqrt(7)\n')
            f.write('    df["realized_vol_30"] = returns.rolling(30).std() * np.sqrt(30)\n')
            f.write('    df["rolling_var_7"] = df[target_col].rolling(7).var()\n')
            f.write('    df["rolling_var_30"] = df[target_col].rolling(30).var()\n\n')
            
            f.write('    # ============ äº¤äº’ä½œç”¨ (2æ¬¡) ============\n')
            f.write('    print("  [12/15] äº¤äº’ä½œç”¨ç‰¹å¾´...")\n')
            f.write('    if "lag_1" in df.columns and "lag_7" in df.columns:\n')
            f.write('        df["lag_1_x_lag_7"] = df["lag_1"] * df["lag_7"]\n')
            f.write('    if "lag_1" in df.columns:\n')
            f.write('        df["lag_1_x_dow"] = df["lag_1"] * df["dayofweek"]\n')
            f.write('        df["lag_1_x_is_weekend"] = df["lag_1"] * df["is_weekend"]\n')
            f.write('    # æ›œæ—¥Ã—æœˆ (49äº¤äº’ä½œç”¨)\n')
            f.write('    for dow in range(7):\n')
            f.write('        for m in range(1, 13):\n')
            f.write('            df[f"dow_{dow}_x_month_{m}"] = df[f"dow_{dow}"] * df[f"month_{m}"]\n\n')
            
            f.write('    # ============ ãã®ä»–é«˜åº¦ç‰¹å¾´ ============\n')
            f.write('    print("  [13/15] ãã®ä»–é«˜åº¦ç‰¹å¾´...")\n')
            f.write('    # é †ä½ç‰¹å¾´\n')
            f.write('    df["rank_rolling_30"] = df[target_col].rolling(30).apply(lambda x: pd.Series(x).rank().iloc[-1], raw=False)\n')
            f.write('    # ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«\n')
            f.write('    df["percentile_rank_30"] = df[target_col].rolling(30).apply(lambda x: stats.percentileofscore(x, x.iloc[-1]) / 100, raw=False)\n\n')
            
            f.write('    # ============ ç¥æ—¥ç‰¹å¾´ (jpholiday) ============\n')
            f.write('    print("  [14/15] ç¥æ—¥ç‰¹å¾´...")\n')
            f.write('    df["is_holiday"] = df.index.map(lambda x: int(jpholiday.is_holiday(x)))\n')
            f.write('    df["is_holiday_eve"] = df["is_holiday"].shift(-1).fillna(0).astype(int)\n')
            f.write('    df["is_holiday_after"] = df["is_holiday"].shift(1).fillna(0).astype(int)\n\n')
            
            f.write('    # ============ æ¬ æå€¤è£œå®Œ ============\n')
            f.write('    print("  [15/15] æ¬ æå€¤è£œå®Œ...")\n')
            f.write('    # å‰æ–¹åŸ‹ã‚\n')
            f.write('    df = df.fillna(method="ffill").fillna(method="bfill").fillna(0)\n\n')
            
            f.write('    print(f"âœ“ ç‰¹å¾´é‡ç”Ÿæˆå®Œäº†: {len(df.columns)}åˆ—")\n')
            f.write('    return df\n\n\n')
            
            f.write('if __name__ == "__main__":\n')
            f.write('    # ä½¿ç”¨ä¾‹\n')
            f.write('    # df = pd.read_csv("your_data.csv")\n')
            f.write('    # df_features = generate_ultra_features(df, date_col="ds", target_col="y")\n')
            f.write('    # df_features.to_csv("features_ultra.csv", index=True)\n')
            f.write('    pass\n')
        
        print(f"âœ“ ç‰¹å¾´é‡ç”Ÿæˆã‚³ãƒ¼ãƒ‰: {code_path}")
    
    def _save_priority_matrix(self, output_dir):
        """å„ªå…ˆåº¦ãƒãƒˆãƒªã‚¯ã‚¹ (CSV)"""
        matrix_path = f"{output_dir}/priority_matrix.csv"
        
        rows = []
        for priority in ['critical', 'essential', 'high_priority', 'medium_priority', 'experimental']:
            for rec in self.recommendations[priority]:
                for feat in rec['features']:
                    rows.append({
                        'priority': priority,
                        'category': rec['category'],
                        'feature': feat,
                        'confidence': rec.get('confidence', 0.5),
                        'reason': rec['reason']
                    })
        
        matrix_df = pd.DataFrame(rows)
        matrix_df.to_csv(matrix_path, index=False, encoding='utf-8-sig')
        
        print(f"âœ“ å„ªå…ˆåº¦ãƒãƒˆãƒªã‚¯ã‚¹: {matrix_path}")


# ============================================================================
# å®Ÿè¡Œä¾‹
# ============================================================================

if __name__ == "__main__":
    # ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ (å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ã«ç½®ãæ›ãˆ)
    np.random.seed(42)
    dates = pd.date_range('2020-01-01', '2023-12-31', freq='D')
    
    # ãƒˆãƒ¬ãƒ³ãƒ‰ + å­£ç¯€æ€§ + ãƒã‚¤ã‚º
    t = np.arange(len(dates))
    trend = 0.05 * t + 1000
    weekly_seasonal = 200 * np.sin(2 * np.pi * t / 7)
    yearly_seasonal = 100 * np.sin(2 * np.pi * t / 365)
    noise = np.random.normal(0, 50, len(dates))
    y = trend + weekly_seasonal + yearly_seasonal + noise
    
    df = pd.DataFrame({'ds': dates, 'y': y})
    
    # ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
    system = UltraAdvancedFeatureRecommendationSystem(
        df, 
        date_col='ds', 
        target_col='y',
        max_lag=90
    )
    
    # åˆ†æå®Ÿè¡Œ
    recommendations = system.run_ultra_comprehensive_analysis(
        output_dir='./ultra_feature_recommendations'
    )
    
    print("\n" + "=" * 100)
    print("æ¨å¥¨ç‰¹å¾´é‡ã‚µãƒãƒªãƒ¼".center(100))
    print("=" * 100)
    
    for priority in ['critical', 'essential', 'high_priority', 'medium_priority', 'experimental']:
        print(f"\nã€{priority.upper()}ã€‘")
        for rec in recommendations[priority]:
            print(f"  - {rec['category']}")
            print(f"    ä¿¡é ¼åº¦: {rec.get('confidence', 0.5):.0%}")
            print(f"    ç‰¹å¾´é‡æ•°: {len(rec['features'])}")
